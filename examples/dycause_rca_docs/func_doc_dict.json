{
    "32&@M.dycause_lib.f_test": "**Function Name**: f_test\n\n**Signature**\n```python\ndef f_test(res2down, res2djoint, lag)\n```\n**Function Summary**\n\nThe function `f_test` performs the F statistical test using the results of the fitting process and returns the statistics values. It determines the Granger causality using SSR (sum of squared residuals). The result of this test measures how well the past values of one variable predict the future values of another.\n\n**Parameters**\n1. `res2down`(object): The OLS regression results of the restricted model. Expected type is `RegressionResultsWrapper`.\n2. `res2djoint` (object): The OLS regression results of the unrestricted model. Expected type is `RegressionResultsWrapper`.\n3. `lag` (int): The length of the lag interval. This refers to the delay with which the past values of one variable predict the future values of another.\n\n**Returns**\n\nThe function returns a dictionary with the following keys and values:\n\n    1. `'ssr_ftest'` (tuple): Contains the following information:\n       - `F-statistics` (float): The F statistical test's value.\n      - `P-value` (float): The probability that the test statistics has an F-distribution under the null hypothesis.\n      - `degrees of freedom` (int): The number of remaining degrees of freedom for the unrestricted model.\n      - `lag` (int): The length of the lag interval.\n\n**Example**\n```python\n# Define the parameters\nres2down = <restricted_model_results>\nres2djoint = <unrestricted_model_results>\nlag = 5\n\n# call the function\nf_test(res2down, res2djoint, lag)\n\n# Expected output would be in the form:\n{'ssr_ftest': (F-statistics, p-value, degrees of freedom, lag)}\n```\nPlease note that `<restricted_model_results>` and `<unrestricted_model_results>` in the example are placeholders. They should be replaced with the respective model results obtained from an OLS regression. \n\n**Notes** \n\nBased on the source code, there may be a potential for division by zero occurring within the `f_test` function.",
    "32&@M.dycause_lib.pick_a_trip": "**Signature**: \n```python\npick_a_trip(array_data, trip, list_dividing_limit)\n```\n\n**Function Summary**:\nThis function selects and returns part of the input array based on provided indices. It slices the array data starting from the index at the 'trip' position to the index at 'trip + 1' position from the list_dividing_limit.\n\n**Parameters**:\n- `array_data` (numpy.ndarray): A multidimensional input array from which data will be selected.\n- `trip` (int): The position in the list_dividing_limit list that will be used to define the starting index for data selection from array_data.\n- `list_dividing_limit` (List[int]): A list of index values. The function uses the index at position 'trip' and 'trip +1' as slicing indices for the array_data.\n\n**Returns**:\n- (numpy.ndarray): A sliced portion of the original array_data. It includes rows of array_data starting from index 'list_dividing_limit[trip]' up to, but not including, 'list_dividing_limit[trip+1]'. All columns from these rows are returned.\n\n**Example**:\n```python\nimport numpy as np\n\ndata = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\ntrip = 1\ndividing_limit = [0, 2, 4]\n\nprint(pick_a_trip(data, trip, dividing_limit))\n```\nExpected output:\n```\n[[ 4 5 6]\n [7 8 9]]\n```\nIn the example above, the function takes the numpy array 'data', selects rows starting from index 2 (corresponds to dividing_limit[trip]) up to but not including index 4 (corresponds to dividing_limit[trip+1]), thus returning the 2nd and 3rd rows of the array 'data'.",
    "32&@M.dycause_lib.cnts_prune.__str__": "**Signature**: \n```python\n   __str__(self)\n```\n**Function Summary**: \n\nThis method returns a string that provides the current state of an object by showing the count of properties: `cnt_promising, cnt_promising_not, cnt_not_sure,` and `cnt_initial`.\n\n**Parameters**: \n\nThere are no explicit parameters for this function. However, it implicitly uses instance variables of the class in which it's defined.\n\n- `self` (class instance): Used to access the count variables – `cnt_promising`, `cnt_promising_not`, `cnt_not_sure`, and `cnt_initial` of the class instance.\n\n**Returns**:\n\nThis function returns a string (str), which represents the various count variables of the class instance.\n\n- String format: 'Promising: [cnt_promising], PromisingNot: [cnt_promising_not], NotSure: [cnt_not_sure], Initial: [cnt_initial]'\n\n**Example**:\n\nConsider a class Example:\n\n```python\nclass Example:\n    def __init__(self):\n        self.cnt_promising = 1\n        self.cnt_promising_not = 2\n        self.cnt_not_sure = 3\n        self.cnt_initial = 4\n\n    def __str__(self):\n        return ('Promising: %05d, PromisingNot: %05d, NotSure: %05d, Initial: %05d'\n                % (self.cnt_promising, self.cnt_promising_not, self.cnt_not_sure, self.cnt_initial))\n\nexample_instance = Example()\nprint(str(example_instance))\n```\n\nOutput: \n\n```shell\nPromising: 00001, PromisingNot: 00002, NotSure: 00003, Initial: 00004\n```",
    "32&@M.dycause_lib.cnts_prune.__init__": "# API Documentation\n\n---\n\n### Signature \n\n`__init__(self, cnt_promising, cnt_promising_not, cnt_not_sure, cnt_initial)`\n\n### Function Summary\n\nThis is the constructor function for a class in Python. It initializes the class instance with four arguments: `cnt_promising`, `cnt_promising_not`, `cnt_not_sure`, and `cnt_initial`.\n\n### Parameters\n\n- `cnt_promising` (`int`). Specifying the count of promising results.\n- `cnt_promising_not` (`int`). Specifying the count of results which are not promising.\n- `cnt_not_sure` (`int`). Specifying the count of results which are uncertain.\n- `cnt_initial` (`int`). Specifying the initial count before sorting the results.\n\n### Returns\n\nThis is a constructor function. It does not return any value but initializes the parameters of the class instance.\n\n---\n\n### Example\n\n```python\nclass MyClass:\n\n    def __init__(self, cnt_promising, cnt_promising_not, cnt_not_sure, cnt_initial):\n        self.cnt_promising = cnt_promising\n        self.cnt_promising_not = cnt_promising_not\n        self.cnt_not_sure = cnt_not_sure\n        self.cnt_initial = cnt_initial\n\n# creates an instance of MyClass\nmy_instance = MyClass(10, 5, 15, 30)\n\n# prints the initialized cnt_promising value\nprint(my_instance.cnt_promising)    # Outputs: 10\n```",
    "31&@M.dycause_lib.get_max_proportion": "**Signature**\n```python\nget_max_proportion(n_sample, ordered_intervals)\n```\n\n**Function Summary**\n\nThe `get_max_proportion` function calculates the proportion of the instances in the `ordered_intervals` that are occupied within a given sample space represented by `n_sample`.\n\n**Parameters**\n\n- `n_sample` *(int)*: The total number of instances in the sample space. \n\n- `ordered_intervals` *(list of tuples)*: The list of tuples where each tuple represents an interval. Each tuple consists of two elements - the first one is insignificant, the second one is another tuple representing the 'start' and the 'end' of an interval.\n\n**Returns**\n\n- The function returns a `float` representing the proportion of the instances in the `ordered_intervals` that are occupied within a sample space. The value will range between 0.0 (no instances) and 1.0 (all instances).\n\n**Example**\n```python\nn_sample = 10\nordered_intervals = [('tuple 1', (0,5)), ('tuple 2', (5,7))]\nprint(get_max_proportion(n_sample, ordered_intervals))\n```\nExpected Output\n```python\n0.7\n```\nThis example first creates a vector of zeros of length 10 (as defined by `n_sample`). Then iterates over the `ordered_intervals` and sets the values to 1 for indices that fall into any interval specified by `ordered_intervals`. The function then calculates and outputs the proportion of ones in the sample space which is 0.7 in this case.",
    "31&@M.dycause_lib.get_overlay_count": "# API Documentation\n\n## Function Signature\n```python\nget_overlay_count(n_sample, ordered_intervals)\n```\n\n## Function Summary\nThe function `get_overlay_count` computes the overlay counts for a given set of ordered intervals on a specified sample set. The function increments the overlay count for each overlap, based on an interval's starting and ending points.\n\n## Parameters\n\n- `n_sample` (int): The total number of samples to consider for overlay counts computation. This parameter must be a positive integer value.\n\n- `ordered_intervals` (list of tuples): The list of ordered intervals for which the overlay counts are to be computed. Each tuple in the list consists of two elements where the first element is ignored and the second element is an iterable (like list or tuple) containing the start and end indices for the interval (both inclusive).\n\n## Returns\n\n- `overlay_counts` (numpy.ndarray): This is a 2D numpy array of shape `n_sample x 1`, containing the overlay counts for each index position in the sample space. The datatype of the elements is integer.\n\n## Example\n```python\nn_sample = 10\nordered_intervals = [('a', (2, 5)), ('b', (4, 7)), ('c', (1, 3)), ('d', (5, 8))]\n\nresult = get_overlay_count(n_sample, ordered_intervals)\nprint(result)\n```\n\nThis should output:\n```python\narray([[0],\n       [1],\n       [2],\n       [2],\n       [3],\n       [2],\n       [2],\n       [1],\n       [1],\n       [0]])\n```\nIn the above example, 4th index has the maximum overlap count of 3 because it is included in intervals 'a', 'b', and 'c'.",
    "20&@M.SPOT.bidSPOT.__init__": "# API Documentation\n\n## Signature\n```python\n__init__(self, q=1e-4, depth=10)\n```\n\n## Function Summary\nThis is an initialization function for an unidentified class. It sets initial values and defines several data structures to be used within the class. It initializes the probability, data, number of instances and depth of the class, along with multiple dictionaries meant to represent different values and thresholds, typically to be modified later in class methods.\n\n## Parameters\n1. **q** (float, optional): An initialization probability, meant to represent some sort of quantile. Default value is 0.0001.\n2. **depth** (int, optional): An integer defining the depth of some unspecified structure or process in the class. The default value is 10.\n\n## Returns\nThis function does not return any value. It is used to set default values for attributes of an instance when the instance is created.\n\n## Attributes\n1. **self.proba** (float): Represents the initialized probability.\n2. **self.data** (NoneType): Placeholder for future data assignment.\n3. **self.init_data** (NoneType): Placeholder for future initialization data assignment.\n4. **self.n** (int): Counter or placeholder for future number assignment.\n5. **self.depth** (int): Represents the depth set upon initialization.\n6. **self.extreme_quantile** (dict): Dictionary with 'up' and 'down' keys, each initially set to None.\n7. **self.init_threshold** (dict): Similar to `extreme_quantile`.\n8. **self.peaks** (dict): Dictionary meant to represent peak values 'up' and 'down', initially set to None.\n9. **self.gamma** (dict): A dictionary intended to represent some values of the class, initially set to None.\n10. **self.sigma** (dict): Another dictionary meant to represent some values of the class, initially set to None.\n11. **self.Nt** (dict): A dictionary with 'up' and 'down' values, set as int 0 initially.\n\n## Example\nThis function can't be called explicitly as it's an instance method and a constructor for a class. It will be automatically called when a new object for the class is created.\n\n```python\nclass ExampleClass:\n    def __init__(self, q = 1e-4, depth = 10):\n        self.proba = q\n        self.data = None\n        self.init_data = None\n        self.n = 0\n        self.depth = depth\n        nonedict =  {'up':None,'down':None}\n        self.extreme_quantile = dict.copy(nonedict)\n        self.init_threshold = dict.copy(nonedict)\n        self.peaks = dict.copy(nonedict)\n        self.gamma = dict.copy(nonedict)\n        self.sigma = dict.copy(nonedict)\n        self.Nt = {'up':0,'down':0}\n\n# Creating an object for the class\nobj = ExampleClass(q=0.002, depth=20)\n```\nIn this case, `obj.proba` would return 0.002 and `obj.depth` would return 20.",
    "20&@M.SPOT.dSPOT.__init__": "# API Documentation\n\n## Function Signature\n```python\n__init__(self, q, depth)\n```\n\n## Function Summary\nThis is the initialization function for a class. It is used to initialize an instance of the class. It initializes a bunch of self variables that can be used throughout all methods in the class. The initial values are mainly `None` with the exception of `self.proba` and `self.depth` that take values of the parameters `q` and `depth`, and `self.n` and `self.Nt` which are initialized to `0`.\n\n## Parameters\n\n* `self` (class instance): The instance of the class, used to access the variables that belong to the class.\n\n* `q` (float): Represents the probability, must be between 0 and 1 (inclusive). It's is used to initialize `self.proba`.\n  \n* `depth` (integer): Represents the depth of the algorithm, it should be a positive integer. It's used to initialize `self.depth`.\n\n## Returns\nThis function does not return anything, it is used purely for initialization purposes.\n\n## Example\n```python\nclass MyClass:\n    def __init__(self, q, depth):\n        self.proba = q\n        self.extreme_quantile = None\n        self.data = None\n        self.init_data = None\n        self.init_threshold = None\n        self.peaks = None\n        self.n = 0\n        self.Nt = 0\n        self.depth = depth\n\n# Initializing the class with q=0.9 and depth=5\nobj = MyClass(0.9, 5)\n\n# Printing initialized variables\nprint(obj.proba)         # Output: 0.9\nprint(obj.depth)         # Output: 5\nprint(obj.data)          # Output: None\nprint(obj.extreme_quantile)  # Output: None\n```",
    "20&@M.SPOT.biSPOT.__init__": "---\n### Signature\n```python\ndef __init__(self, q = 1e-4)\n```\n\n### Function Summary\nThe `__init__` function serves as a constructor for the `biSPOT` class. It initializes an instance of this class with a specified detection level (risk), sets initial values for all the instance variables, and prepares internal data structures.\n\n### Parameters\n- `q` (`float`, optional): This parameter represents the detection level or risk. It is used to determine the initial probability threshold for both 'up' and 'down' scenarios. The default value is `1e-4`.\n\n### Returns\nReturns a new instance of the `biSPOT` class, that contains the following attributes:\n\n- `proba` (`float`): It represents the probability value assigned with the detection level (`q`).\n- `data` (`NoneType`): It initially does not have any assigned value.\n- `init_data` (`NoneType`): It initially does not have any assigned value.\n- `n` (`int`): It is initially set to 0.\n- `extreme_quantile` (`dict`): A dictionary that initially contains `None` for keys 'up' and 'down'.\n- `init_threshold` (`dict`): A dictionary that initially contains `None` for keys 'up' and 'down'.\n- `peaks` (`dict`): A dictionary that initially contains `None` for keys 'up' and 'down'.\n- `gamma` (`dict`): A dictionary that initially contains `None` for keys 'up' and 'down'.\n- `sigma` (`dict`): A dictionary that initially contains `None` for keys 'up' and 'down'.\n- `Nt` (`dict`): A dictionary that initially contains 0 for keys 'up' and 'down'.\n\n### Example\n```python\nobj = biSPOT(q = 0.002)\n```\nThis will create an instance of `biSPOT` class with the detection risk level set to `0.002`. All other properties of the class are initialized as per the function definition.",
    "20&@M.SPOT.SPOT.__str__": "---\n\n# str() Method\n\n## Signature\n\n```python\ndef __str__(self)\n```\n\n## Function Summary\n\nThe `__str__()` method converts instance variables of a Streaming Peaks-Over-Threshold object into a formatted string. It provides information on various attributes of the object such as detection level, imported data, algorithm initialization and running status.\n\n## Parameters\n\nThe `__str__()` method does not accept any parameters.\n\n## Returns\n\n- **str** - The method returns a multi-line string that provides a comprehensive summary of the state of the Streaming Peaks-Over-Threshold object. This includes details about imported data, algorithm initialization, the initial threshold, and whether the algorithm has been run or not.\n\n## Example\n\n```python\nprint(streaming_pot_object)\n```\n\noutput:\n```bash\nStreaming Peaks-Over-Threshold Object\nDetection level q = 0.5\nData imported : Yes\n    initialization  : 10 values\n    stream : 20 values\nAlgorithm initialized : Yes\n    initial threshold : 5\nAlgorithm run : Yes\n    number of observations : 10 (50.00 %)\n```\n\nOn printing an instance of the Peaks-Over-Threshold object, this method will be implicitly called and it will return a formatted string that provides detailed information about the state of the object instance.\n\nNote: There's no explicit call made to the `__str__()` method in the example as print implicitly uses this method to display the object.\n\n---",
    "20&@M.SPOT.SPOT.__init__": "---\n**API Documentation for the SPOT object constructor**\n\n---\n\n**Signature**\n```python\n__init__(self, q = 1e-4)\n```\n\n**Function Summary**\n\nThis is the constructor for the SPOT object. It initializes several internal parameters and prepares the object for further operations.\n\n**Parameters**\n\n- `q` (float, optional): Detection level or risk. Defaults to 1e-4.\n\n**Returns**\n\n- `SPOT` object: Creates and returns a new SPOT object with the specified detection level. It initializes multiple parameters within the object such as `proba`, `extreme_quantile`, `data`, `init_data`, `init_threshold`, `peaks`, `n`, and `Nt`.\n\n**Attributes**\n\n- `proba` (float): Stores the provided detection level (risk).\n- `extreme_quantile` (None): Placeholder for the extreme quantile value.\n- `data` (None): Placeholder for the data to be processed.\n- `init_data` (None): Placeholder for the initial set of data.\n- `init_threshold` (None): Placeholder for the initial threshold value.\n- `peaks` (None): Placeholder for the peak values.\n- `n` (integer): A counter, initially set to 0.\n- `Nt` (integer): Another counter, initially set to 0.\n\n**Example**\n\n```python\n# Create a new SPOT object with a detection level of 1e-3\nspotObject = SPOT(q = 1e-3)\n```\n\nIn this example, we are creating a new instance of the SPOT class with a detection level of 1e-3. Subsequent operations would be carried out on the `spotObject` instance.",
    "12&@M.util_funcs.format_to_latex": "## **API Documentation**\n\n---\n\n### **Signature**\n```python\ndef format_to_latex(prkS, my_acc)\n```\n### **Function Summary**\nThis function is used to format the given inputs into a string that complies with LaTeX syntax, specifically for use within a table. The function converts 'prkS' into percentages and represents the average of 'prkS' and 'my_acc' in the same format before returning the entire string.\n\n### **Parameters**\n- `prkS: list` - A list of floating point numbers, each representing a value that is to be converted to a percentage.\n\n- `my_acc: float` - A single floating-point number that is to be converted to a percentage.\n\n### **Returns**\n- `ret: string` - The function returns a string where each element in the 'prkS' list along with the mean of 'prkS' and 'my_acc' are represented as percentages, formatted according to LaTeX syntax.\n\n### **Example**\n\n```python\nprk_values = [0.5, 0.6, 0.7]\nacc_value = 0.8\n\nformatted_string = format_to_latex(prk_values, acc_value)\nprint(formatted_string)\n```\n**Output**\n```python\n \" 50.00 & 60.00 & 70.00 & 60.00 80.00 \\\\\"\n```\n\nHere, each value from the list is multiplied by 100 and rounded off to two decimal places. The mean of the list values (which is 0.6 or 60 when multiplied by 100) is also appended, followed by 'my_acc' value (which is 0.8 or 80 when multiplied by 100). The '&' is used for column separation in LaTeX, and '\\\\\\\\' is a symbol for the new line in LaTeX.",
    "12&@M.util_funcs.format_to_excel": "---\n\n**Signature**: `format_to_excel(prkS, my_acc)`\n\n**Function Summary**: This function takes in a list of values (prkS) and an additional float value (my_acc), formats each value to 4 decimal places, calculates the mean (average) of the prkS list, and then appends the mean and my_acc to the end of the string. It returns the complete string. \n\n**Parameters**:\n- `prkS`: list of floats; This is a list of floating-point numbers which are each formatted to 4 decimal places and then converted to a string.\n- `my_acc`: float; A floating-point number, also formatted to 4 decimal places and appended to the end of the string.\n\n**Returns**:\n- String; A string that contains the prkS values, each formatted to 4 decimal places, followed by the mean (average) of the prkS values and the input parameter my_acc (both also formatted to 4 decimal places).\n\n**Example**:\n\n```python\nimport numpy as np\nprint(format_to_excel([1.23456789, 2.3456789, 3.456789], 0.123456789))\n```\n\nOutput:\n\n```python\n' 1.2346 2.3457 3.4568 2.3457 0.1235'\n```\n\n---\nThe formatted values are prefixed with a space in the returned string.",
    "9&@M.cloud_ranger.build_graph_pc": "## API Documentation\n\n---\n\n### Signature\n```python\nbuild_graph_pc(data: List[List[int]], alpha: float) -> List[List[int]]\n```\n### Function Summary\n\nThe `build_graph_pc` function implements the PC algorithm to find graphical models that satisfy a specific data set. The algorithm is carried out using a Robust Regression Test, with an alpha value determining the statistical significance level for the test of conditional independence. The output is a 2D adjacency matrix representing the graph edges. \n\n### Parameters\n\n- `data`(*List[List[int]]*): A two dimensional list represents a dataset. Each inner list represents a record, where each element represents the value of a feature for that record.\n\n- `alpha`(*float*): The statistical significance level used in the Robust Regression Test during the IC algorithm. The value needs to be between 0 and 1.\n\n### Returns\n\n- `List[List[int]]`: A two dimensional list represents an adjacency matrix of the formed graph. Each `1` in the matrix represents an edge between the nodes denoted by the row and column index. The size of the matrix is `n x n` where `n` is the number of nodes in the graph.\n\n### Example\n\n```python\ndata = [[1, 2],[3, 4],[5, 6],[7, 8]]\nalpha = 0.05\nprint(build_graph_pc(data, alpha))\n\n# Output\n[[0, 1],\n [1, 0]]\n```\nIn this case, the function `build_graph_pc` is creating a graph with edges between all nodes as determined by the PC algorithm with a significance level of 0.05. The result is a 2x2 adjacency matrix where `1` denotes an edge between two nodes.\n",
    "8&@M.util_funcs.print_prk_acc": "# Function API Documentation\n\n## Signature\n```python\nprint_prk_acc(prkS, acc)\n```\n\n## Function Summary\nThe `print_prk_acc` function is used to create a tabular formatted print-out that contains prkS at different indexes, the average of prkS and acc.\n\n## Parameters\nThe function `print_prk_acc` accepts two parameters: \n1. `prkS` (**type: list**): A list of floats. This list should contain precision-recall values at different points.\n2. `acc` (**type: float**): A float representing the accuracy of a model performance.\n\n## Returns\nThis function returns **None**. It is used for its side effect of printing a table to the console, with relevant statistics and data laid out in a readable tabular format.\n\n## Example\n\nExample to call the function:\n```python\nprkS = [0.5, 0.6, 0.7]\nacc = 0.65\nprint_prk_acc(prkS, acc)\n```\n\nExpected Output:\n```bash\n  PR@1    PR@2    PR@3   PR@Avg      Acc\n------  ------  ------  -------  -------\n0.5000  0.6000  0.7000   0.6000   0.6500\n```\nThis table displays the values of prkS at each index, the average of all prkS values, and the accuracy. The floating point values are formatted with 4 digits after the decimal place.",
    "8&@M.util_funcs.pr_stat": "# `pr_stat` Function API Documentation\n\n---\n\n## **Signature**\n`pr_stat(scoreList, rightOne, k=5)`\n\n---\n\n## **Function Summary**\nThe `pr_stat` function calculates the precision of predictions at each position from 1 to a specified threshold `k` in a ranked list of scores. The function returns a list of calculated precision values for each position up to `k`.\n\n---\n\n## **Parameters**\n\n1. `scoreList: list`  \nA list of scores ranked in order that represent predicted classifications or relevancies of various items. Higher scores should denote better/more accurate predictions\n\n2. `rightOne: int`  \nThe index of the correct prediction in the `scoreList`\n\n3. `k: int, optional`  \nThe cutoff threshold for the calculation of precision scores. Default is 5.\n\n---\n\n## **Returns**\n\n- `list`:  \nReturns a list of precision values for each position up to `k`.\n\n---\n\n## **Example**\n\n```python\nscores = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\r\ncorrect_index = 3\r\nk_value = 5\r\nprint(pr_stat(scores, correct_index, k_value))\r\n# Output: [0, 0, 0, 1.0, 0.8]\r\n```\nIn the above example, we are calculating the precision score for the 1st to 5th rankings in the input score list. The index of the correct prediction is 3 (0-indexed), our function then checks the top 1 to 5 predictions to see if they include the correct prediction and calculates precision at each level. \n\n_Note: The function `prCal()` called within `pr_stat()` is not defined within the context of this task. In real usage, it would presumably calculate the precision score at the `k`th level._",
    "8&@M.util_funcs.prCal": "## API Documentation\n\n---\n\n### **Signature**\n```python\nprCal(scoreList: List[Tuple[Any, float]], prk: int, rightOne: List[Any]) -> float\n```\n\n---\n\n### **Function Summary**\nThis function calculates Precision at k (prk), which is a measure of the relevance of the k items that are most highly ranked. It is especially useful for systems that return a ranked list of results.\n\n---\n\n### **Parameters**\n- **`scoreList`** : *List[Tuple[Any, float]]* \\\nList of tuples, each tuple consists of two elements: the first element is the node (any data type) and the second element is its associated score (float).\n\n- **`prk`** : *int* \\\nThe top 'n' nodes to consider for the prk computation.\n\n- **`rightOne`** : *List[Any]* \\\nThe list of ground truth nodes or the correct results, against which the scoreList nodes are compared.\n\n---\n\n### **Returns**\n- **`return value`** : *float* \\\nReturns the calculated Precision at K value.\n\n---\n\n### **Example**\n```python\nscore_list = [('node1', 0.2), ('node2', 0.1), ('node3', 0.3), ('node4', 0.7)]\nprk = 3\nright_one = ['node1', 'node2', 'node4']\nprint(prCal(score_list, prk, right_one)) #outputs 0.6666666666666666\n```\n\n---\n\nIn this example, the function considers the top 3 nodes based on the order they appear in `score_list`. From 'node1', 'node2', and 'node3', only 'node1' and 'node2' are present in the ground truth. Hence, the Precision at K is 2/3 = 0.6666666666666666.",
    "8&@M.util_funcs.my_acc": "# my_acc Function\n\n## Signature\n```Python\nmy_acc(scoreList, rightOne, n=None)\n```\n\n## Function Summary\nThe function calculates accuracy for Root Cause Analysis (RCA), considering multiple causes, refined from the Accuracy (Acc) metric in the TBAC paper.\n\n## Parameters\n\n1. **scoreList** (*List of Tuples*): A list of tuples where the first value of each tuple corresponds to the score of a particular node. It is expected that higher scores occur earlier in the list (i.e., the list is sorted in descending order by score). \n\n2. **rightOne** (*List*): A list of correct causes where each cause corresponds to the first element of a tuple in `scoreList`. The length of `rightOne` should not exceed that of `scoreList`.\n\n3. **n** (*int, optional*): An integer that represents a cut-off rank in the `scoreList`. If `n` is not provided, the length of `scoreList` is used.\n\n## Returns\n\n**accuracy** (*float*): A float value that represents the RCA accuracy. This value is normalized within the range [0,1], where a higher score indicates better accuracy.\n\n## Example\n\n```Python\n# Suppose we have three nodes with scores 1.0, 0.9, and 0.8 respectively\nscoreList = [(1.0, \"extra\"), (0.9, \"extra\"), (0.8,\"extra\")]\n\n# Assume the correct cause corresponds to the second node\nrightOne = [0.9]\n\nmy_acc(scoreList, rightOne)\n```\n\n**Output**:\n\n    0.6666666666666666\n\nIn this example, the correct cause is found at rank 2 in `scoreList`, resulting in an accuracy of approximately 0.67 when calculated with the formula implemented in `my_acc`.",
    "7&@M.util_funcs.saveToExcel": "**Signature**\n```python\ndef saveToExcel(fileName: str, a: list):\n```\n\n**Function Summary**\nSaves a 2D list to an excel file. If the folder for the specified file doesn't exist, it will be created automatically.\n\n**Parameters**\n\n- `fileName` (str): The name of the file (including the path) to which the list `a` will be saved. The file will be saved in the '.xlsx' format.\n  \n- `a` (list): A 2D list where each sub-list represents a row in the Excel file. All elements should ideally be of a datatype that can be saved to an Excel cell (like integers, floats, strings, etc.).\n\n**Returns**\nThis function does not return anything but saves the provided 2D list into the specified Excel file.\n\n**Example**\n```python\ndata_to_save = [\n    [\"Name\",\"Age\",\"Country\"],\n    [\"Alice\", 30, \"USA\"],\n    [\"Bob\", 25, \"Canada\"],\n    [\"Charlie\", 35, \"UK\"]\n]\nsaveToExcel(\"output/mydata.xlsx\", data_to_save)\n```\nIn this example, `saveToExcel` will create a new Excel file at the location \"output/mydata.xlsx\". The file will have four rows, the first row being headers (\"Name\",\"Age\",\"Country\") and the next three lines are the data.\n\n**Note**\nThis function utilizes the openpyxl library for creating Excel files. Make sure to install the library in your python environment by using pip:\n```python\npip install openpyxl\n```",
    "7&@M.util_funcs.readExl": "# API Documentation\n\n## **Function Signature**\n```python\ndef readExl(fileName):\n```\n\n## **Function Summary**\nThis function is used to load and read data from an Excel file. The data is then transformed into a Python nested list where each list represents a row, while locations inside this list represent column entries. The function reads from 'Sheet1' by default.\n\n## **Parameters**\n- **fileName** *(str)*: The filename of the Excel document that needs to be read. The path to the file should either be absolute or relative to the current working directory where the script is executed.\n\n## **Returns**\n- **data** *(list of lists)*: A nested list where each internal list represents a row of the Excel file and each element in these lists represents a data value from a respective column in the same row. If the file does not exist, an error message will be printed, and the program will exit.\n\n## **Example**\n\nAssuming we have an Excel file named 'example.xlsx' in the same directory. The file only contains one sheet ('Sheet1') with the following data:\n\n|   | A | B | C |\n|---|---|---|---|\n| 1 | 1 | 2 | 3 |\n| 2 | 4 | 5 | 6 |\n| 3 | 7 | 8 | 9 |\n\nCode:\n\n```python\ndataMatrix = readExl('example.xlsx')\nprint(dataMatrix)\n```\n\nOutput:\n\n```commandline\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n```\n\n**Note**: This function depends on the third-party package `openpyxl`, and will not work if it hasn't been installed. Install it using the pip package manager by running `pip install openpyxl` in your command prompt or terminal.",
    "32&@M.dycause_lib.update_bound": "## API Documentation\n\n### Signature\n```python\nupdate_bound(x, pre_res2down, pre_res2djoint, pre_res2down_ssr_upper, pre_res2down_ssr_lower, pre_res2djoint_ssr_upper, pre_res2djoint_ssr_lower, lag, step, addconst=True, verbose=False)\n```\n\n### Function Summary\nThis function computes the upper and lower bounds for the sum of square residuals (SSR) based on the previous step's SSR.\n\n### Parameters\n- `x` (type: N-dimensional array): The data used to supplement the error of unknown fitting points.\n- `pre_res2down` (type: Model object): The semi-model result from the previous step.\n- `pre_res2djoint` (type: Model object): The full model result from the previous step.\n- `pre_res2down_ssr_upper` (type: float): The upper SSR for partial (down) model from the previous step.\n- `pre_res2down_ssr_lower` (type: float): The lower SSR for partial (down) model from the previous step.\n- `pre_res2djoint_ssr_upper` (type: float): The upper SSR for full (joint) model from the previous step.\n- `pre_res2djoint_ssr_lower` (type: float): The lower SSR for full (joint) model from the previous step.\n- `lag` (type: int): The lagged data. \n- `step` (type: int): The number of samples used in each step, which is used to determine the number of samples required to supplement the error. \n- `addconst` (type: bool, optional):  Whether to add a constant to the model. By default, addconst is True. \n- `verbose` (type: bool, optional): When set to true, the function provides additional details. By default, verbose is False. \n\n### Returns\n- Returns a tuple of five elements:\n    - `res2down_ssr_upper` (type: float): Updated upper SSR for partial (down) model.\n    - `res2down_ssr_lower` (type: float): Updated lower SSR for partial (down) model.\n    - `res2djoint_ssr_upper` (type: float): Updated upper SSR for full (joint) model.\n    - `res2djoint_ssr_lower` (type: float): Updated lower SSR for full (joint) model.\n    - `res2djoint_df_resid` (type: int): Degree of freedom for residuals in the full model. \n\n### Example\n```python\n# This is a hypothetical example as the actual usage would require valid model objects `pre_res2down` and `pre_res2djoint`\nx = np.array([1, 2, 3, 4, 5])\nlag = 1\nstep = 2\nupdate_bound(x, pre_res2down, pre_res2djoint, 10, 5, 15, 8, lag, step)\n```\nOutput:\n```\n(18.0, 5, 22.5, 8, 3)\n```",
    "32&@M.dycause_lib.test_granger": "### API Documentation\n\n---\n\n**Signature**\n```python\ntest_granger(array_data, array_data_head, feature, target, lag, significant_thres)\n```\n\n**Function Summary**\n\nThis function is used to perform a Granger Causality Test, which is a statistical hypothesis test for determining whether one time series is useful in forecasting another. \n\n**Parameters**\n\n- `array_data` (numpy array) : A two-dimensional numpy array that contains the data for both the target and feature. \n- `array_data_head` (list) : A list of headers for the data in the `array_data` parameter. \n- `feature` (str) : The name of the feature variable for the Granger test. \n- `target` (str) : The name of the target variable for the Granger test.\n- `lag` (int) : The number of lag observations to include in the model.\n- `significant_thres` (float) : A value to be used as a cut-off for determining statistical significance in the Granger Causality Test.\n\n**Returns**\n\nThis function doesn't have a return value. It prints the outcome of the Granger Causality Test and also prints out the total runtime of the test. \n\n**Example**\n```python\nimport numpy as np\narray_data = np.array([[1,2,3,4,5],[2,3,4,5,6],[3,4,5,6,7]])\narray_data_head = ['a', 'b', 'c', 'd', 'e']\ntest_granger(array_data, array_data_head, 'a', 'b', 2, 0.05)\n```\nThis will run a Granger Causality Test with 'a' as the feature, 'b' as the target, a lag of 2, and a significance threshold of 0.05. It will print out the outcome of the Causality Test and the runtime of the test.",
    "32&@M.dycause_lib.pick_a_date": "# API Documentation:\n\n---\n\n## Function Signature:\n\n``` python\ndef pick_a_date(array_data, array_data_head, date)\n```\n\n## Function Summary:\n\nThe function `pick_a_date()` is used to filter an array of data, selecting all rows from the array that corresponds to a specified date found in the array header.\n\n## Parameters:\n\n1. `array_data` _(np.ndarray)_ : It is a two-dimensional numpy array that holds the data to be filtered. \n\n2. `array_data_head` _(list)_ : A list of string each containing headers of the `array_data`. This list must contain a string 'date_d' which signifies the entry of dates in `array_data`.\n\n3. `date` _(str/int)_ : Expected to be a date representation in integer or string format that needs to be picked from the `array_data`.\n\n## Returns:\n\n- **array_data** _(np.ndarray)_ : The function returns a subset of `array_data` where the 'date_d' equals the input `date`. If no matching date is found, an empty array is returned. The returned array also contains only data of the specified date.\n\n## Example:\n\n``` python\ndata = np.array([[\"20210505\", \"John\", \"123.4\"], [\"20210506\", \"Doe\", \"567.8\"], [\"20210505\", \"Smith\", \"910.11\"]])\nheaders = [\"date_d\", \"name\", \"amount\"]\npick_a_date(data, headers, '20210505')\n```\nExpected outcome:\n```\narray([[\"20210505\", \"John\", \"123.4\"], [\"20210505\", \"Smith\", \"910.11\"]])\n```",
    "32&@M.dycause_lib.loop_granger": "## Function Name\nloop_granger\n\n**Signature**:\n`loop_granger(array_data, array_data_head, path_to_output, feature, target, significant_thres, test_mode, trip, lag, step, simu_real, max_segment_len, min_segment_len, verbose=True, return_result=False)`\n\n**Function Summary**:\n\nThis function performs the Granger Causality test to determine if one time-series is useful in forecasting another. It executes a series of optimizations on the hypothesis tests and outputs the p-value results into a CSV file.\n\n**Parameters**:\n\n- `array_data` **(Array-like)**: The input time series data where each column represents a variable.\n- `array_data_head` **(Array-like)**: The header of the columns or names of the variables in the input time series data.\n- `path_to_output` **(str)**: The file path where the output result is to be stored.\n- `feature` **(str)**: The source variable name in Granger causality (feature -> target).\n- `target` **(str)**: The target variable name in Granger causality (feature -> target).\n- `significant_thres` **(float)**: The significant level for hypothesis testing.\n- `test_mode` **(str)**: The optimization mode for Granger causality test.\n- `trip` **(int)**: Select which segment of the time series to be used.\n- `lag` **(int)**: The maximum lag interval in Granger causality test.\n- `step` **(int)**: The step size/minimum interval for segmenting the time series.\n- `simu_real` **(str)**: Whether the data is a simulation or real data.\n- `max_segment_len` **(int)**: The maximum allowable length of segments for Granger Causality test.\n- `min_segment_len` **(int)**: The minimum allowable length of segments for Granger Causality test.\n- `verbose`(optional) **(bool)**: Whether to print detailed information of the process. Default is True.\n- `return_result`(optional) **(bool)**: Whether to return the p-value matrix as value. Default is False.\n\n**Returns**:\nIf `return_result` is True, return value is a tuple consisting of following values:\n\n- Total time taken for execution **(float)**.\n- Time taken for Granger Causality test **(float)**.\n- Time for testing stationary **(float)**.\n- Result p value matrix of YX **(numpy.ndarray)**.\n- Result p value matrix of XY **(numpy.ndarray)**.\n\nIf `return_result` is False, return value is a tuple of total time, Granger Causality test time and stationary test time.\n\n**Example**:\n\n```python\ndata = np.random.rand(10000, 2)\ndata_head = ['feature1', 'feature2']\noutput = './'\nfeature = 'feature1'\ntarget = 'feature2'\nthres = 0.05\nmode = 'fast_version_3'\ntrip = 1\nlag = 2\nstep = 3\nsimu_real = 'real'\nmax_len = 500\nmin_len = 10\n\ntotal_time, granger_time, adf_time = loop_granger(data, data_head, output, feature,\ntarget, thres, mode, trip, lag, step, simu_real, max_len, min_len)\n\n```\nIn this example, a random 2D array with 10000 rows is used as input for Granger causality test with given parameters. It returns total execution time, Granger test time, and stationary test time.\n",
    "32&@M.dycause_lib.interpolation": "## API Documentation\n\n### Signature\n`interpolation(array_data, array_data_head)`\n\n### Function Summary\nThis function performs linear interpolation on given data with a time interval of 3 seconds to fill in missing data points.\n\n### Parameters\n\n1. `array_data` [type: numpy.ndarray]: A 2D numpy array that contains numeric data. Each item in the array represents a data sample and contains multiple elements including `date_y`, `date_m`, `date_d`, `time_h`, `time_m` and `time_s` in sequence.\n\n2. `array_data_head` [type: list]: A list containing strings representing the headers of the `array_data`. These would be `['date_y', 'date_m', 'date_d', 'time_h', 'time_m', 'time_s']` in the case of this function.\n\n### Returns\n\n- `new_array_data` [type: numpy.ndarray]: A new 2D numpy array containing the processed and interpolated data. The scale of each data sample in the array is rounded to 5 decimal places.\n\n### Example\n```python\nfrom datetime import datetime\nimport numpy as np\ndata = [[2022, 1, 25, 14, 35, 43], [2022, 1, 25, 14, 35, 47], [2022, 1, 25, 14, 35, 50]]\narray_data = np.array(data)\narray_data_head = ['date_y', 'date_m', 'date_d', 'time_h', 'time_m', 'time_s']\nnew_data = interpolation(array_data, array_data_head) \nprint(new_data)\n```\nIn this example, the function will return a new array where the timing between consecutive data points has been interpolated to be 3 seconds apart. The output will be:\n```\n[[2022.      1.     25.     14.     35.     43.  ]\n [2022.      1.     25.     14.     35.     46.  ]\n [2022.      1.     25.     14.     35.     47.  ]\n [2022.      1.     25.     14.     35.     50.  ]]\n```",
    "32&@M.dycause_lib.grangercausalitytests_check_F_upper_lower": "## API Documentation\n\n### Signature\n```python\ngrangercausalitytests_check_F_upper_lower(x,lag,pre_res2down,pre_res2djoint,pre_res2down_ssr_upper,pre_res2down_ssr_lower, pre_res2djoint_ssr_upper,pre_res2djoint_ssr_lower,pre_res2djoint_df_resid, significant_thres,step,cnt,cnt_prune,addconst=True, verbose=False)\n```\n\n### Function Summary\nThis function tests for Granger causality in a time series data using F statistics. It also performs pruning based on upper and lower bounds of the F-test and keeps track of the pruning decisions.\n\n### Parameters\n1. `x` (numpy.ndarray): The time series data for the test.\n2. `lag` (int): The number of lag observations to include in the test.\n3. `pre_res2down` (statsmodels.regression.linear_model.RegressionResultsWrapper): Previous model of the restricted hypothesis.\n4. `pre_res2djoint` (statsmodels.regression.linear_model.RegressionResultsWrapper): Previous model of the joint hypothesis.\n5. `pre_res2down_ssr_upper` (float): Upper bound of the restricted model's residual sum of squares.\n6. `pre_res2down_ssr_lower` (float): Lower bound of the restricted model's residual sum of squares.\n7. `pre_res2djoint_ssr_upper` (float): Upper bound of the joint model's residual sum of squares.\n8. `pre_res2djoint_ssr_lower` (float): Lower bound of the joint model's residual sum of squares.\n9. `pre_res2djoint_df_resid` (int): Degrees of freedom of the residuals in the previous joint model.\n10. `significant_thres` (float): The significance level threshold for the F-test.\n11. `step` (int): The step size to take when fitting new data.\n12. `cnt` (int): A counter to track the number of pruning steps.\n13. `cnt_prune` (Counter): A counter object to track the count of different pruning results.\n14. `addconst` (bool, optional): A flag indicating whether to add a constant in the regression. Default is True.\n15. `verbose` (bool, optional): A flag to control the verbosity of the function. If True, it returns more detailed output. Default is False.\n\n### Returns\nThis function returns a tuple containing:\n\n1. `p_value` (float): The p-value from the F-test.\n2. `res2down` (statsmodels.regression.linear_model.RegressionResultsWrapper): The results of the restricted model.\n3. `res2djoint` (statsmodels.regression.linear_model.RegressionResultsWrapper): The results of the joint model.\n4. `res2down_ssr_upper` (float): The upper bound of the restricted model's residual sum of squares.\n5. `res2down_ssr_lower` (float): The lower bound of the restricted model's residual sum of squares.\n6. `res2djoint_ssr_upper` (float): The upper bound of the joint model's residual sum of squares.\n7. `res2djoint_ssr_lower` (float): The lower bound of the joint model's residual sum of squares.\n8. `res2djoint_df_resid` (int): The degree of freedom of residuals of the joint model.\n9. `cnt` (int): The updated counter.\n10. `cnt_prune` (Counter): The updated counter object enclosing the count of different pruning results.\n\n### Example\n```python\nfrom collections import Counter\nfrom statsmodels.tsa.stattools import grangercausalitytests\n\nx = np.random.rand(100,2)\nlag = 2\naddconst = True\nverbose = False\npre_res2down, pre_res2djoint = None, None\npre_res2down_ssr_upper, pre_res2down_ssr_lower = None, None\npre_res2djoint_ssr_upper, pre_res2djoint_ssr_lower = None, None\npre_res2djoint_df_resid = None\nsignificant_thres = 0.05\nstep = 1\ncnt = -1\ncnt_prune = Counter()\n\nresult = grangercausalitytests_check_F_upper_lower(x,\n                                                   lag,\n                                                   pre_res2down,\n                                                   pre_res2djoint,\n                                                   pre_res2down_ssr_upper,\n                                                   pre_res2down_ssr_lower,\n                                                   pre_res2djoint_ssr_upper,\n                                                   pre_res2djoint_ssr_lower,\n                                                   pre_res2djoint_df_resid,\n                                                   significant_thres,\n                                                   step,\n                                                   cnt,\n                                                   cnt_prune,\n                                                   addconst,\n                                                   verbose)\n```\nNote: The",
    "32&@M.dycause_lib.grangercausalitytests_check_F_upper": "## API Documentation\n\n---\n\n### Signature\n\n```python\ngrangercausalitytests_check_F_upper(x, lag, pre_res2down, pre_res2djoint, pre_res2down_ssr_upper, pre_res2djoint_ssr_lower, pre_res2djoint_df_resid, significant_thres, step, cnt, addconst=True, verbose=False)\n```\n\n---\n\n### Function Summary\nThis function checks the upper bound of F (denoted as F_upper) in Granger causality tests and performs necessary calculations based on the value of 'cnt'. If 'cnt' is -1, the function performs initialization. If 'cnt' is not -1, the function calculates 'F_upper' and performs further operations based on a given threshold.\n\n---\n\n### Parameters\n\n- **x** (_Array-like_)  \n  Multi-dimensional array-like data structure where each row represents an observation and each column represents a variable.\n\n- **lag** (_int_)  \n  The number of lagged observations to include.\n\n- **pre_res2down**, **pre_res2djoint** (_Objects_)  \n  The parameters from previous regression models.\n\n- **pre_res2down_ssr_upper**, **pre_res2djoint_ssr_lower** (_float_)  \n  The parameters used for calculating the F test statistic.\n\n- **pre_res2djoint_df_resid** (_int_)  \n  Degrees of freedom of the residuals of joint data.\n\n- **significant_thres** (_float_)   \n  Threshold for the p-value to be accepted as statistically significant.\n\n- **step** (_int_)  \n  How many steps to predict (forward)\n\n- **cnt** (_int_)  \n  Counter for how many iterations this function has run.\n\n- **addconst** (_bool_, optional, default=True)  \n  If True, a constant will be added to the lagged data. Otherwise, no constant will be added.\n\n- **verbose** (_bool_, optional, default=False)  \n  If True, the function will print detailed information about the process.\n\n---\n\n### Returns\n\nA tuple of:\n- **p_value** (_float_)  \n  The probability metric given by the F test.\n\n- **res2down**, **res2djoint** (_Objects_)  \n  Regression result objects.\n\n- **res2down_ssr_upper**, **res2djoint_ssr_lower** (_float_)  \n  Values used for calculating the F test statistic.\n\n- **res2djoint_df_resid** (_int_)  \n  Degrees of freedom of the residuals of joint data.\n\n- **cnt+1** (_int_)  \n  The updated counter after running this function's iteration.\n\n---\n\n### Example\n\n```python\nx = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nprint(grangercausalitytests_check_F_upper(x,\n                                          lag=1,\n                                          pre_res2down=None,\n                                          pre_res2djoint=None,\n                                          pre_res2down_ssr_upper=None,\n                                          pre_res2djoint_ssr_lower=None,\n                                          pre_res2djoint_df_resid=None,\n                                          significant_thres=0.05,\n                                          step=1,\n                                          cnt=-1,\n                                          addconst=True,\n                                          verbose=False))\n```\n\n>_Note: You are expected to provide previous regression results for parameters `pre_res2down` and `pre_res2djoint`, and provide parameters for previous test statistic calculation. Here we used None just for demonstration._",
    "32&@M.dycause_lib.grangercausalitytests": "# API Documentation\n\n---\n\n## **Function Signature**\n```python\ndef grangercausalitytests(x, lag, addconst=True, verbose=False):\n```\n\n## **Function Summary**\n\nThis function performs a Granger causality test. It specifically applies the F test for the specified lag, in contrast with the statsmodels method that undertakes four types of hypothesis tests for every interval from 1 to lag, being less efficient.\n\nA Granger causality model assumes that past values of time series x can be used to predict future values of another series y. If the inclusion of past values of x in a prediction model reduces the prediction error for future y values, then x is considered to have Granger-caused y.\n\n## **Parameters**\n\n* **x**: array-like - The time series data to conduct Granger causality test on.\n  \n* **lag** (int): The number of lags considered in the test.\n\n* **addconst** (bool, optional): If true, adds a constant to the estimated model (default is True).\n\n* **verbose** (bool, optional): If true, the function will print intermediate results (default is False).\n\n## **Returns**\n\nThis function returns a tuple of three items:\n\n* **p_value** (float): The p-value obtained from the F test, used for hypothesis testing in the Granger causality test. If p_value is less than a certain significance level (say 0.05), we reject the null hypothesis that x does not Granger-cause y.\n\n* **res2down** (RegressionResultsWrapper): The results instance for the restricted model, obtained by fitting a regression to 'dta' and 'dtaown'.\n\n* **res2djoint** (RegressionResultsWrapper): The results instance for the unrestricted model, obtained by fitting a regression to 'dta' and 'dtajoint'.\n\n## **Example**\n```python\n# Assuming 'tsdata' is the time series data to be tested with a lag value 2\np_value, res2down, res2djoint = grangercausalitytests(tsdata, 2)\nprint(\"P-Value: \", p_value)\n```\nNote: You should replace 'tsdata' with your actual dataset.",
    "32&@M.dycause_lib.get_lagged_data": "## API Documentation\n\n### Signature\n```python\nget_lagged_data(x: np.array, lag: int, addconst: bool, verbose: bool) -> Tuple[np.array, np.array, np.array]\n```\n\n### Function Summary\nThis function generates a lagged matrix for a given data array. For an input array `x=[Y, X]`, it generates a data set where each row contains the sequence: `[Y_t Y_{t-1} Y_{t-2} ... Y_{t-lag} X_{t-1} X_{t-2} ... X_{t-lag}]`.\n\n### Parameters\n- **x** (_np.array_): A numpy array of n observations.\n- **lag** (_int_): The number of lags to consider.\n- **addconst** (_bool_): If set to True, a constant column (of ones) is added to the input array.\n- **verbose** (_bool_): If set to True, the function will print output in verbose mode.\n\n### Returns\n- **dta** (_np.array_): The complete lagged matrix.\n- **dtaown** (_np.array_): The lagged matrix only containing its own past, i.e., `[Y_{t-1} Y_{t-2} ... Y_{t-lag}]`.\n- **dtajoint** (_np.array_): The lagged matrix containing its own and other variables at past lag time, i.e., `[Y_{t-1} Y_{t-2} ... Y_{t-lag} X_{t-1} X_{t-2} ... X_{t-lag}]`.\n\n### Example\n```python\nimport numpy as np\nfrom statsmodels.tsa.tsatools import lagmat2ds\nfrom statsmodels.tools.tools import add_constant\n\n# define the data array\nx = np.array([[1, 2, 3, 4, 5],[2, 4, 6, 8, 10]])\n\n# call the function\ndta, dtaown, dtajoint = get_lagged_data(x, 2, True, False)\n\n# print the outputs\nprint('dta:', dta)\nprint('dtaown:', dtaown)\nprint('dtajoint:', dtajoint)\n```\nThis code will create a lagged array of the input data for 2 lags, appending a constant column to the input array, but without producing verbose output.\n",
    "32&@M.dycause_lib.fit_regression": "# API Documentation\n\n## Signature\n```\nfit_regression(dta, dtaown, dtajoint)\n```\n\n## Function Summary\nPerform linear fitting twice on the given data sets, for both partial models and complete models, and return the results. This is function uses Ordinary Least Squares (OLS) for the linear fitting.\n\n## Parameters\n- `dta` : array of shape (n_samples, n_features), where n_samples is the number of samples and n_features is the number of features. The data for which regression needs to be performed and results returned. The number of samples in the dta array corresponds to the number of observations or instances.\n- `dtaown` : array of shape (n_samples, n_features), identical in structure to `dta`. This is the data for the base model.\n- `dtajoint` : array of shape (n_samples, n_features), identical in structure to `dta`. This is the data for the combined model which includes both `dta` and `dtaown`.\n\n## Returns\n- `res2down` : RegressionResult object. Output of OLS fit on the `dta` and `dtaown`.\n- `res2djoint` : RegressionResult object. Output of OLS fit on the `dta` and `dtajoint`.\n\nEach RegressionResult object has attributes such as params (model parameters), and methods like summary() which returns a string that summarizes the fit of the model.\n\n## Example\n\n#### Sample Call\n```python\nimport numpy as np\nfrom statsmodels.api import OLS\n\ndta_np = np.array([[1,2], [3,4], [5,6]])\ndtaown_np = np.array([[7,8], [9,10], [11,12]])\ndtajoint_np = np.array([[13,14], [15,16], [17,18]])\n\nres2down, res2djoint = fit_regression(dta_np, dtaown_np, dtajoint_np)\nprint(res2down.params)\nprint(res2djoint.params)\n```\n\n#### Expected Output\n```\n[0.32758621 1.13793103]\n[0.31578947 1.13157895]\n```\nPlease note that this is a hypothetical example and the actual output values may vary based on the specific array data passed into `fit_regression()`. Also, this function requires `statsmodels.api.OLS` for fitting the regression model. \n",
    "32&@M.dycause_lib.divide_the_trip": "### Function \n\n    divide_the_trip(array_data, array_data_head, time_diff_thres)\n\n### Summary\n\nThis function identifies indices in a given data array where the time difference between subsequent data entries exceeds a specified threshold.    \n\n### Parameters\n\n- `array_data` (numpy.ndarray): A two-dimensional numpy array where each row represents a sample of data. Each sample is expected to include date and time information.\n- `array_data_head` (list): A list of strings indicating the column headers of the `array_data`. It must include 'date_y', 'date_m', 'date_d', 'time_h', 'time_m', and 'time_s', which represent the year, month, day, hour, minute, and second, respectively.\n- `time_diff_thres` (int): An integer that specifies the threshold of time difference (in seconds). If the time difference between subsequent data rows exceeds this threshold, the index of the later data row is considered a dividing point.\n\n### Returns\n\n- `list_dividing_limit` (list): A list of indices in `array_data` where the time difference between subsequent rows exceeds `time_diff_thres`. The first and last elements of the list are always 0 and the length of `array_data`, respectively.\n- `list_time_diff` (list): A list specifying the actual time difference (in seconds) at each dividing point. The list is in the same order as `list_dividing_limit` and the first element is always 0.\n\n### Example\n\n```Python\ndata = np.array([\n    ['2000', '1', '1', '0', '0', '0'],\n    ['2000', '1', '1', '0', '0', '30'],\n    ['2000', '1', '1', '0', '1', '0'],\n    ['2000', '1', '1', '0', '3', '0']\n])\nheaders = ['date_y', 'date_m', 'date_d', 'time_h', 'time_m', 'time_s']\ndivide_the_trip(data, headers, 60)\n# expected output: ([0, 3, 4], [0, 60, 120])\n```\nIn this example, data at indices 0, 3, and 4 are generated with a time difference of more than 60 seconds. The actual time differences at these points are 0, 60, and 120 seconds, respectively.",
    "31&@M.dycause_lib.normalize_by_row": "## API Documentation\n\n### Signature\n`normalize_by_row(transition_matrix)`\n\n### Function Summary\nThe `normalize_by_row` method normalizes each row of the provided transition matrix. If a row in the matrix adds up to zero, the row is left alone to avoid division by zero errors.\n\n### Parameters\n- `transition_matrix` *(numpy.ndarray)*: 2-D array containing numerical values. This input matrix represents the transition matrix to be normalized on a row-by-row basis.\n\n### Returns\n- This function returns a `numpy.ndarray` - a copy of the input `transition_matrix` where each row has been normalized i.e., the sum of all values in a row is 1. Note that this function does not modify the input matrix, it creates a normalized copy.\n\n### Example\n\n```python\nimport numpy as np\n\n# Let's define a sample transition 3x3 matrix\nmatrix = np.array([[3, 3, 3],\n                   [1, 2, 3], \n                   [0, 0, 0]])\n\n# Let's normalize this matrix by row\nnormalized_matrix = normalize_by_row(matrix)\n\nprint(normalized_matrix)\n\n# This will output:\n# array([[0.33333333 0.33333333 0.33333333]\n#        [0.16666667 0.33333333 0.5       ]\n#        [0.         0.         0.        ]])\n```\nIn the above example, each row in `normalized_matrix` is derived by dividing each element in the corresponding row of `matrix` by the sum of values in that row. Note that for rows where the sum is zero, as in the third row of `matrix`, the normalization process is skipped to avoid dividing by zero. Thus, the corresponding row in `normalized_matrix` remains all zeros.",
    "31&@M.dycause_lib.normalize_by_column": "# API Documentation\n\n## `normalize_by_column(transition_matrix)`\n\n### Signature\n\n```python\ndef normalize_by_column(transition_matrix):\n```\n\n### Function Summary\n\nThe `normalize_by_column` function normalizes the values of each column in the provided matrix, such that the sum of values in each column equals 1. Normalization is accomplished by dividing each value in the column by the sum of all values in the respective column. If a column sum is 0, no normalization is performed on that column, and it remains as is.\n\n### Parameters\n\n- `transition_matrix` (`numpy.ndarray`) - The input matrix that needs to be column-wise normalized. The expected matrix should be of numeric types (integer, float, etc). It is assumed that this matrix has at least 1 column.\n\n### Returns\n\n- `numpy.ndarray` - This function returns a matrix that is same dimension as the input matrix but with the columns normalized such that each column's values sum to 1 (excluding the columns whose sum was initially 0).\n\n### Example\n\n```python\nimport numpy as np\n\nmat = np.array([[1, 2, 3], [4, 5, 0], [0, 1, 1]])\nnormalized_mat = normalize_by_column(mat)\nprint(normalized_mat)\n```\n\nExpected Output:\n```\narray([[0.2       , 0.25      , 0.75      ],\n       [0.8       , 0.625     , 0.        ],\n       [0.        , 0.125     , 0.25      ]])\n```\nIn this example, the function `normalize_by_column` takes a `numpy.ndarray` `mat` as an input and returns another `numpy.ndarray` `normalized_mat` in which each column's values are normalized to sum to 1.",
    "31&@M.dycause_lib.get_segment_split": "---\n\n# **Function Name**: get_segment_split\n\n## **Signature**\n```python\ndef get_segment_split(n_sample, step)\n```\n\n## **Function Summary**\n\nThis function takes the total number of samples `n_sample` and a step size `step` as input to calculate segment splits. It returns a list which indicates where the segment splits occur.\n\n## **Parameters**\n\n- `n_sample` : int  \n  The total number of samples. Represents the dataset to be split.\n   \n- `step` : int  \n  The size of each step, i.e., the number of samples in each segment.\n\n## **Returns**\n\n- list of int  \n  Returns a list of integers, each representing the starting index of each segment. The last element in the list represents the end of the last segment, which may or may not be a complete segment, depending on whether `n_sample` is divisible by `step`.\n\n## **Example**\n\n```python\n>>> get_segment_split(27, 5)\n[0, 5, 10, 15, 20, 27]\n```\nIn this case, the dataset will be split at indexes 0, 5, 10, 15, 20, and 27. The segments will be as follows: \n- from index 0 to 5 (inclusive), \n- from 5 to 10,\n- from 10 to 15,\n- from 15 to 20,\n- and from 20 to 27.\n\n---",
    "31&@M.dycause_lib.get_ordered_intervals": "# API Documentation\n\n## Signature\n```python\nget_ordered_intervals(matrics, significant_thres, list_segment_split)\n```\n\n## Function Summary\nThis function processes two 2D numpy arrays, representing some form of matrics, to get significant intervals according to a given threshold. These intervals are then ordered based on their related metrics.\n\n## Parameters\n- **matrics** (`tuple` of two `numpy.ndarray`): A tuple of two 2D numpy arrays. Each array should have the same shape.\n- **significant_thres** (`float`): The threshold that enables the function to consider an interval as significant. This should be a positive value.\n- **list_segment_split** (`list` of `int`): A list of segmented splits.\n\n## Returns\n- **ordered_intervals** (`list` of `tuple`): List of tuples where each tuple contains a pair of metric values in the first position taken from both matrics given by `matrics`, and an interval in the second position. The list is sorted based on the metric values. Lower first metric values and higher second metric values come first in the list.\n\n## Example\n```python\nimport numpy as np\n\nYX = np.array([[0.2, 1.2, 0.3], [0.7, 0.4, 0.9],[1.4, 0.6, 0.6]])\nXY = np.array([[0.1, 1.1, 0.2], [0.6, 0.3, 0.8],[1.3, 0.5, 0.5]])\nmatrics = (YX, XY)\nsignificant_thres = 0.75\nlist_segment_split = [4, 5, 6]\n\nget_ordered_intervals(matrics, significant_thres, list_segment_split)\n```\nIn this case, the function should return: \n\n```python\n[((0.2, 1.2), (4, 5))}\n```\nThis is because, for the interval (4, 5), the absolute value of YX's corresponding metric (0.2) is less than the significant threshold and either XY's corresponding metric (1.2) is greater than or equal to the significant threshold or is -1. None of the other intervals satisfy these conditions according to our `significant_thres` value of 0.75. Also, the interval is sorted based on the corresponding metrics' values.",
    "31&@M.dycause_lib.get_max_overlay_intervals": "## API Documentation\n\n---\n\n### Signature\n`def get_max_overlay_intervals(counts)`\n\n### Function Summary\nThis function goes through the given list of counts and finds the maximal consecutive intervals where the maximum count is obtained.\n\n### Parameters\n`counts (List[int])`: A list of integers that represents counts of some events. \n\n### Returns\n`List[Tuple[int]]`: A list of tuples, with each tuple representing a start and end index for maximal overlay intervals. The start and end indices correspond to the positions in the input list that has maximal counts. Returns an empty list if the maximum count is 0.\n\n### Example\n```python\ncounts = [0, 1, 2, 2, 1, 0, 2, 2, 2, 0]\nprint(get_max_overlay_intervals(counts))\n# Output: [(2, 4), (6, 9)]\n```\nIn the example above, the `counts` list has maximum of `2`, which occurs at intervals `(2, 4)` and `(6, 9)`. These intervals are thus returned by the function. The end index in the intervals is exclusive.",
    "31&@M.dycause_lib.get_intervals_over_overlaythres": "**Signature**\n```python\nget_intervals_over_overlaythres(counts, overlay_thres)\n```\n\n**Function Summary**\n\nThis function identifies the intervals in a given list where the values surpass a certain threshold. It returns a list of tuples where each tuple represents a start and end index within the original list, which marks a continuous region of values greater than the overlay threshold.\n\n**Parameters**\n\n1. `counts` : list of integers *required*.  \n    A list of integers in which intervals of values greater than the overlay threshold are to be found.\n2. `overlay_thres` : integer *required*.  \n    The threshold value which determines the intervals to be found in the \"counts\" parameter. Each interval will contain contiguous values greater than this threshold.\n\n**Returns**\n\n`output_list` : list of tuples \n* The function returns a list of tuples, each of which includes two indices. These pairs represent the start and end index of an interval in the input list (counts) where all values are greater than the overlay threshold. If no such intervals exist, an empty list is returned.\n\n**Example**\n\n```python\ncounts=[5,7,10,13,2,3,15,18,20,3,2]\noverlay_thres=9\nprint(get_intervals_over_overlaythres(counts, overlay_thres))\n```\n\nOutput:\n\n```python\n[(2, 4), (6, 9)]\n```\n\nIn this example, there are two intervals in the input list that have all values greater than 9: [10, 13] starting at index 2 and ending at 4 (exclusive), and [15, 18, 20] starting at index 6 and ending at 9 (exclusive). Therefore, the function returns [(2, 4), (6, 9)].",
    "30&@M.dycause_lib.randwalk": "# API Documentation\n\n## Function Signature\n```python\nrandwalk(\n    transition_matrix,\n    epoch=1000,\n    mean_method=\"arithmetic\",\n    max_path_length=None,\n    entry_point=29,\n    use_new_matrix=False,\n    verbose=True,\n)\n```\n\n## Function Summary\nThis function performs a random walk using a provided transition matrix, and outputs all paths sorted by path probability. The random walk algorithm can be customized with several parameters.\n\nDuring the random movements, each vertex and edge are visited carefully and these details are recorded. There are 3 possible actions at each visit point - find a father node, find a child node or stay still.\n\nEventually, the function performs BFS (Breadth-First Search) on all possible paths and then estimates path joint probabilities. These probabilities along with corresponding paths are returned, with an optional new transition matrix (if the `use_new_matrix` flag is set to True).\n\n## Parameters\n- `transition_matrix` (`numpy.ndarray`): The initial transition matrix to be used for the random walk. This matrix should be a square 2D array where the number of rows and columns equals the total number of nodes.\n\n- `epoch` (`int`, *optional*, default=`1000`): The number of iterations to perform the random walk.\n\n- `mean_method` (`str`, *optional*, default=`'arithmetic'`): The method used for estimating the path joint probabilities. Available options are `'arithmetic'`, `'geometric'` and `'harmonic'`.\n\n- `max_path_length` (`int`, *optional*, default=`None`): The maximum length of the random walk paths. Default is `None`, which allows unlimited path length.\n\n- `entry_point` (`int`, *optional*, default=`29`): The starting point for the random walk.\n\n- `use_new_matrix` (`bool`, *optional*, default=`False`): If set to `True`, the function will generate and return a new transition matrix from the walks.\n\n- `verbose` (`bool` or `int`, *optional*, default=`True`): If this parameter is `True` or an integer greater than `0`, the function prints log messages during its execution. When `verbose>=2`, it also prints backward paths in BFS.\n\n## Returns\n- `out` (`list` of `tuple`): A list of tuples, each containing path probability and corresponding path, sorted by descending order of probability.\n\n- `new_transition_matrix` (`numpy.ndarray`): A new transition matrix generated from the walks. This is returned only when `use_new_matrix` is `True`.\n\n## Example\n```python\nimport numpy as np\n\ntransition_matrix = np.array([\n    [0.1, 0.2, 0.4, 0.3],\n    [0.2, 0.1, 0.3, 0.4],\n    [0.4, 0.3, 0.1, 0.2],\n    [0.3, 0.4, 0.2, 0.1]\n])\n\nresult, new_transition_matrix = randwalk(transition_matrix, epoch=500, mean_method='geometric', entry_point=1, use_new_matrix=True, verbose=2)\n```\nThis example should print out the random walk paths with their joint probabilities, and return the sorted list of path probabilities with corresponding paths, as well as a new transition matrix. The output varies with each call due to the randomness in the random walk. The transition matrix will be normalized.",
    "30&@M.dycause_lib.normalize_by_column": "# API Documentation\n\n---\n\n## Signature\n```python\ndef normalize_by_column(transition_matrix)\n```\n\n## Function Summary\n`normalize_by_column` is a function that normalizes each column in a 2D NP array or transition matrix by dividing each element in the column by the sum of the column. This ensures that the sum of all elements in each column is 1. If the sum of a column is 0, the function skips over it (avoids division by zero).\n\n## Parameters\n\n- `transition_matrix` (`numpy.ndarray`): The 2D numpy array to be normalized. This should be a square matrix.\n\n## Returns\n\n- `numpy.ndarray`: The normalized transition matrix. Each column of the matrix sums up to 1 unless it initially contained only zeros.\n\n## Example\n\n```python\nimport numpy as np\n\n# Create a 3*3 matrix\ntransition_matrix = np.array([[1,3,5],[4,2,0],[1,1,1]])\nnormalized_matrix = normalize_by_column(transition_matrix)\n\nprint(normalized_matrix)\n\n# Output: array([[0.16666667, 0.5       , 0.83333333],\n#                [0.66666667, 0.33333333, 0.        ],\n#                [0.16666667, 0.16666667, 0.16666667]])\n```\nIn this example, values in each column of `transition_matrix` are divided by the sum of their respective columns resulting in the `normalized_matrix` where the sum of values in each column equals 1.",
    "30&@M.dycause_lib.geo_mean_overflow": "# API Documentation\n\n---\n\n**Function Signature:**\n\n```python\ndef geo_mean_overflow(iterable)\n```\n\n**Function summary:**\n\nThis function calculates the geometric mean of an iterable (like list, tuple etc.). It manages to avoid overflow problems by first taking the natural logarithm of all numbers in the iterable, and then exponentiating the result.\n\n**Parameters:**\n\nName: `iterable`  \nType: Iterable (List, Tuple etc.) of Numeric values (float, int).\nDescription: An iterable containing numeric values for which to calculate the geometric mean.\n\n**Returns:**\n\nReturn Type: Numeric Value (float, int).\nDescription: Returns the geometric mean of the provided iterable. If the iterable is empty, it returns 1.0.\n\n**Example:**\n\n```python\n>>> geo_mean_overflow([1, 2, 3, 4, 5])\n2.605171084697352\n```\nIn the above example, the function `geo_mean_overflow` is iterated over the list [1, 2, 3, 4, 5] and the calculated geometric mean is returned. \n\n---\n\nPlease note: This function uses `np.log` and `np.exp` from the numpy library, please ensure numpy is properly installed and imported when using this function.",
    "30&@M.dycause_lib.bfs": "## API Documentation\n\n\n---\n\n### 1. Signature\n```python\nbfs(transition_matrix, entry_point, reach_end=True, max_path_length=None, verbose=False)\n```\n\n### 2. Function Summary\nThis function performs a Breadth-First Search (BFS) on a graph. The search starts from a given entry point and explores the graph recording the paths taken in the search.\n\n### 3. Parameters\n\n- **transition_matrix** (`numpy.ndarray`): It is the transition matrix of the graph. The shape of this parameter should be `(n, n)` where `n` refers to the total number of nodes in the graph.\n\n- **entry_point** (`int`): This parameter specifies the starting point or node for the BFS.\n\n- **reach_end** (`bool`, optional): It defines the behavior of the search in case the same node is encountered twice. If `True`, the search will continue to a node until there are no more previous nodes. If `False`, it will stop. By default, it is `True`.\n\n- **max_path_length** (`int`, optional): This defines the maximum length of paths the BFS can trace. If set to `None`, there is no limit on length of paths. The default is `None`.\n\n- **verbose** (`bool`, optional): This parameter controls the verbosity of function execution. If `True`, it prints detailed information about the BFS. By default, it is `False`.\n\n### 4. Returns\n\n- **path_list** (`set` of `tuple`): The function returns a set of tuple, where each tuple represents a unique path in the BFS from the entry point. If the same path is traversed more than once, it is recorded only once in the returned set.\n\n### 5. Example\n\n```python\nimport numpy as np\ntrans_matrix = np.array([[0, 1, 1, 0], [0, 0, 1, 1], [0, 0, 0, 1], [1, 0, 0, 0]])\nentry = 2\nres_paths = bfs(trans_matrix, entry, reach_end=True, max_path_length=3, verbose=True)\nprint(res_paths)\n```\nThis will output:\n```python\n{(1, 2, 0), (1, 2, 0, 3), (1, 2)}\n```\nThe function above represented a bfs search on a graph, using a transition matrix as the adjacency matrix representation and `2` as the entry point. With verbose set to `True`, the function prints details about the BFS process. In the end, it returned a set of paths traversed in the BFS as tuples.",
    "29&@M...test_monitor_rank": "# Function Signature:\n\n```python\ndef test_monitor_rank(data_source=\"real_micro_service\", pc_aggregate=5, pc_alpha=0.1, testrun_round=1, frontend=14, true_root_cause=[6, 28, 30, 31], rho=0.2, save_data_fig=False, verbose=False, runtime_debug=True, *args, **kws)\n```\n\n## Function Summary:\n\nThis function tests a monitoring ranking system in micro-service architectures.\n\n## Parameters:\n\n1. `data_source (str, optional)`: Name of the data source. Defaults to 'real_micro_service'.\n2. `pc_aggregate (int, optional)`: The aggregation degree for PC algorithm. Defaults to 5.\n3. `pc_alpha (float, optional)`: The confidence level for PC algorithm. Defaults to 0.1.\n4. `testrun_round (int, optional)`: The number of test runs. Defaults to 1.\n5. `frontend (int, optional)`: The id of the front-end server. Defaults to 14.\n6. `true_root_cause (list, optional)`: The list of true root causes. Defaults to [6, 28, 30, 31].\n7. `rho (float, optional)`: The damping factor for power iteration. Defaults to 0.2.\n8. `save_data_fig (bool, optional)`: If True, save transitions matrix and the graph. Defaults to False.\n9. `verbose (bool, optional)`: The debugging print level (0: Nothing, 1: Method info, 2: Phase info, 3: Algorithm info). Defaults to False.\n10. `runtime_debug (bool, optional)`: If True, each process is always executed. Defaults to True.\n11. `*args`: Variable length argument list.\n12. `**kws`: Arbitrary keyword arguments.\n\n## Returns: \n\nThe function returns a tuple of two elements:\n\n1. `prkS (list)`: A list of precision at rank k values.\n2. `acc (float)`: The accuracy of root cause identification.\n\n## Example:\n\n```python\n\n# Define true root causes\ntrue_root_cause_list = [6, 7, 9, 10]\n\n# Call the test_monitor_rank function\nprkS, acc = test_monitor_rank(data_source=\"my_micro_service\", pc_aggregate=3, pc_alpha=0.05, testrun_round=5, frontend=10, true_root_cause=true_root_cause_list, verbose=True)\n\n# Output\n# This will print debug information, PR@k and accuracy values, and return them.\n```\nNote: This example assumes that the required files exist and the paths are valid.",
    "29&@M...relaToRank": "**Function Signature**\n\n```python\nrelaToRank(rela, access, rankPaces, frontend, rho=0.3, print_trace=False)\n```\n\n**Function Summary**\n\nThis function takes in several parameters related to a graph (edge weights, adjacency matrix, etc.), performs some operations on this graph (normalizing flow matrix, self-edge addition etc.), then executes a 1st order random walk on the graph. Finally, it returns the ranked list and the final state matrix.\n\n**Parameters**\n\n- `rela` (`List[float]`): A list containing the weights of each edge in the graph.\n\n- `access` (`List[List[int]]`): A 2-D list (matrix) denoting the access or adjacency relationship between vertices (nodes).  Non-zero entries indicate an edge between nodes, while a zero entry indicates no edge.\n\n- `rankPaces` (`int`): Number of random walk steps.\n\n- `frontend` (`int`): The label of the start node in the graph for the random walk.\n\n- `rho` (`float`, optional, defaults to `0.3`): A constant used for calculating backward edge weights.\n\n- `print_trace` (`bool`, optional, defaults to `False`): If set to `True`, provides a printed trace of the random walk.\n\n\n**Returns**\n\n- `tuple` (`List[int]`, `numpy.ndarray`): A tuple containing two items: \n\n  - `List[int]`: A sorted list of nodes labels indicating their rank after the random walk.\n\n  - `numpy.ndarray`: The final state matrix after the last update.\n\n\n**Example**\n\n```python\nrela = [1, 2, 3, 4]\naccess = [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [1, 0, 0, 0]]\nrankPaces = 10\nfrontend = 1\n\nranked_nodes, final_state_matrix = relaToRank(rela, access, rankPaces, frontend)\n```",
    "29&@M...normalize": "## API Documentation\n\n---\n\n### Signature\n\n```python\nnormalize(p)\n``` \n\n--- \n\n### Function Summary \n\nThe `normalize` function takes a 2D NumPy array (matrix) as input and normalizes each row. Each row of the returned matrix sums up to 1. The function ignores rows whose sum is zero which will remain zeros.\n\n---\n\n### Parameters \n\n- `p` (numpy.ndarray): A 2D NumPy array which is to be normalized row-wise.\n\n---\n\n### Returns \n\n- numpy.ndarray: A 2D NumPy array. Each non-zero row in the array has been normalized so that the sum of its elements equals 1.\n\n---\n\n### Example \n\n#### Example Call\n```python\nimport numpy as np\n\np = np.array([[1, 1, 1], [1, 2, 3], [0, 0, 0]])\nprint(normalize(p))\n```\n\n#### Expected Output\n\n```python\narray([[0.33, 0.33, 0.33],\n       [0.17, 0.33, 0.5 ],\n       [0.  , 0.  , 0.  ]])\n```\n\nIn the example, the input 2D array (matrix) `p` is passed to the `normalize` function. The function normalizes each row in the matrix - except the row with all zeros - and returns it.\n\nThe normalization is performed by dividing each element in the row by the sum total of the elements in that row. In the resulting array, each row sums up to 1 (except the row that was all zeros).",
    "29&@M...firstorder_randomwalk": "# API Documentation\n\n## Signature\n```python\nfirstorder_randomwalk(P, epochs, start_node, teleportation_prob, label=[], walk_step=1000, print_trace=False)\n```\n\n## Function Summary\nThis function conducts a first-order random walk on a graph. It returns the node labels and their corresponding scores denoting the frequency of their visits.\n\n## Parameters\n- **P** (numpy.ndarray): A square matrix representing the transition probability matrix for the directed graphm where each row sums up to 1.\n- **epochs** (int): The number of iterations the random walk should perform. \n- **start_node** (int): The node ID where the walk should start. Note that node ID starts from 1.\n- **teleportation_prob** (numpy.ndarray): A vector defining the teleportation probability for each node. It is used when the current node has no outgoing edges.\n- **label** (list, optional): A list of node labels. If provided, it should be in the same order with the nodes in the matrix P. Default is an empty list.\n- **walk_step** (int, optional): The maximum steps each walk can take in one epoch. Default is 1000.\n- **print_trace** (bool, optional): Print the details of each walk if set to True. Default is False.\n\n## Returns\n- **score_list** (list of tuples): The list contains tuples of node label and the score of each node. The score is defined as the frequency that the node is visited by the walk. The list is sorted in descending order of the score.\n\n## Example\n\n```python\nimport numpy as np\nP = np.array([[0, 0.5, 0.5], [0.5, 0, 0.5], [0.5, 0.5, 0]])\nepochs = 10\nstart_node = 1\nteleportation_prob = np.array([0.3, 0.3, 0.4])\nlabel = ['A', 'B', 'C']\nwalk_step = 5\nprint_trace = True\nscore_list = firstorder_randomwalk(P, epochs, start_node, teleportation_prob, label, walk_step, print_trace)\nprint(score_list)\n```\nExample Output:\n```python\nA -> B -> C -> A -> B -> A\nA -> C -> A -> C -> A -> C\nA -> C -> B -> A -> C -> A\nA -> C -> B -> A -> C -> B\nA -> B -> C -> B -> C -> A\nA -> C -> B -> C -> A -> C\nA -> C -> B -> C -> B -> C\nA -> C -> B -> A -> B -> A\nA -> C -> A -> C -> B -> A\nA -> C -> B -> C -> B -> C\n[('A', 14.0), ('B', 13.0), ('C', 23.0)]\n```\nThis output means after performing first order random walks starting from node 'A', 'A' was visited 14 times, 'B' was visited 13 times, 'C' was visited 23 times at most. Therefore, the function returns the scores in descending order.",
    "28&@M...weighted_powermean": "---\n## weighted_powermean Function\n\n**Signature:**   \n`weighted_powermean(score, p, weight)`\n\n**Function Summary:**  \nThis function computes the weighted power mean of a given set of scores with their respective weights. The power mean is a formula used in math and statistics to generalize a range of mean types (including the arithmetic mean, geometric mean, and harmonic mean). It also allows negative values in the scores to be processed. \n\n**Parameters:**  \n\n- `score` (list of floats): The input list of scores on which the weighted power mean is to be calculated. The function assumes that you pass a list of real numbers. \n- `p` (float): The power exponent used in the power mean calculation. The function assumes that you pass a real number.\n- `weight` (list of floats): The weight assigned for each respective score. This list should have the same length as `score` list. The weights should be non-negative numbers.\n\n**Returns:**  \n- The function returns a float value which is the calculated weighted power mean of the given scores and weights.\n\n**Example:**  \n\n```python\n#assigning list of scores and their weights\nscores = [2.3, 4.5, 3.4]\nweights = [1, 2, 3]\n\n#calling the weighted_powermean function\nmean_val = weighted_powermean(scores, 2, weights)\n\n#Printing the output\nprint(mean_val)\n```\n\nThe above example call to `weighted_powermean` function would output `3.3480993684210527` which is the calculated weighted power mean of scores `[2.3, 4.5, 3.4]` with weights `[1, 2, 3]` and power exponent `2`.\n---\n",
    "28&@M...test_tbac": "## API Documentation\n\n### Function Signature\n```python\ndef test_tbac(\n    data_source=\"pymicro\", \n    pc_aggregate=5, \n    pc_alpha=0.1, \n    frontend=16, \n    true_root_cause=[1],\n    verbose=False,\n    runtime_debug=False,\n    *args,\n    **kws\n)\n```\n\n### Function Summary\nThe function `test_tbac` is used to test the TBAC (TraceBack Anomaly Correlation) algorithm. This function takes several parameters to conduct the TBAC test using a given dataset. It also provides verbose and runtime debugging options for more detailed output and testing respectively.\n\n### Parameters\n\n* **data_source** (str, default='pymicro'): The name of the dataset to use for the TBAC algorithm. Default value is 'pymicro'.\n* **pc_aggregate** (int, default=5): This parameter is used in the data preprocessing step to aggregate the data. Default value is 5.\n* **pc_alpha** (float, default=0.1): Significance value used in the PC algorithm. Default value is 0.1.\n* **frontend** (int, default=16): It represents the number of frontend services in the service call path. Default value is 16.\n* **true_root_cause** (list of integers, default=[1]): The true root cause(s) in the dataset. Used for testing the result of TBAC algorithm. Default value is [1].\n* **verbose** (bool, default=False): If set to `True`, the function will print additional information during execution.\n* **runtime_debug** (bool, default=False): If set to `True`, the function will run in debug mode.\n* **args**: Any additional arguments to be passed to the function.\n* **kws**: Any additional keyword arguments to be passed to the function.\n\n### Returns\n* **prkS** (list of floats): The precision at k for each k in the range from 1 to 5\n* **acc** (float): The accuracy of the TBAC algorithm.\n\n### Example\n```python\nresults = test_tbac(data_source='real_micro_service', pc_aggregate=3, pc_alpha=0.05, \n                    frontend=10, true_root_cause=[2], verbose=True, runtime_debug=True)\nprint(results)\n```\nThis example uses a different dataset ('real_micro_service') with pc_aggregate as 3 and pc_alpha as 0.05. The number of frontend services is set to 10 and the true root cause is set to [2]. The verbose and runtime_debug parameters are set to True to enable a more detailed output. \n",
    "28&@M...get_callers": "# API Documentation\n\n## Signature\n```python\nget_callers(access, node, search_depth=1)\n```\n\n## Function Summary\nThe function `get_callers` is used to find the callers of a given node in a call graph, up to a specified search depth. \n\n## Parameters\n\n* `access` (2-dimensional list of integers): A matrix representing the call graph, where global call relations are represented. If a call from node `i` to node `j` exists, then `access[i][j]` is `1`, otherwise `0`.\n\n* `node` (integer): The node in the call graph for which you want to identify the potential callers. \n\n* `search_depth` (integer, optional): The maximum depth of search in the call graph. Its default value is `1`, which means the function will only find direct (first-level) callers.\n\n## Returns\n\n* `callers` (List of lists): A list of callers identified. Each element in the list is a pair `[j, depth]`, where `j` is the index of the caller node and `depth` is the depth of the call. If there are no callers found, an empty list will be returned.\n\n## Example\n```python\naccess = [[0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 1], [0, 0, 0, 0]]\nnode = 2\nsearch_depth = 2\nprint(get_callers(access, node, search_depth))\n# Output: [[1, 1], [0, 2]]\n```\nIn the provided call graph, the node `2` is directly called by node `1` (at depth 1) and indirectly by node `0` (at depth 2).",
    "28&@M...get_callees": "## API Documentation for `get_callees` Python Function\n\n### Signature\n```python\nget_callees(access, node, search_depth=1)\n```\n\n### Function Summary\nThe `get_callees` function is used to find the callers (callees) of a given node within a specified search depth. It operates using a fixed-search-depth approach.\n\n### Parameters \n\n- `access` (*list of list of integers*): This is a two-dimensional list (matrix) indicating the access privilege or relation between nodes. Each row and each column represent a node, and the value at the intersection (`[i][j]`) indicates whether `i` has access to `j`. An access value of `> 0` signifies access.\n\n- `node` (*integer*): This is the origin node from which the search will be initiated.\n\n- `search_depth` (*integer, optional, default=1*): This determines how far the function should search in terms of nodes away from the original node. Its default value is 1, meaning the function would only find the direct callers of the given node.\n\n### Returns\n\n- `list of list of integers`: The function will return a two-dimensional list where the first integer represents the node that has access to the given node and the second integer represents the depth at which it was found. \n\n### Example\nHere is a simple example to demonstrate how to call the function:\n```python\naccess_matrix = [[0, 1, 0, 1], [1, 0, 1, 0], [0, 1, 0, 1], [1, 0, 1, 0]]\nnode = 1\nsearch_depth = 2\nprint(get_callees(access_matrix, node, search_depth))\n```\nExpected output:\n```python\n[[0, 1], [2, 2]]\n```\nThis output means that node 0 has access to node 1 at depth 1 and node 2 has access to node 1 at depth 2.",
    "28&@M...correlation_algorithm": "# API Documentation\n\n---\n\n## Function Signature\n```\ncorrelation_algorithm(scores: list, access: list) -> list\n```\n\n## Function Summary\nThis function implements an algorithm that evaluates and modifies node ratings based on certain conditions. It is based on a correlation algorithm that takes into account the access relations between different nodes, their scores, and manipulates them using helpers methods like weighted arithmetic mean and maximum values.\n\n## Parameters\n| Parameter | Type | Description |\n| --- | --- | --- |\n| `scores` | list | List of initial scores for the nodes. The score for a node at index `i` is `scores[i]`. |\n| `access` | list | List of access data for the nodes. The access data for a node at index `i` is `access[i]`. |\n\n## Returns\n| Return | Type | Description |\n| --- | --- | --- |\n| `rating` | list | List of updated scores after processing through the correlation algorithm. The scored node at index `i` is `rating[i]`. |\n\n## Example\n```python\nscores = [0.2, 0.4, 0.6, 0.8, 1.0]\naccess = [\n    [(1, 0.1), (2, 0.2), (3, 0.3)],  \n    [(0, 0.3), (2, 0.4)],  \n    [(3, 0.1), (4, 0.5)],\n    [(2, 0.3), (4, 0.7)],\n    [(0, 0.2), (1, 0.4), (3, 0.1)]\n]\nrating = correlation_algorithm(scores, access)\n\nprint(rating)  # list of updated ratings for nodes.\n```\nOutput will depend on the function `get_callers()`, `get_callees()`, and `weighted_powermean()`, which are not defined and hence cannot be pre-determined.\n\n> Please note that the `get_callers()`, `get_callees()`, and `weighted_powermean()` functions are not defined and should be provided or imported for this code to run correctly.",
    "27&@M...worker_process": "# API Documentation\n\n## Signature\n```python\ndef worker_process(ind, params_dict)\n```\n\n## Function Summary\nThe `worker_process` function wraps the `test_granger_extend` function with the parameters from a dictionary (`params_dict`) and also includes a specified index (`ind`).\n\n## Parameters\n\n- `ind` _(int or float)_ - The unique identifier used to differentiate worker processes. \n\n- `params_dict` _(Dict[str, Union[int, bool, str, float]])_ - A dictionary that should contain the parameters as below:\n    - `ela` _(int or float)_ - The aggregation delay for the dataset.\n    - `bef` _(int)_ - The length of the data before the current time.\n    - `aft` _(int)_ - The length of data after the current time.\n    - `step` _(int)_ - The step in the granger interval based graph construction process.\n    - `sig_value` _(float)_ - The threshold above which a value is considered significant in the granger interval based graph construction process.\n    - `lag` _(int)_ - The period of time in the past which is investigated for patterns that help predict the present.\n    - `thres` _(float)_ - The threshold ratio for automatic thresholding.\n    - `max_path_length` _(int)_ - The maximum path length in the root cause analysis process.\n    - `mean_method` _(str)_ - The method used to calculate means in the root cause analysis process.\n    - `topk_path` _(int)_ - Number of top paths to be considered in the root cause analysis process.\n    - `num_sel_node` _(int)_ - Number of selected nodes to be considered in the root cause analysis process.\n\n## Returns\n\n- `prks` _(float)_ - Precision of retrieved documents in the root cause analysis process.\n- `acc` _(float)_ - Accuracy of the root cause analysis process.\n- `ind` _(int or float)_ - The original input index passed to the function.\n\n## Example\n\n```python\nparams_dict = {\n    'ela': 20,\n    'bef': 200,\n    'aft': 250,\n    'step': 2,\n    'sig_value': 0.05,\n    'lag': 3,\n    'thres': 0.6,\n    'max_path_length': 100,\n    'mean_method': 'mean',\n    'topk_path': 5,\n    'num_sel_node': 4,\n}\n\nind = 1\nprks, acc, ind = worker_process(ind, params_dict)\n\n# Expected output: prints the prks, acc and ind values\nprint(prks, acc, ind)\n```\n",
    "27&@M...main": "# API Documentation\n\n## Function Signature\n\n```python\nmain()\n```\n\n## Function Summary\n\nThe `main()` function executes multiple tasks concurrently with a maximum limit of six workers. It tracks and outputs the number of failed tasks. After all tasks have run, it serializes and stores the successful tasks into a pickle file named 'granger_extend_parameter_tune_ibm_708.pkl'.\n\nNote: This function heavily depends on other functions (like `worker_process`) and variables (`params_list`) which are not defined within the scope of the function itself. They must be defined elsewhere in your code. \n\nThis function is typically used within a script and does not receive any input or give any output.\n\n## Parameters\n\nThis function does not accept any parameters.\n\n## Returns \n\nThis function does not return any value.\n\n## Example\n\nThis function does not accept any arguments or return a value, hence a relative example can not be provided. However, running the function itself would look something like this:\n\n```python\nmain()\n```\n\nAfter the function has been run, a message will print indicating how many parameter sets are found in `params_list`. \n\nThe function will use these parameters to execute worker tasks concurrently. When each task concludes, the function updates a progress bar. \n\nIf a task fails due to an exception, the function increments a fail counter and prints an error message. \n\nFinally, after all tasks have completed, the function dumps the results of the successful tasks into a pickle file. These results include the successfully processed parameters subset, some sort of 'prks' and 'acc' values associated with each parameter set.  \n\nPlease note the example might look different based on the behavior of the code associated with variables such as `params_list`, `worker_process`.",
    "26&@M...worker_process": "# `worker_process()` Function\n\n## Signature\n```python\ndef worker_process(ind, params_dict):\n```\n\n## Function Summary\n\nThis function represents a worker process in a cloud ranger system. It evaluates the cloud ranger by passing the parameters from the given parameters dictionary to the `test_cloud_ranger()` method which simulates the behavior of cloud ranger according to these specified parameters. It returns the prks, acc variables calculated by the `test_cloud_ranger()` method and input parameter ind as its outcome.\n\n## Parameters\n\n- `ind (int)`: The worker process identifier. Must be a non-negative integer.\n- `params_dict (dict)`: A dictionary of parameters to be sent to the `test_cloud_ranger()` function.\n\n    The `params_dict` dictionary expects the following keys:\n   - `ela (float)`: The elastic aggregation parameter. Must be a positive float.\n   - `alpha (float)`: The alpha parameter. Must be a float between 0 and 1. \n   - `testround (int)`: The number of round for testing. A positive integer.\n   - `beta (float)`: The beta parameter. Must be a float between 0 and 1.\n   - `rho (float)`: The rho parameter. Must be a positive float.\n\n## Returns\n\n- `(tuple)`: A tuple of three values.\n  - `prks (variable type)`: The prks result from `test_cloud_ranger()` function.\n  - `acc (variable type)`: The acc result from `test_cloud_ranger()` function.\n  - `ind (int)`: A repetition of the incoming parameter ind.\n\n## Example\n\n```python\nparam_dict = {'ela': 0.2, 'alpha': 0.4, 'testround': 10, 'beta': 0.3, 'rho': 0.5}\nind = 1\nresults = worker_process(ind, param_dict)\n```\n\nNote: The actual values for results will vary depending on the behavior of the `test_cloud_ranger()` function, which is not provided here. Also, the types of `prks` and `acc` can't be determined from the provided context and hence are referred to as \"variable type\" above.",
    "26&@M...main": "**Signature**: \n```python\nmain()\n```\n\n**Function Summary**: \nThe `main` function is the driver function that serves as a test suite for IBM Microservices. It initializes and manages a collection of worker processes, submits tasks to these workers and collects and stores their results. If a task fails, it logs the exception and counts the number of failed tasks.\n\n**Parameters**: \nThe function does not accept any parameters. \n\n**Returns**: \nThe function does not return any values. Instead, it writes a file 'cloudranger_parameter_tune_ibm.pkl' with the result list which holds the success results of the process pool tasks. \n\n**Example**: \nFollowing the code:\n\n```python\n# Before calling the function, ensure the params_list is available and worker_process function is defined\nparams_list = [...]\n\ndef worker_process(i, params_dict):\n    ...\n    return some_values\n\n# Call the function\nmain()\n```\n\nThis will initialize the process pool and prepare the workers for the tasks as per the contents in `params_list`. For each task in `params_list`, the function `worker_process` will be invoked in a separate worker process. The progress will be displayed in the console. Any exceptions during the process pool tasks will be counted and displayed. If there are successful tasks, their results will be stored in a file called 'cloudranger_parameter_tune_ibm.pkl'. Please note that 'params_list' variable should be available in the scope and 'worker_process' function should be defined before calling 'main()' function.",
    "25&@M.dycause_lib.anomaly_detect": "---\n\n### Signature\n```python\ndef anomaly_detect(data, weight=1, mean_interval=60, anomaly_proportion=0.3, verbose=True, save_fig=True, path_output=None):\n```\n\n### Function Summary\nThe function `anomaly_detect()` is used to identify the time instance when an anomaly first appears in a given dataset. For each variable in the data, the function applies a moving average and a moving standard deviation to compute anomaly scores, and then combines these scores based on a weight value to determine the starting point of the anomaly.\n\n### Parameters\n- `data` (numpy array): A multi column data array where each column represents a variable. The columns represent different measurements over time and the rows represent individual timestamp observations.\n- `weight` (int, optional): The weight assigned to each variable. Default is 1.\n- `mean_interval` (int, optional): Size of the sliding window used to calculate the moving average and moving standard deviation. The default value is 60.\n- `anomaly_proportion` (float, optional): This must be related to the weight. It represents the proportion of anomaly scores, used to determine when an anomaly is detected. Default is 0.3.\n- `verbose` (bool, optional): Control the level of debugging print information. Default is True.\n- `save_fig` (bool, optional): If set to `True`, the function will save figures summarizing the anomaly detection results. Default is `True`.\n- `path_output` (string, optional): Specifies the path where figures should be saved. If None, figures will be saved in the current working directory.\n\n### Returns\n- `start_time` (int): Index in the data when the anomaly starts.\n- `dither_proportion[start_time]` (float): Score corresponding to the start_time.\n\n### Example\n\n```python\nimport numpy as np\n\n# generating random data for example\ndata = np.random.rand(500, 4)\nstart_time, score = anomaly_detect(data, weight=1, mean_interval=20, anomaly_proportion=0.2, save_fig=False)\nprint(\"Anomaly starts at index: \", start_time)\nprint(\"Anomaly score: \", score)\n```\nOutput:\n```python\nAnomaly starts at index:  75\nAnomaly score:  1.0\n```\n\nIn this example, a numpy array of random values is created where each row represents a timestamp and each column is a different variable. The function `anomaly_detect()` is used to find the index in the data where the anomaly starts and its corresponding score.\n\n---",
    "24&@M...test_dycause": "## **API Documentation**\n\nHere is the description of the *test_dycause* function.\n\n### Signature\n\n```python\ntest_dycause(data_source=\"real_micro_service\", aggre_delta=1, start_time=None, before_length=300, after_length=300, step=50, significant_thres=0.05, lag=5, auto_threshold_ratio=0.8, runtime_debug=False, testrun_round=1, frontend=14, max_path_length=None, mean_method=\"arithmetic\", true_root_cause=[28], topk_path=60, num_sel_node=1, plot_figures=False, verbose=True, max_workers=5, **kws,)\n```\n\n### Function Summary\n\nThis function performs a root cause analysis on a given dataset using the DyCause methodology, which involves a granger interval-based graph construction and a backtrace analysis.\n\n### Parameters\n\nThe parameters accepted by the function `test_dycause` are:\n\n1. `data_source` (str; default: *\"real_micro_service\"*): The path to the dataset.\n2. `aggre_delta` (int; default: 1): Aggregation level of data.\n3. `start_time` (datetime/object; default: None): The starting point of the interval to analyze.\n4. `before_length` (int; default: 300): Number of steps of data before the `start_time`.\n5. `after_length` (int; default: 300): Number of steps of data after the `start_time`.\n6. `step` (int; default: 50): Number of steps in partitioning the data.\n7. `significant_thres` (float; default: 0.05): Threshold for significance while constructing the graph.\n8. `lag` (int; default: 5): Lag order to handle while calculating the Granger causality test.\n9. `auto_threshold_ratio` (float; default: 0.8): Ratio to apply as threshold while making edges on the graph.\n10. `runtime_debug` (bool; default: False): When True, the function provides debugging information in runtime.\n11. `testrun_round` (int; default: 1): Number of rounds to run the backtrace analysis.\n12. `frontend` (int or list; default: 14): Entry point for root cause analysis.\n13. `max_path_length` (int or None; default: None): Maximum path length in the root cause analysis graph.\n14. `mean_method` (str; default: \"arithmetic\"): Method used to compute the mean in the analysis.\n15. `true_root_cause` (list; default: [28]): List of nodes that are the true root causes.\n16. `topk_path` (int; default: 60): Number of paths to consider in the top-k path selection.\n17. `num_sel_node` (int; default: 1): Number of selected nodes for root cause analysis.\n18. `plot_figures` (bool; default: False): Draw and save figures for results.\n19. `verbose` (bool; default: True): Print extra information about the analysis process.\n20. `max_workers` (int; default: 5): Maximum number of workers in multiprocessing.\n21. `**kws` (optional arguments): Additional parameters if needed.\n\n### Returns\n\n1. `prkS`: (list) Precision of the k-ranked anomalies.\n2. `acc`: (float) Accuracy of anomaly detection.\n\n### Example\n\nDefine the test_dycause function and use:\n\n```python\nprkS, acc = test_dycause(data_source=\"local_data.xlsx\", start_time=datetime.now(), testrun_round=5)\n```\n\nThe output will provide a list displaying precision of the k-ranked anomalies and a float for the accuracy of anomaly detection.",
    "24&@M...granger_process": "---\n## **Function Signature**\n`granger_process(shared_params_dict, specific_params, shared_result_dict)`\n\n## **Function Summary**\nThis function performs a Granger causality test using the provided parameters, captures the result, and returns the output. \n\n## **Parameters**\n\n* `shared_params_dict: dict`\n  \n  A dictionary containing the shared parameters for the Granger causality test. The keys used here include: 'local_data', 'data_head', 'dir_output', 'significant_thres', 'method', 'trip', 'lag', 'step', 'simu_real', 'max_segment_len' and 'min_segment_len'.\n  \n* `specific_params: dict`\n  \n  A dictionary containing the specific parameters for the function. The keys used here include: 'x_i' and 'y_i'.\n  \n* `shared_result_dict: dict`\n  \n  A dictionary where results of the Granger causality testing process will be stored.\n\n## **Returns**\n\n* `ret: tuple`\n  \n  Returns a tuple representing the return result of `loop_granger` execution. It can contain various elements depending on the result of the test or `(None, None, None, None, None)` if any exception occurs during execution. The value is also added to the `shared_result_dict`.\n\n## **Example**\n```python\nshared_params_dict = {\n    'local_data': some_data,\n    'data_head': data_head,\n    'dir_output': \"/output\",\n    'significant_thres': 0.05,\n    'method': \"method1\",\n    'trip': \"trip1\",\n    'lag': 5,\n    'step': 1,\n    'simu_real': \"simu\",\n    'max_segment_len': 100,\n    'min_segment_len': 50\n}\n\nspecific_params = {\n    'x_i': 1,\n    'y_i': 2\n}\n\nshared_result_dict = {}\n\nresult = granger_process(shared_params_dict, specific_params, shared_result_dict)\n```\nIn this example, the function `granger_process` is called with shared parameters, specific parameters and a dictionary to store the result. The return value is assigned to the variable `result`.\n\n---\n",
    "23&@M.dycause_lib.ranknode": "## API Documentation\n\n### Signature \n```python\nranknode(data, out_path, entry_point, node_num, topk_path=60, prob_thres=0.4, num_sel_node=1)\n```\n\n### Function Summary\nThis function ranks nodes based on Pearson correlation. It selects the nodes according to their path count and Pearson correlation with the entry point.  \n\n### Parameters\n* `data` (`numpy.ndarray`): A 2D numpy array representing the data to be processed.\n* `out_path` (`list`): A list of paths where each path is represented as `(probability, list_of_nodes)`.\n* `entry_point` (`int`): The node which is considered entry point.\n* `node_num` (`int`): The number of nodes in the input graph.\n* `topk_path` (`int`, optional, default `60`): The number of top paths considered in calculation. Must be >= 1.\n* `prob_thres` (`float`, optional, default `0.4`): Probability threshold. Only paths with probability >= prob_thres are considered. Must be >= 0.0 and <= 1.0.\n* `num_sel_node` (`int`, optional, default `1`): The number of nodes to select from each path. Must be >= 1.\n\n### Returns\n* `rank_list` (`list`): Returns a list of tuples where each tuple represents a node and its rank; The format of each tuple in the list is (node, rank). The higher the rank, the higher its correlation with the entry point.\n\n### Example\n```python\nimport numpy as np\n\ndata = np.array([[1,2,3], [2,3,4], [4,5,6]])\nout_path = [(0.6, [1,2,3]), (0.5, [2,3,4])]\n\nresult = ranknode(data, out_path, 1, 3)\n\nprint(result)\n# Output: [[0, 0.6], [2, 0.5]]\n```\nIn this example, we have 3 nodes and we are identifying the rank of these nodes based on their correlation with the entry point. The result indicates that `node 0` has the highest correlation with the entry point, followed by `node 2`.",
    "23&@M.dycause_lib.analyze_root": "**Signature**\n```python\ndef analyze_root(\n    transition_matrix,\n    entry_point,\n    local_data,\n    epoch=1000,\n    mean_method=\"arithmetic\",\n    max_path_length=None,\n    topk_path=60,\n    prob_thres=0.4,\n    num_sel_node=1,\n    use_new_matrix=False,\n    verbose=False,\n):\n```\n\n**Function Summary**\n\n`analyze_root` function performs a random walk over a transition matrix and returns a set of nodes ranked according to defined conditions. The ranking process is adjustable through various parameters.\n\n**Parameters**\n\n- `transition_matrix` (numpy.array): A matrix representing the transition probabilities in a graph or Markov Chain.\n- `entry_point` (int): The starting node for the random walks.\n- `local_data` (numpy.array): Local data associated with each node on the transition matrix to be used for node ranking.\n- `epoch` (int, optional, default=1000): The number of iterations for the random walk.\n- `mean_method` (str, optional, default=\"arithmetic\"): The method for calculating the mean during the random walk (currently only supports \"arithmetic\").\n- `max_path_length` (int, optional): The maximum allowed length of a path in the random walk. If None, paths can be of any length. \n- `topk_path` (int, optional, default=60): The number of top-ranked paths that will be returned.\n- `prob_thres` (float, optional, default=0.4): The probabilistic threshold for filtering out low probability nodes.\n- `num_sel_node` (int, optional, default=1): The number of selected nodes required for ranking.\n- `use_new_matrix` (bool, optional, default=False): If set to True, creates and uses a new transition matrix for each epoch.\n- `verbose` (bool, optional, default=False): If set to True, prints detailed execution information during processing\n\n**Returns**\n\n- Tuple: A tuple containing two elements:\n  - `ranked_nodes` (list): A list of node ranks (adjusted by the function to match the data representation) and their computed values from the random walk.\n  - `new_matrix` (numpy.array): The final transition matrix used after completing the random walk, useful for subsequent walks.\n\n**Example**\n\n```python\ntm = np.array([[0.1,0.6,0.3],[0.3,0.4,0.3],[0.5,0,0.5]])\nlocal_data = np.array([[10,100,1000],[20,200,2000],[30,300,3000]])\noutput = analyze_root(transition_matrix=tm, entry_point=1, local_data=local_data, epoch=10, topk_path=2)\n```",
    "22&@M.dycause_lib.draw_overlay_histogram": "---\n## function: draw_overlay_histogram(histogram, title, filepath)\n\n---\n### Signature\n```python\ndraw_overlay_histogram(histogram: list, title: str, filepath: str) -> None\n```\n### Function Summary\nThis function takes a histogram, a title, and a file path as input, generates a histogram plot with the title and saves it in the provided filepath.\n\n### Parameters\n1. **histogram (List[int])**: The data for which the histogram is to be drawn. Each element represents the frequency of an item.\n\n2. **title (str)**: The title of the histogram. It will also be included in the plot.\n\n3. **filepath (str)**: The location where the histogram plot image file will be saved. Please include the desired file extension, for example, '.png' or '.jpg'. \n\n### Returns\nThis function does not return anything. It saves the image of the plot at the specified filepath.\n\n### Example\n```python\n# Assuming you have a histogram list as follows:\nhistogram = [5, 3, 6, 7, 2, 4, 4, 6, 7, 5, 4, 2]\n# You can call the function as follows:\ndraw_overlay_histogram(histogram, 'Histogram Title', 'path/to/save/histogram.png')\n```\nOn calling the function, an image file named 'histogram.png' will be saved in the directory 'path/to/save/'. The file will contain a plot for the provided histogram and title 'Histogram Title'.",
    "22&@M.dycause_lib.draw_bar_histogram": "## API Documentation\n\n---\n\n### Signature\n```python\ndraw_bar_histogram(histogram, auto_threshold_ratio,  title, filepath)\n```\n### Function Summary\nThis function plots a bar histogram to visualize distributions of different services. The histogram bars are in black color. It also plots a red horizontal line to represent a threshold. Finally, it saves the generated graph as a file to a specific location.\n\n### Parameters\n- **histogram (Iterable)**: An iterable object (e.g., list) of numerical values representing the service distributions. Each value represents the quantity of each service.\n- **auto_threshold_ratio (float)**: A floating-point number representing the ratio for calculating the threshold line.\n- **title (str)**: A string to be used as the title of the plotted histogram.\n- **filepath (str)**: A string containing the location and name of the file where the generated histogram should be saved.\n\n### Returns\nThis function has no return value. It outputs the histogram by saving it as a file in the given `filepath`.\n\n### Example \nLet's take an example where `histogram` is `[12, 17, 9, 15]`, `auto_threshold_ratio` is `0.5`, `title` is `'Service Distribution'`, and `filepath` is `'output.png'`.\n\n```python\ndraw_bar_histogram([12, 17, 9, 15], 0.5, 'Service Distribution', 'output.png')\n```\nThere will be no return value for this function. Instead, a .png file will be created in the current working directory with the name `output.png` which contains the plotted histogram showing the distribution of services. The red horizontal line plotted in the histogram represents 50% of the maximum quantity present amongst all services (since `auto_threshold_ratio = 0.5`).",
    "22&@M.dycause_lib.draw_alldata": "# API Documentation\n\n## Signature\n`draw_alldata(data, data_head, filepath)`\n\n## Function Summary\nThis function generates plots for all variables inside a dataset. It creates a figure with multiple subplots, one for each variable. The figure is then saved into a specified path. \n\n## Parameters\n* `data` : numpy array\n    * This should be a 2-D array where each column represents a different variable and each row represents an observation. \n* `data_head` : list of string\n    * This is a list containing names of each variable in the data. The length of the list should be equal to `data.shape[1]` (i.e. number of columns in the data).\n* `filepath` : string\n    * The full path (including filename) where you want the output figure to be saved. Ensure the directory exists. The file format will be inferred from filename.  Supports formats like PNG, PDF, PS, EPS and SVG.\n\n## Returns\nThis function does not return any value but saves a figure comprising subplots of each variable in the specified filepath.\n\n## Example\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# generate some sample data\ndata = np.random.rand(50, 4)\n\n# assume these are data headers\ndata_head = ['Var1', 'Var2', 'Var3', 'Var4']\n\n# draw the data and save figure at specified path\ndraw_alldata(data, data_head, '/path/to/save/figure.png')\n```\n\nNote: In this example, '/path/to/save/' should be replaced by an existing path on your local machine.\n\nAfter executing the function, a figure named 'figure.png' will be saved at '/path/to/save/' location. It will contain 4 subplots, each showing a plot for a variable along 50 observations.",
    "21&@M...test_dycause": "## API Documentation for the function `test_dycause`\n\n### Signature\n\n```python\ndef test_dycause(\n    data_source=\"real_micro_service\",\n    aggre_delta=1,\n    start_time=None,\n    before_length=300,\n    after_length=300,\n    step=50,\n    significant_thres=0.05,\n    lag=5,\n    auto_threshold_ratio=0.8,\n    testrun_round=1,\n    frontend=14,\n    max_path_length=None,\n    mean_method=\"arithmetic\",\n    true_root_cause=[6, 28, 30, 31],\n    topk_path=60,\n    num_sel_node=1,\n    plot_figures=False,\n    verbose=True,\n    runtime_debug=False,\n    *args,\n    **kws\n)\n```\n\n### Function Summary\n\nThe `test_dycause` function performs root cause analysis on service anomalies in a micro service environment. It identifies the root cause of the service anomaly by analyzing the interaction between different micro services and charts a dynamic causal graph based on this analysis.\n\n### Parameters\n\n* `data_source` (str): The source of the data to be analyzed. Default is 'real_micro_service'.\n* `aggre_delta` (int): The time delta for data aggregation. Default is 1.\n* `start_time` (str): The start time for the analysis. If not provided, the start time is calculated based on anomaly detection.\n* `before_length` (int): The length of time before the anomaly to be considered in the analysis. Default is 300.\n* `after_length` (int): The length of time after the anomaly to be considered in the analysis. Default is 300.\n* `step` (int): The step size for the Granger causal interval based graph construction. Default is 50.\n* `significant_thres` (float): The significance threshold for the Granger causality test. Default is 0.05.\n* `lag` (int): The lag to be applied in the Granger causality test. Default is 5.\n* `auto_threshold_ratio` (float): The threshold ratio for determining the transition matrix. Default is 0.8.\n* `testrun_round` (int): The round of test run to conduct. Default is 1.\n* `frontend` (int or list): The frontend service that is considered in the root cause analysis. Default is 14.\n* `max_path_length` (int): The maximum path length for the analysis. If not provided, there is no restriction.\n* `mean_method` (str): The method to calculate the mean. Default is 'arithmetic'.\n* `true_root_cause` (list): The list of true root causes. Default is [6, 28, 30, 31].\n* `topk_path` (int): The number of top paths to consider. Default is 60.\n* `num_sel_node` (int): The number of selected nodes in the path. Default is 1.\n* `plot_figures` (bool or list): Whether to plot figures and which figures to plot. Default is False. Can be either a boolean value or a list of figure names.\n* `verbose` (bool): Controls the debugging output level. Default is True.\n* `runtime_debug` (bool): Determines whether the function should run in a runtime debug mode. Default is False.\n* `args` (tuple): Additional positional arguments to pass to the function.\n* `kws` (dict): Additional keyword arguments to pass to the function.\n\n### Returns\n\n* If `runtime_debug` is False:\n  * `prkS` (list): The PR@k statistic, indicating how well the function can identify root causes within top-k results.\n  * `acc` (float): The accuracy of the function in identifying root causes.\n* If `runtime_debug` is True:\n  * `prkS` (list), `acc` (float), `time_stat_dict` (dict): In addition to `prkS` and `acc`, a dictionary recording time spent in each phase of the function is also returned.\n\n### Example\n\n```python\nprkS, acc, time_stat = test_dycause(data_source=\"test_data\", runtime_debug=True)\n```",
    "20&@M.SPOT.dSPOT.run": "## API Documentation\n\n---\n\n### Signature\n\n```python\nrun(self, with_alarm: bool = True) -> dict\n```\n\n### Function Summary\n\nThis function is used to execute the biSPOT process on a data stream, determining extreme quantiles and potentially alarming values.\n\n### Parameters\n\n|   Name     |   Type     | Description                                                                                   |\n| :--------- | :--------- | :-------------------------------------------------------------------------------------------- |\n| `self`     |            | Implicit default argument representing the object instance.                                   |\n| `with_alarm`| `bool`    | An optional argument indicating whether the biSPOT approach should consider alarming values. Its default value is `True`. If `False`, SPOT will adapt the threshold assuming there are no abnormal values. |\n\n### Returns\n\n|  Type  | Description                                                                                             |\n| :----- | :------------------------------------------------------------------------------------------------------ |\n| `dict` | A dictionary containing the keys `thresholds` and `alarms`. The `thresholds` key contains the extreme quantiles while `alarms` contains the indexes of the values which have triggered alarms. If a warning is triggered (due to the algorithm already having been run and not reinitialized), an empty dictionary is returned. |\n\n### Example\n\n```python\n# Assuming 'biSPOT_instance' is an instance of a class containing the `run` function\nresult = biSPOT_instance.run(with_alarm=True)\n\n# Sample Output\n{\n    'thresholds': [0.15, 0.22, 0.35, 0.41, 0.56],\n    'alarms': [4, 15, 10, 27, 42]\n}\n```\nIn this example, the method is run on `biSPOT_instance` with the intention to raise alarms, and returns the recorded thresholds and alarms in a dictionary.",
    "20&@M.SPOT.dSPOT.plot": "# API Documentation\n\n## Signature\n```python\nplot(self, run_results, with_alarm=True)\n```\n\n## Function Summary\nThis function takes the `run_results` of a method and plots the data. The plot includes optional alarms.\n\n## Parameters\n* `run_results` (_dict_): The run results from the 'run' method. This dictionary may include keys such as 'thresholds' and 'alarms'.\n* `with_alarm` (_bool_, optional): This parameter controls if alarms are included in the plot. The default value is True.\n\n## Returns\n* _list_: This function returns a list of matplotlib's plot objects which represent the various elements of the constructed plot. This can include results data, thresholds, and alarms.\n\n## Example\n```python\n# Assuming 'run_results' is a dictionary containing keys such as 'thresholds' and 'alarms'\nrun_results_dict = {'thresholds': [10, 20, 30], 'alarms': [5, 15, 25]}\nplot_func = PlotClass() # Assuming a class with method 'plot'\nplots = plot_func.plot(run_results_dict)\n```\nIn the above example, the plot method will generate plot of the 'thresholds' and 'alarms' on the same graph. Then, it will return list of these plots represented as matplotlib plot objects.\n\n## Note\nThe color scheme used in the plot method is predefined with 'air_force_blue' for data, 'deep_saffron' for thresholds and 'red' for alarms which is not revealed in the function definition. Please refer to matplotlib documentation for more detailed usage of matplotlib plots. Also, the method will silently ignore the keys that are not recognized. This includes everything except 'thresholds' and 'alarms' for now.",
    "20&@M.SPOT.dSPOT.initialize": "## API Documentation\n\n---\n\n### Function Signature\n\n```python\ninitialize(self, verbose=True)\n```\n\n### Function Summary\n\nThis function represents the calibration or initialization step of a larger process. It involves computations on the initial data array, calculation of an initial threshold, the generation of peaks, and the calculation of a parameter named extreme quantile. The function also handles verbose logging — detailed printouts of the initialization process such as the initial threshold, number of peaks, and extreme quantile.\n\n### Parameters\n\n- `verbose` : `bool` (default = True)  \n    This parameter controls the verbosity of the initialization process. If `verbose` is set to `True`, detailed information about intermediate steps and variables (such as the initial threshold, number of peaks, and extreme quantile) will be printed to the console. \n\n### Returns\n\nAlthough the function seems to perform several computations and define several instance variables, it does not return any value.\n\n### Example\n\nAs the function is a method inside a class we do not have enough context to create a specific example on how to use it. However, in general, it could be utilized as shown below:\n\n```python\nclass SomeClass:\n    def initialize(self, verbose=True):\n        ...\n        \nsome_instance = SomeClass()\nsome_instance.initialize(verbose=True)\n```\n\nWhen `verbose=True` is set, this will print detailed information about the initialization process. If `verbose=False` is set, the function will remain silent, carrying out its computations without printing any information to the console.\n",
    "20&@M.SPOT.dSPOT.fit": "## API documentation\n\n**Signature**\n```python\ndef fit(self, init_data, data)\n```\n\n**Function Summary**\n\nThe `fit` function is used to import data to a DSPOT object. It validates and converts the input data and initial data into numpy arrays for further processing.\n\n**Parameters**\n\n- `init_data` (`list`, `numpy.array`, `pandas.Series`, `int`, `float`): It is the initial batch to calibrate the algorithm. If `init_data` is a number, it could be an integer value representing the index till where the data should be initialized or a float value between 0 and 1 exclusive which represents the proportion of data to initialize.\n- `data` (`list`, `numpy.array`, `pandas.Series`): It is the data that needs to be run in the DSPOT algorithm. \n\n**Returns**\n\nThis function does not return any value, it just updates the `self.data` and `self.init_data` attributes of the DSPOT object directly.\n\n**Exceptions**\n\nThis function prints error messages in case of unsupported types of `init_data` or `data`.\n\n**Example**\n\n```python\ndsp = DSPOT()\ninit_data = [1,2,3,4,5]\ndata = [6,7,8,9,10]\ndsp.fit(init_data, data)\n```\n\nIn this example, both `init_data` and `data` are lists and will be converted into numpy arrays and assigned to dsp's attributes `init_data` and `data`, respectively.",
    "20&@M.SPOT.dSPOT.add": "# API Documentation\n\n---\n\n## **Signature**\n```python\nadd(self, data)\n```\n\n## **Function Summary**\nThis function allows you to append new data to an existing data set. The existing data set is assumed to have been previously fitted to a model.\n\n## **Parameters**\n- `self`: The instance on which the method is invoked. No type or value is needed because it is automatically provided by Python.\n- `data` : This could be a list, numpy.array, or pandas.Series. This parameter is the new data that you aim to append to the existing dataset. \n\n## **Returns**\nIteration of this method does not return any explicit value. However, it modifies the `self.data` attribute of the instance by appending the new data.\n\nIn case an unsupported data format is provided, it prints a message stating that the format is not supported and returns `None` implicitly.\n\n## **Example**\n```python\nclass Example:\n    def __init__(self):\n        self.data = np.array([1, 2, 3])\n\n    def add(self,data):\n        if isinstance(data,list):\n            data = np.array(data)\n        elif isinstance(data,np.ndarray):\n            data = data\n        elif isinstance(data,pd.Series):\n            data = data.values\n        else:\n            print('This data format (%s) is not supported' % type(data))\n            return\n    \n        self.data = np.append(self.data,data)\n\ne = Example()\nprint(e.data)  # prints: array([1, 2, 3])\n\ne.add([4, 5, 6])\nprint(e.data)  # prints: array([1, 2, 3, 4, 5, 6])\n\ne.add(\"Unsupported\")  # prints: This data format (<class 'str'>) is not supported\nprint(e.data)  # prints: array([1, 2, 3, 4, 5, 6])\n```",
    "20&@M.SPOT.dSPOT._rootsFinder": "---\n\n## Signature\n```python\ndef _rootsFinder(fun, jac, bounds, npoints, method):\n```\n\n---\n\n## Function Summary\nThe function `_rootsFinder` is used to find possible roots of a scalar function within a given range. It uses two methods `regular` and `random` to define the initial X points that will be used for the roots search.\n\n---\n\n## Parameters\n- `fun (function)`: The scalar function for which to find possible roots.\n- `jac (function)`: The first order derivative of `fun`, i.e., the Jacobian.\n- `bounds (tuple)`: A tuple `(min, max)` representing the interval within which to search for roots.\n- `npoints (int)`: The maximum number of roots to output.\n- `method (str)`: A string representing the method to use for sampling the search interval. It can be either 'regular' for a regular sample or 'random' for a uniformly distributed sample.\n\n---\n\n## Returns\n- `numpy.array`: An array of potential roots of the scalar function, rounded to five decimal places and with duplicates removed.\n\n---\n\n## Example\n```python\n# Define a function and its derivative\ndef func(x):\n    return x**3 - x**2\n\ndef der_func(x):\n    return 3*x**2 - 2*x\n\n# Call _rootsFinder\nn_points = 10\nbounds = (-5, 5)\nmethod = 'regular'\npossible_roots = _rootsFinder(func, der_func, bounds, n_points, method)\n```\n\nThis would return a numpy array with potential roots for the function `func` within the interval (-5,5) based on a regular method sample. The exact output would depend on the `func` function and the given bounds.",
    "20&@M.SPOT.dSPOT._quantile": "# API Documentation\n\n## Function Signature\n```python\ndef _quantile(self, gamma: float, sigma: float) -> float:\n```\n\n## Function Summary\nThis function computes the quantile at level 1-q for the Generalized Pareto Distribution (GPD) with parameter γ, σ and threshold value μ = 0. \n\n\n## Parameters\n\n* `gamma` (float): This is the GPD parameter, typically referred to as the shape parameter. \n\n* `sigma` (float): This is the GPD parameter, usually referred to as the scale parameter.\n\n\n## Returns\n* `float`: The function returns the quantile at level 1-q for the GPD with parameters gamma, sigma, and threshold μ = 0. If `gamma` is not 0, the return value will be the calculated `gamma` quantile; if `gamma` is 0, the return value will be the calculated `sigma` quantile.\n\n\n## Example\n\n```python\n# assuming the class instance has already been created\n# defining gamma and sigma parameters for GPD\ngamma_val = 0.2\nsigma_val = 1\n\n# computing the quantile level\nquantile_val = instance._quantile(gamma_val, sigma_val)\nprint(quantile_val)\n```\n\nThe output would be a float value representing the calculated quantile level.\n\n**Note**: You would likely need some specific values for `self.n`, `self.proba` and `self.Nt` which appear to be attributes of the class where this method belongs. Ensure these attributes are correctly set before calling this method in a class instance.",
    "20&@M.SPOT.dSPOT._log_likelihood": "# API Documentation\n\n## _log_likelihood(Y,gamma,sigma)\n\n---\n\n### Signature\n\\_log_likelihood(Y: numpy.array, gamma: float, sigma: float) -> float\n\n### Function Summary\nThis function computes the log-likelihood for the Generalized Pareto Distribution (μ=0). It is used primarily in the field of statistics to understand the probability of observing a particular set of data under a chosen distribution.\n\n### Parameters\n* **Y** (_numpy.array_):  \n  It represents a collection of observations.\n\n* **gamma** (_float_):  \n  This is the GPD index parameter.\n\n* **sigma** (_float_):  \n  This parameter represents the GPD scale parameter and it should be greater than 0.\n\n### Returns\nThis function returns a **float** value which represents the log-likelihood of the sample `Y` to be drawn from a Generalized Pareto Distribution (GPD) with parameters γ (gamma), σ (sigma), and μ (mu) equalling 0.\n\n### Example\n\n```python\nimport numpy as np\n\nY = np.array([1,2,3,4,5])\ngamma = 0.5\nsigma = 1.2\n\nresult = _log_likelihood(Y, gamma, sigma)\nprint(result)\n```\n\nIn the above example, the function is called with a numpy array `Y` of observations, and `gamma` and `sigma` parameters for the GPD. The function computes the log-likelihood of the sample `Y` to be drawn from a GPD with given parameters and outputs the resulted float value.",
    "20&@M.SPOT.dSPOT._grimshaw": "# API Documentation\n\n## Signature \n```python\n_grimshaw(self, epsilon = 1e-8, n_points = 10)\n```\n\n## Function Summary\n\nThis function carries out the Generalized Pareto Distribution (GPD) parameters estimation using the Grimshaw's trick. \n\nIt uses numerical parameter to perform and the maximum number of candidates for maximum likelihood. After computing, it returns the estimates for gamma, sigma and the corresponding log-likelihood.\n\n## Parameters\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `epsilon` | FLOAT | The numerical parameter to perform. It has a default value of `1e-8`. |\n| `n_points` | INT | The maximum number of candidates for maximum likelihood. The default value is `10`. |\n\n## Returns\n\n| Return Value | Type | Description |\n|--------------|------|-------------|\n| `gamma_best` | FLOAT | The estimate value for gamma. |\n| `sigma_best` | FLOAT | The estimate value for sigma. |\n| `ll_best` | FLOAT | The corresponding log-likelihood for the estimated gamma and sigma values. |\n\n## Example \n\nThe `_grimshaw` function of class instance `spot`, can be called with the arguments `epsilon` and `n_points` as follows:\n\n```python\nspot._grimshaw(epsilon = 0.00001, n_points = 15)\n```\n\nTaken `spot.peaks` as a numpy array with values `[2.5, 3.6, 1.8, 2.9, 3.2]`, the function call is expected to output:\n\n```python\n(0.1596316156247468, 1.1620265035077923, -5.23212022074442)\n```\n\nThis result is the estimate for gamma, sigma, and corresponding log-likelihood, respectively.",
    "20&@M.SPOT.dSPOT.__str__": "## **API Documentation**\n\n---\n\n### **Signature**\n```python\n    def __str__(self)\n```\n\n### **Function Summary**\n\nThis function is used to return a formatted string that represents various details of a Peaks-Over-Threshold object.\n\n### **Parameters**\n\nThis is a method specific to the Peaks-Over-Threshold object class and does not take any additional parameter while calling. However, it uses the following attributes from the object:\n\n* `self.proba` (**Type:** float) - The detection level.\n* `self.data` (**Type:** np.array/None) - The data values. It would be None if no data is available.\n* `self.init_data.size` (**Type:** int) - The count of initial data values.\n* `self.n` (**Type:** int) - The total number of observations/data points.\n* `self.init_threshold` (**Type:** float) - The initial threshold value by the streaming algorithm.\n* `self.Nt` (**Type:** int) - The number of peak values.\n* `self.extreme_quantile` (**Type:** float) - Extreme quantile value.\n* `self.alarm` (**Type:** List) - A list of alarm trigger.\n\n### **Returns**\n\nThe function returns a descriptive string **(Type: str)** representing the details of the Peaks-Over-Threshold object.\n\n### **Example**\n\n```python\n    print(pot_obj) #- where pot_obj is the Peaks-Over-Threshold object\n```\n\n**Returns**\n\n```python\nStreaming Peaks-Over-Threshold Object\nDetection level q = 0.8\nData imported : Yes\n    initialization  : 100 values\n    stream : 5000 values\nAlgorithm initialized : Yes\n    initial threshold : 0.2\nAlgorithm run : Yes\n    number of observations : 4900 (98.00 %)\n    triggered alarms : 25 (0.50 %)\n```\nIn the above printout, 0.8 is the detection level, and there are 100 initial data points in a stream of 5000 values. The initialized algorithm had an initial threshold of 0.2. 98% of the time, the algorithm ran with observations, and 0.5% of the time, triggered alarms.",
    "20&@M.SPOT.bidSPOT.run": "## API Documentation for run function\n\n### Signature\n```python\nrun(self, with_alarm=True, plot=True)\n```\n\n### Function Summary\nThis function runs biDSPOT on the stream data. biDSPOT is an algorithm for anomaly detection in the stream data. The behavior is also controlled by optional flags to toggle the usage of an alarm and graphical visualization.\n\n### Parameters\n\n#### with_alarm : bool (default=True)\nFlag for deciding whether to enable the alarm feature. If this flag is set to False, SPOT will adapt the threshold assuming there are no abnormal values.\n\n#### plot : bool (default=True)\nFlag for deciding whether to enable the plot feature or not. This does not seem to affect the function in its current form.\n\n\n### Returns\n\n#### Return Type: \ndict\n\n#### Description:\nThe returned dictionary contains the keys: 'upper_thresholds', 'lower_thresholds' and 'alarms'. \n\n- '**upper_thresholds**' and '**lower_thresholds**' contain the extreme quantiles in the data. \n\n- '**alarms**' contains the indexes of the values which triggered alarms.\n\n### Example\n\n```python\nmy_instance.run(with_alarm=False, plot=False)\n```\nThis example runs the biDSPOT algorithm with both flags set to False. The function will return a dictionary as described in the Returns section.",
    "20&@M.SPOT.bidSPOT.plot": "# API Documentation \n\n## Function Signature\n\n```python\nplot(self, run_results: dict, with_alarm: bool = True) -> list\n```\n\n## Function Summary\nThis method takes the results from a previous run, given by the 'run' method, and plots them. It may also plot alarms based on the input settings. \n\n## Parameters\n1. `self`: this object instance. \n2. `run_results` (Dictionary):\n   - The function expects a dictionary containing the results from the 'run' method. This dictionary could potentially include three keys: 'upper_thresholds', 'lower_thresholds' and 'alarms', where each key maps to an array of values.\n3. `with_alarm` (Boolean):\n   -  Default value is True. If set to True, the plot includes the alarms if they exist in `run_results` dictionary. \n\n## Returns\n* `list`: A list of all the article plots drawn including the time series plot (ts_fig), upper threshold plot (uth_fig), lower threshold plot (lth_fig) and possibly, the alarms scatter plot (al_fig), if `with_alarm` parameter is set to True and 'alarms' exist in `run_results` dictionary.\n\n## Example\n```python\nrunresult = {\n    \"upper_thresholds\": [1, 2, 3, 4],\n    \"lower_thresholds\": [0.5, 1.5, 2.5, 3.5],\n    \"alarms\": [2, 3]\n}\nself.plot(runresult, with_alarm=True)\n```\nTo understand the output, you'll need to visualize the returned plot list which contains all the plots drawn. If alarms exist in the dictionary, they will be plotted as red dots on the plot.",
    "20&@M.SPOT.bidSPOT.initialize": "# API Documentation\n\n## Signature\n```python\ninitialize(self, verbose = True)\n```\n\n## Function Summary\n\nThe `initialize` function runs the calibration (initialization) step and calculates statistical parameters such as extreme quantile, sigma, likelihood, and gamma values based on the initial data and depth.\n\n## Parameters\n\n- `self` (type: instance): The object instance calling the function.\n- `verbose` (type: bool, default = True): If True, this gives detailed output about the batch initialization during processing. \n\n## Returns\n\nThe function does not return a value. Instead, it modifies the calling instance's state by computing various statistical parameters related to the initial data and printing them if verbose is set to True.\n\n\n## Example\n\n```python\n# Assume we have an object called 'calibrate_obj' of a class which has 'initialize() method and appropriate data are already set in the object\n\ncalibrate_obj.initialize(verbose=True)\n```\n\nThe function will calculate various statistical parameters and output them if verbose is set to True. For instance, here's example output:\n\n```\nInitial threshold: {'up': 1.1, 'down': -0.6}\nNumber of peaks: {'up': 750, 'down': 250}\nGrimshaw maximum log-likelihood estimation ... \nParameters                        Upper               Lower\n--------------------------------------------------------------\n                                γ                0.11                -0.05\n                                σ                0.5                  0.6\n                        likelihood              -150.52              -250.57\n                  Extreme quantile              11.09               -5.05\n--------------------------------------------------------------\n```\n\nNote: The function will not return any values but modify the object's internal state and print output if the verbose flag is set.",
    "20&@M.SPOT.bidSPOT.fit": "# API Documentation\n\n## Signature\n```python\nfit(self, init_data, data)\n```\n\n## Function Summary\nThe 'fit' function is used to import and assign data to a biDSPOT object. The data can be initial for calibration or for a run. The function accepts inputs in various formats including lists, numpy array or pandas series and is responsible for internal optimal handling and conversion if necessary.\n\n## Parameters\n1. **init_data**: (list, numpy.array, pandas.Series, int, float) - The data required to calibrate the algorithm. This could be a list, numpy array, or a pandas Series. If an integer is provided, it would assume this to be an index and fetch that part of the run-data for calibration. In case a float between 0 and 1 is given, it treats it as portion of the run-data for initialization.\n\n2. **data**: (list, numpy.array, pandas.Series) - This is the actual data that the algorithm will run on. It could be a list, numpy array, or pandas series.\n\n## Returns\nThe function does not return a value. Instead, it assigns input parameters to instance variables 'self.init_data' and 'self.data'. In case of unsuccessful assignments or unsupported data type, an error message will be printed.\n\n## Example\n```python\nclass BiDSPOT:\n    def fit(self, init_data, data):\n        ...\n        \nspot = BiDSPOT()\nspot.fit([1,2,3,4], [5,6,7,8,9])\n```\nIn the above example, the list [1, 2, 3, 4] will be set as the initial data for calibration and [5, 6, 7, 8, 9] will be the run-data for the algorithm.",
    "20&@M.SPOT.bidSPOT.add": "# API Documentation\n\n---\n\n## Function Signature\n\n```python\nadd(self, data)\n```\n\n## Function Summary\n\nThe `add` function is utilized to append additional data to the already fitted data. It handles several data types, such as list, numpy arrays, and pandas Series.\n\n## Parameters\n\n- `self` (required): This is a standard convention in Python for an instance method. It must always be the first argument.\n- `data` (list, numpy.array, pandas.Series): The given parameter `data` represents the data that needs to be appended to the existing data. \n\n## Returns\n\nThis function doesn't return any value but modifies the `data` attribute of the object on which it is called. It appends the provided data to the original data. In case of an unsupported data format, it prints an error message and doesn't alter the original data. \n\n## Example\n\n```python\nclass MyClass:\r\n    def __init__(self):\r\n        self.data = np.array([1, 2, 3])\r\n    \r\n    add(self,data)\r\n\r\nmy_object = MyClass()\r\nmy_object.add([4, 5, 6])\r\nprint(my_object.data)\r\n```\r\n\r\nThis will output: \n\n```python\narray([1, 2, 3, 4, 5, 6])\n```\n\nIn this example, the `add` method is called on the `my_object` instance of the class `MyClass`, and the list `[4,5,6]` is passed as the data parameter. This list is then converted to a numpy array and appended to the existing array `[1, 2, 3]`.\n\n\n> **NOTE:** This documentation assumes that we're in a class since `self` is used.",
    "20&@M.SPOT.bidSPOT._rootsFinder": "# API Documentation\n\n## Function Signature\n\n```python\n_rootsFinder(fun: Callable, jac: Callable, bounds: Tuple[float, float], npoints: int, method: str) -> np.ndarray\n```\n\n## Function Summary \n\nThis function finds the possible roots of a scalar function within a given range. It uses different sampling methods to identify the roots and utilises the `L-BFGS-B` method for minimization.\n\n## Parameters\n\n- `fun` : Callable\n\n  User-defined scalar function for which the roots are to be found.\n\n- `jac` : Callable\n\n  User-defined function that returns the first-order derivative of the `fun` parameter.\n\n- `bounds` : Tuple[float, float]\n\n  A tuple representing the interval (min, max) where the search for roots will be performed.\n\n- `npoints` : int\n\n  The maximum number of roots the function should output.\n\n- `method` : str\n\n  Specifies the type of sampling to be used on the search interval.\n  \n  - `'regular'`: Regularly spaced samples of the search interval.\n  - `'random'`: Uniformly distributed samples of the search interval.\n\n## Returns\n\n- `np.ndarray`\n\n  An array representing the unique possible roots of the function within the given bounds.\n\n## Example\n\n```python\nimport numpy as np\n\ndef fun(x):\n    return x**2 - 4\n\ndef jac(x):\n    return 2*x\n\nbounds = (-10,10)\nnpoints = 5\nmethod = 'regular'\n\n_rootsFinder(fun, jac, bounds, npoints, method)\n```\n\nOutput:\n\n```python\narray([-2., 2.])\n```\nThis example demonstrate finding the roots for the function `f(x) = x^2 - 4` in the range `(-10,10)` by sampling 5 points regularly. The expected roots are `-2` and `2` within the specified bounds.",
    "20&@M.SPOT.bidSPOT._quantile": "# API Documentation for `_quantile` function\n\n---\n\n## Signature\n```python\n_quantile(self,side: str,gamma: float,sigma: float) -> float\n```\n\n## Function Summary\nThis function computes and returns the quantile at level 1-q for a given side. It is specifically designed to handle inputs from a Generalized Pareto Distribution (GPD) model.\n\n## Parameters\n1. `side (str)`: This parameter specifies the side for which the quantile needs to be computed. It can hold either 'up' or 'down'.\n2. `gamma (float)`: This is a GPD parameter. It is used in the formula to compute the quantile.\n3. `sigma (float)`: This is another GPD parameter, also used in the formula to calculate the quantile.\n\n## Returns\n1. `(float)`: The function returns a float. This float represents the quantile at level 1-q for the GPD(γ,σ,μ=0).\n\n## Example\n```python\n_quantile('up', 0.2, 0.5)\n```\nThe above example demonstrates how the function is called with the parameters `'up'` for side, `0.2` for gamma, and `0.5` for sigma. The function would execute the calculations and provide the quantile value for the given parameters. Please note that the specific return value will depend on the values of `self.n`, `self.proba`, `self.Nt[side]` and `self.init_threshold['up']` in the context where this method is defined.",
    "20&@M.SPOT.bidSPOT._log_likelihood": "## API Documentation\n\n---\n\n### Signature\n```python\n_log_likelihood(Y, gamma, sigma)\n```\n\n### Function Summary\nThis function computes the log-likelihood for the generalized Pareto Distribution (GPD) with mean (μ) equal to 0. It provides a method to assess the probability that a given sample Y could be drawn from a GPD with specific γ (index parameter) and σ (scale parameter). \n\n### Parameters\n- `Y (numpy.array)`: This parameter represents the observations or the sample data for which the log-likelihood needs to be calculated. \n\n- `gamma (float)`: This is the index parameter (γ) of the generalized Pareto Distribution. The value of gamma shapes the distribution.\n\n- `sigma (float)`: This is the scale parameter (σ) of the generalized Pareto Distribution, which must be greater than 0. This parameter scales the distribution.\n\n### Returns\n- `float`: The return value is a float which represents the log-likelihood of the input sample (Y) being drawn from a generalized Pareto Distribution with the given γ (gamma) and σ (sigma) parameters.\n\n### Example\n```python\nimport numpy as np\nY = np.array([1,2,3,4,5])\ngamma = 2\nsigma = 1.5\n\nll = _log_likelihood(Y, gamma, sigma)\n\nprint(ll)\n```\n\nThis will calculate the log-likelihood of the array [1,2,3,4,5] being drawn from a generalized Pareto Distribution with γ = 2 and σ = 1.5.\nNote: The actual output value will depend on the internal calculations and could be a negative float as it's a log-likelihood.",
    "20&@M.SPOT.bidSPOT._grimshaw": "# API Documentation\n\n## Signature\n```python\n_grimshaw(self, side, epsilon = 1e-8, n_points = 8)\n```\n\n## Function Summary\nThis function computes the Generalized Pareto Distribution (GPD) parameters using Grimshaw's trick for extreme value theory, returning the estimates for gamma, sigma, and the corresponding log-likelihood.\n\n## Parameters\n- `self` (type: instance of the class)\n   This parameter refers to the instance of the class itself. It's not directly passed by the user.\n- `side` (type: Unknown)\n   The datatype and description of this parameter is not clear from the given context.\n- `epsilon` (type: float, default: 1e-8)\n   This parameter is a numerical value used for performing estimates.\n- `n_points` (type: int, default: 8)\n   This parameter specifies the maximum number of candidates for maximum likelihood.\n\n## Returns\nThis function returns a tuple of three values.\n- `gamma_best` (type: Unknown): Estimated gamma value.\n- `sigma_best` (type: Unknown): Estimated sigma value.\n- `ll_best` (type: Unknown): Corresponding log-likelihood value related to the best gamma and sigma estimates.\n\nNote: The exact data types of these return values are not clear from the given context.\n\n## Example\nThe function is part of a class and uses instance data so an isolated example cannot be provided without the definition and data setup for the entire class. However, calling the function would look something like:\n\n```python\ninstance = ClassName()  # ClassName is a placeholder for the actual class name\ngamma, sigma, ll = instance._grimshaw(\"side\")\n```\n\nReplace `ClassName` with the actual class name and `\"side\"` with an appropriate value for `side`.\n\nIn the example above, `gamma`, `sigma`, and `ll` would hold the gamma, sigma and log-likelihood estimates respectively.",
    "20&@M.SPOT.bidSPOT.__str__": "## API Documentation\n\n### Signature\n```python\n__str__(self)\n```\n\n### Function Summary\nThis function takes an instance of a Peaks-Over-Threshold Object as parameter and generates a comprehensive summary of its attributes. If the instance has data, it provides detailed information about the data, recorded observations, alarms triggered if the algorithm has run, or information about different thresholds and extreme quantiles. \n\n### Parameters\nThis function doesn't accept any parameters apart from the self\\-reference to instance this function is being called on.\n\n### Returns\n- **Type**: str\n- **Description**: A string representing the summary of the data attributes held by the Peaks-Over-Threshold Object. For example, it includes whether data has been imported, whether the algorithm has been initialized and run, the detection level, number of alarms triggered, initial threshold, number of observations, upper and lower extreme quantiles, and number of peaks.\n\n### Example\n\nConsider a Peaks-Over-Threshold Object named 'pot' initialized with desired data and probability. \n\n```python\nprint(pot.__str__())\n```\n\nThe output will be a summary string about the data attributes of the instance 'pot'. The exact values in the output will depend on specific instance attributes. A potential output could look like this:\n\n```python\n'Streaming Peaks-Over-Threshold Object\\n\nDetection level q = 0.95\\n\nData imported : Yes\\n\n\\t initialization : 100 values\\n\n\\t stream : 2000 values\\n\nAlgorithm initialized : Yes\\n\n\\t initial threshold : 0.7\\n\nAlgorithm run : Yes\\n\n\\t number of observations : 1000 (50.00 %)\\n\n\\t triggered alarms : 5 (0.25 %)\\n'\n```",
    "20&@M.SPOT.biSPOT.run": "# API Documentation\n\n## Signature\n```python\nrun(self, with_alarm: bool = True) -> dict\n```\n\n## Function Summary\nThis function runs the biSPOT algorithm on a given data stream. If a data point crosses a certain threshold, an alarm may be triggered. The function returns a dictionary with the upper and lower thresholds and the indices of alarm-triggering data points.\n\n## Parameters\n* **self** (*required*): This is the instance of the class where the function is being called.\n* **with_alarm** (bool, *optional, default=True*): This is a bool parameter which if set to False, the algorithm will adjust the threshold to reflect that there are no abnormal values. If set to True, the algorithm may trigger an alarm if a data value is above the established threshold.\n\n## Returns\nThe function returns a dictionary with three keys:\n**dict**\n* **'upper_thresholds'** (list[float]): A list containing the upper threshold value at each point in the data stream.\n* **'lower_thresholds'** (list[float]): A list containing the lower threshold value at each point in the data stream.\n* **'alarms'** (list[int]): List of indexes of the data points that have triggered alarms due to crossing established thresholds.\n\n## Example\n```python\nmy_spot_object = SPOT(b=0.99)             # Assuming SPOT class exists and 'b' is a parameter of its initialization function\nmy_spot_object.init_data = np.array([3,7,12,45,21,34,60,23]) # Initializing data\nmy_spot_object.data = np.array([22,14,46,30,41,22,47,25,33,60,34,20,80,30,45,27,49])  # Adding more data\nresult = my_spot_object.run(with_alarm=True)\n\nprint(result['upper_thresholds']) # Prints: [list of upper thresholds]\nprint(result['lower_thresholds']) # Prints: [list of lower thresholds]\nprint(result['alarms']) # Prints: [list of alarm indexes]\n```\nIn the Example section, we assume an object of class SPOT, my_spot_object, already exists and has appropriate initial data and data attributes. Data is a NumPy array of integers. The example demonstrates how to call the run function with the with_alarm parameter set to True. The output will be a dictionary where each key has a list of corresponding values.",
    "20&@M.SPOT.biSPOT.plot": "# plot Function\n\n## Signature\n```python\ndef plot(self, run_results, with_alarm=True)\n```\n\n## Function Summary\nThis method plots the results from a dictionary generated by the `run` method. It creates a base plot from `self.data` and optionally adds thresholds and alarm data to the graphic. The generated plots are then returned as a list.\n\n## Parameters\n- **self** (`object`): The instance of the object calling this method.\n- **run_results** (`dict`): The data to be plotted. This dictionary is generated by the `run` method of the same object.\n- **with_alarm** (`bool`, optional): Determines whether to include data from the 'alarms' key in the plot. The default is `True`.\n\n## Returns\n- **fig** (`list`): It contains a list of the plots generated. Each element in the list is a reference to a plotting object created in the method.\n\n## Example\n```python\n# Assuming `object_instance` is an instance of the class containing `plot` method \n# and `run_results` is the output of 'run' method\n\nresults = object_instance.plot(run_results)\n\n# To plot without alarms \nresults_without_alarms = object_instance.plot(run_results, with_alarm=False)\n```\n\nIn the above examples, the function will plot the data present in `run_results`. In the first example, alarms will be included in the plot if they exist in `run_results`. The second example will generate the plots without alarms regardless of whether they exist in `run_results`.\n",
    "20&@M.SPOT.biSPOT.initialize": "# API Documentation\n\n---\n## Signature\n```python\ninitialize(self, verbose = True)\n```\n---\n## Function Summary\nThe `initialize` function is used to run the calibration (initialization) step of the program. It sorts the initialized data and sets up the threshold values 'up' and 'down'. It then identifies and counts the number of peaks 'up' and 'down', computes Grimshaw's maximum log-likelihood estimation, and prints these details if the verbose parameter is set to True. \n\n---\n## Parameters\n- `self`: The instance of the class the initialize method is a part of. \n- `verbose`: Boolean (Default = True). If set to True, the function will provide details about the batch initialization.\n\n---\n## Returns\nThis function does not return a value. It performs the initialization operation in-place.\n\n---\n## Example\n```python\nclass MyClass:\n    # Assumed class variables and methods\n    init_data = np.array([5, 2, 1, 3, 4])\n    init_threshold = {'up': None, 'down': None}\n    peaks = {'up': [], 'down': []}\n    Nt = {'up': 0, 'down': 0}\n    n = 0\n    extreme_quantile = {'up': None, 'down': None}\n    gamma = {'up': None, 'down': None}\n    sigma = {'up': None, 'down': None}\n\n    def _grimshaw(self, side):\n      pass\n\n    def _quantile(self, side, g, s):\n      pass\n\n    #initialize function\n    #...\n\nobj = MyClass()\nobj.initialize()\n```\nGiven the class has other required methods properly defined, the `initialize` function would work properly. However, without context, it is not possible to provide the function's expected output.\n",
    "20&@M.SPOT.biSPOT.fit": "# fit\n\n## Signature\n`fit(self, init_data:Union[List, np.ndarray, pd.Series], data:np.ndarray) -> None`\n\n## Function Summary\nThis method is used to import data to a biSPOT object.\n\n## Parameters\n- `init_data: Union[List, np.ndarray, pd.Series]`: The initial dataset used to calibrate the algorithm. This can be a list, numpy array or a pandas Series. If it's an integer, it is considered an index, so the method will select the first `init_data` elements from `data` for calibration. If it's a float between 0 and 1, it's treated as a ratio, so `init_data*len(data)` elements starting from the first one are selected for calibration.\n\n- `data: np.ndarray`: The input dataset for the run mode. This should be a list, numpy array, or pandas Series. \n\n## Returns\nThis method does not return a value. It assigns the input data to the `data` attribute and initial data to `init_data` attribute of the biSPOT object. If the data types of provided inputs are not supported, respective error messages are printed.\n\n## Example\nLet's say you have a biSPOT object `bspot`, and you want to set up its initial data with first 3 elements of `data: np.array([1, 2, 3, 4, 5, 6, 7])` for calibration and the rest to run mode:\n```\nbspot.fit(init_data=3, data=np.array([1, 2, 3, 4, 5, 6, 7]))\n```\nAfter the execution, `bspot.init_data` will be `[1, 2, 3]` and `bspot.data` will be `[4, 5, 6, 7]`.",
    "20&@M.SPOT.biSPOT.add": "# Add Function API Documentation\n\n---\n\n## Signature\n\n```python\ndef add(self, data)\n```\n\n---\n\n## Function Summary\n\nThe `add()` function is used to append new data to existing data that has been previously fitted. The incoming data can be of types list, numpy.array, or pandas.Series. If the data is not of these types an error message will be displayed, and the function will terminate without making any changes to the existing fitted data.\n\n---\n\n## Parameters\n\n| Parameter | Type                             | Description                       |\n| --------- | -------------------------------- | --------------------------------- |\n| `self`    | Self                             | Represents the instance of a class|\n| `data`    | list, numpy.array, pandas.Series | The data to be appended to the existing fitted data  |\n\n---\n\n## Returns\n\nThis function does not have a specific return value. It updates the internal state of the object by appending the provided data to `self.data`. If the data provided is not a list, numpy.array, or pandas.Series, it prints out an error message and terminates, thereby not causing any changes to `self.data`.\n\n---\n\n## Example  \n\n```python\nobj = ClassName() #Replace ClassName with the name of the class 'add' belongs to\ndata_to_append = [2, 3, 5]\nobj.add(data_to_append)\n# self.data now contains the appended data\n```\n\nIn this example, we first create an object `obj` of the class where `add` belongs to. We then call `obj.add()` with the list `[2, 3, 5]` as an argument. The function will then append the elements `2`, `3`, and `5` to `self.data` of the object `obj`.\n",
    "20&@M.SPOT.biSPOT._rootsFinder": "# API Documentation\n\n## Signature\n\n`_rootsFinder(fun: Callable, jac: Callable, bounds: Tuple[int, int], npoints: int, method: str) -> np.array`\n\n## Function Summary\n\nThis function aims to find possible roots of a scalar function within a given range and by a specified method. It either adopts a regular interval sampling or a uniform (distribution) sampling of the search interval, according to the method parameter.\n\n## Parameters\n\n`fun (Callable):` \nA scalar function for which the root(s) is to be found. It should be callable and accept a single argument.\n\n`jac (Callable):`\nThe first order derivative of the function. It should also be callable and accept a single argument.\n\n`bounds (Tuple[int, int]):`\nA tuple consisting of two integers that design the interval in which to search for the roots. The first integer is the minimum bound, and second is the maximum.\n\n`npoints (int):`\nThe maximum number of roots to output. It should be a positive integer.\n\n`method (str):`\nA string that determines the sampling of the search interval. Can be either 'regular' for regular sampling or 'random' for a uniform distribution sample.\n\n## Returns\n\n`np.array:`\nThe function will return a NumPy array of unique possible root values of the function, rounded to 5 decimal places.\n\n## Example\n\n```\nfun = lambda x: x**3 - 3*x + 1\njac = lambda x: 3*x**2 - 3\nbounds = (-3,3)\nnpoints = 20\nmethod = 'random'\n\nprint(_rootsFinder(fun, jac, bounds, npoints, method))\n```\n\nThe output would be the potential roots of the function `x**3 - 3*x + 1` within the interval of `-3` to `3`.",
    "20&@M.SPOT.biSPOT._quantile": "---\n# API Documentation for Function: _quantile\n\n## Signature\n```python\n_quantile(self,side, gamma, sigma)\n```\n\n## Function Summary\nThis function calculates the quantile at level 1-q for a given side ('up' or 'down') according to the Generalised Pareto Distribution (GPD) parameters.\n\n## Parameters\n1. **`self`** : *object*\n    - The object instance that the function is a part of. This will be automatically passed to the function when it's called as a method of an object. \n\n2. **`side`** : *str*\n    - A string parameter indicating the side, can be either 'up' or 'down'.\n\n3. **`gamma`** : *float*\n    - The shape parameter of the GPD.\n\n4. **`sigma`** : *float*\n    - The scale parameter of the GPD.\n\n## Returns\n**`float`**\n- The quantile at level 1-q for the specified side using GPD parameters.\n\n## Example\nCalling the function:\n```python\nmyInstance._quantile('up', 0.5, 1.0)\n```\n\nThis would compute the 'up' quantile with GPD parameters gamma as 0.5 and sigma as 1.0, and return the calculated float value.\n- Note: 'myInstance' is the instance of the class that _quantile function belongs to.\n\nIn case a wrong value for side is input, “error: the side is not right” will be printed.\n\n```python\nmyInstance._quantile('middle', 0.5, 1.0)\n```\n\nThis would return: \"error: the side is not right\" as 'middle' is not a valid input for 'side'.\n",
    "20&@M.SPOT.biSPOT._log_likelihood": "# API Documentation\n\n## Signature\n```python\ndef _log_likelihood(Y, gamma, sigma)\n```\n\n## Function Summary\nThe function `_log_likelihood` computes the log-likelihood for the Generalized Pareto Distribution (μ=0). The computation uses the input values of observations and two GPD parameters.\n\n## Parameters\n1. `Y` (numpy.array): A numpy array consisting of observations. No constraint is placed on its length, it can be as long or short as required.\n2. `gamma` (float): The GPD index parameter, it can be any real number.\n3. `sigma` (float): The GPD scale parameter. It should always be greater than 0.\n\n## Returns\n* `float`: The function returns a single floating point number which represents the log-likelihood of the sample Y to have originated from a GPD(γ,σ,μ=0) distribution.\n\n## Example\n```python\nimport numpy as np\nY = np.array([2.3, 3.6, 1.9, 2.7])\ngamma = 0.2\nsigma = 0.5\nresult = _log_likelihood(Y, gamma, sigma)\n```\nFor these values, the function `_log_likelihood` returns the computation of the log-likelihood for the Generalized Pareto Distribution (μ=0). The observed result would be a float output.\n\n*Please note that the actual output would depend on the input parameters you put into the function*.",
    "20&@M.SPOT.biSPOT._grimshaw": "## API Documentation\n\n---\n\n### Function Signature\n``` python\n_grimshaw(self, side, epsilon = 1e-8, n_points = 10)\n```\n\n### Function Summary\n\nThe function `_grimshaw` is used to compute the Generalized Pareto Distribution (GPD) parameters estimation with the utilization of Grimshaw's trick.\n\n### Parameters\n\nThe function takes the following parameters:\n\n| Parameter | Type | Description |\n| :------------ | :---- | :----------------- |\n| `self` | object | The instance of the class in which `_grimshaw` is a method. |\n| `side` | string | The side from which peaks need to be fetched (usually 'left' or 'right'). |\n| `epsilon` | float | A numerical parameter to perform computations, defaults to 1e-8. |\n| `n_points` | int | Maximum number of candidates for maximum likelihood estimation, defaults to 10. |\n\n### Returns\n\nThe function returns the following:\n\n| Return value | Type | Description |\n| :-------------- | :---- | :----------------- |\n| `gamma_best` | float | The best estimate for the gamma parameter. |\n| `sigma_best` | float | The best estimate for the sigma parameter. |\n| `ll_best` | float | The value of the corresponding log-likelihood for the best gamma and sigma. |\n\n### Example\n\n``` python\nclass biSPOT:\n    peaks = {'left': np.array([1, 2, 3]), 'right': np.array([4, 5, 6])}\n    _log_likelihood = staticmethod(lambda x, y, z: y*z)\n    \nbiSPOT()._grimshaw('left', 1e-8, 10)\n```\n\n**Expected Output:**\n\n``` python\n(0.0, 2.0, 4.0)\n```\n\nNote: `_log_likelihood` and `peaks` are used as simple mock representations to make the example runnable. In a real scenario, they represent more complex behaviours.",
    "20&@M.SPOT.biSPOT.__str__": "## API Documentation\n\n### Signature\n```python\n__str__(self)\n```\n\n### Function Summary\nThe `__str__` function is a built-in string representation method in Python. In this specific context, it is used to visualize and understand the state of a Streaming Peaks-Over-Threshold (SPOT) object. This function returns a detailed string representation of the SPOT object's attributes including detection level, data status, initialization state, and algorithm run details.\n\n### Parameters\nThis method doesn't take any parameters apart from the implicit `self` parameter which refers to the instance of the SPOT object.\n\n### Returns\n\n- **Type**: `str`\n- **Description**: Returns a string representation explaining the state of the SPOT object.\n\n### Example\n\n```python\n# Assuming an instance `spot` of the SPOT object is already created\nprint(spot)\n```\n\nThe output of this command will return a string detailing the state of the SPOT object such as:\n\n```\nStreaming Peaks-Over-Threshold Object\nDetection level q = 0.95\nData imported : Yes\n     initialization  : 1000 values\n     stream : 5000 values\nAlgorithm initialized : Yes\n     initial threshold : 1.5\nAlgorithm run : Yes\n     number of observations : 4000 (80.00 %)\n     triggered alarms : 200 (4.00 %)\n```\n\n_Note: The actual content of the string depends on the state and parameters of the SPOT instance._",
    "20&@M.SPOT.backMean": "---\n\n## **Function Signature**\n```python\ndef backMean(X,d):\n```\n---\n\n## **Function Summary**\n\nThe `backMean()` function calculates a backward moving average on an input array, over a certain \"window\" length `d`. \n\n---\n\n## **Parameters**\n\n- **X** (numpy array, or similar list-like object): This is the input array on which the backward moving average is calculated. The array can contain any numeric type (int, float, etc.).\n  \n- **d** (integer): This is the window length for the moving average calculation. It represents the number of elements to consider in the mean calculation. \n\n---\n\n\n## **Returns**\n\n- numpy array (float): The function returns a numpy array of the calculated backward moving averages. Each element in the output array corresponds to the moving average of the previous `d` elements in the input array `X`.\n\n---\n\n## **Example**\n```python\nX = np.array([2, 4, 6, 8, 10])\nd = 2\nprint(backMean(X, d))\n```\nexpected output:\n```python\narray([3., 5., 7., 9.])\n```\nIn this example, for an input `X` array of [2, 4, 6, 8, 10], and window length `d` of 2, the function calculates moving averages of:\n- (2+4)/2 = 3\n- (4+6)/2 = 5\n- (6+8)/2 = 7\n- (8+10)/2 = 9\n",
    "20&@M.SPOT.SPOT.run": "# API Documentation\n\n## Function Signature\n```python\ndef run(self, with_alarm = True):\n``` \n\n## Function Summary\nThis function performs the Sequential Probability Ratio Test (SPOT) on a data stream. The SPOT algorithm operates on the principle of setting alarm thresholds and identifying abnormal values (known as \"alarms\") in the data stream when data exceeds the thresholds. The function can be operated in two modes which can be controlled with the `with_alarm` boolean flag; triggering alarms with abnormal values or simply updating the thresholds without triggering alarms.\n\n## Parameters\n1. **self**: No type restriction, The instance of the class wherein this function is defined. No default value.\n2. **with_alarm (bool)**: If False, SPOT will not trigger alarms when abnormal values are encountered and instead adapt the threshold. Defaults to True.\n\n## Returns\n**return type**: dict\n\nThe return dictionary has two keys `'thresholds'` and `'alarms'`.\n\n1. **'thresholds'**: Stores the extreme quantiles calculated during the operation of the function. The quantile updates every time an abnormal value peaks above the initial threshold.\n2. **'alarms'**: Contains the indexes of the values which have triggered alarms. This key is present only if `with_alarm` is set to True.\n\n## Example\n```python\n# Assuming we have an instance of the class \"spot\"\noutput = spot.run(with_alarm = True)\nprint(output['thresholds'])\nprint(output['alarms'])\n```\nIn the example above, the function is run for the 'spot' instance with alarms triggering enabled. The thresholds and alarm indexes are then printed.\nNote: This function must be a method inside a class for it to perform properly because it relies heavily on instance variables not shown in the function such as `self.n`, `self.init_data.size`, `self.data.size` etc. This function cannot be run as a standalone function.",
    "20&@M.SPOT.SPOT.plot": "## API Documentation\n\n---\n\n### Signature\n```python\nplot(self, run_results, with_alarm = True)\n```\n\n### Function summary\n\nThis function creates and returns a list of plots based on the given run results dictionary. By default, the function also includes alarm plots (if alarm data exists in the run results).\n\n### Parameters\n\n1. **self** (required): No data type required. The parameter refers to the instance calling the function. This is a standard convention in Python's class method definition.\n\n2. **run_results** (dict, required): A dictionary container that holds results given by the 'run' method. It is expected to contain keys like 'thresholds' or 'alarms' based on which the plots are drawn.\n\n3. **with_alarm** (bool, optional): A flag that determines whether or not to plot alarms. The default value is `True`. If set to `True` and if 'alarms' key is present in the `run_results`, alarm data will be plotted.\n\n### Returns\n\n- **fig** (list): The return is a list of plots created from 'thresholds', 'alarms' (if 'with_alarm' is set to `True`), and other data from `run_results` dictionary.\n\n### Example\n\n```python\n# Assuming that 'obj' is an instance of the class that has the 'plot' function\n# and 'run_result' is a dictionary with the required keys.\n\nrun_result = {\n    'thresholds': [10, 20, 30, 40, 50],\n    'alarms': [15, 25, 35, 45, 55]\n}\n\nplt.figure()\nobj.plot(run_result, with_alarm=True)\nplt.show()\n```\n\nIn this example, the function will create 3 plots from `run_results` (using 'thresholds' data, 'alarms' data, and original data) and return a list containing the 3 plots. The plots would be displayed when `plt.show()` is executed.\n",
    "20&@M.SPOT.SPOT.initialize": "## API Documentation\n\n---\n\n### Signature\n```python\ndef initialize(self, level = 0.98, verbose = True)\n```\n\n### Function Summary\nThe `initialize` function runs the calibration or initialization step of the algorithm. It calculates the initial threshold, extracts initial peaks, calculates the maximum log-likelihood estimation with the Grimshaw method, as well as the extreme quantile of the data. All the calculated details can be printed out if the verbose flag is set to True.\n\n### Parameters\n\n- __level (float, optional)__: Probability associated with the initial threshold t. It defaults to 0.98 if not specified.\n- __verbose (bool, optional)__: If set to True, additional details about the batch initialization are printed out. It is set to True by default.\n\n### Returns\n\nThe function does not return a value. Instead, it modifies the attributes of the class instance.\n\n### Example\n\n```python\nclass_instance.initialize(level=0.9, verbose=True)\n```\n- Upon calling the function this way, it will calibrate the algorithm with the specified settings. If the dataset is large, the details about the initialization will be outputted. \n\n__Note__: Please note that since this function is a method in a class, to call this function you first have to create an instance of the class. In the example above, `class_instance` is an instance of the class this function belongs to.",
    "20&@M.SPOT.SPOT.fit": "---\n\n## **Function Signature**\n```python\ndef fit(self, init_data, data)\n```\n\n## **Function Summary**\nThis method inputs data into a SPOT object. It accepts data as list, numpy array or pandas series, and converts them into the appropriate format for internal processing.\n\n## **Parameters**\n\n- **init_data (numpy.array, pandas.Series, list, int, float)**: This is the initial batch to calibrate the algorithm. It can be either a list, numpy array, pandas series, an integer, or a float. For number types, it's used as an index to slice the `data` array where all elements before this index inclusively are selected as initial data.\n\n- **data (numpy.array, pandas.Series, list)**: This is the actual data for the run. It can be either a list, numpy array or pandas series.\n\n## **Returns**\nThis function doesn't return any value. It updates the internal state of the SPOT object by setting `self.init_data` and `self.data`.\n\n## **Error Messages**\n- \"This data format (`type`) is not supported\": Thrown when the `data` parameter is not in the specified acceptable formats.\n- \"The initial data cannot be set\": Thrown when the `init_data` is neither list, numpy array, pandas.Series, nor int or float number type which could be used to slice the `data` array.\n\n## **Example**\n```python\nspot = SPOT() # Assuming SPOT object initialized\nspot.fit([1,2,3,4], [5,6,7,8]) # input data as list\n\n# Or use numpy array\nspot.fit(np.array([1,2,3,4]), np.array([5,6,7,8]))\n\n# Or pandas series\nspot.fit(pd.Series([1,2,3,4]), pd.Series([5,6,7,8]))\n\n# Using integer or float to slice data for initialization\nspot.fit(2, np.array([5,6,7,8])) # Uses first two elements of data for initialization\nspot.fit(0.5, np.array([1, 2, 3, 4, 5, 6, 7, 8])) # Uses first half of data for initialization\n```",
    "20&@M.SPOT.SPOT.add": "# API Documentation\n\n---\n\n## Function Signature\n`add(self, data)`\n\n## Function Summary\nThis function is used to append additional data to an existing dataset. It supports list, numpy.array, and pandas.Series data types. If successful, the method will update the 'self.data' attribute with the newly appended data.\n\n## Parameters\n- `self`: Represents the instance of the class. This parameter is implicit and doesn't need to be provided when calling the `add` function.\n- `data` (`list`, `numpy.array`, `pandas.Series`): The input data to be appended to the existing dataset. It must be of type list, numpy.array, or pandas.Series.\n\n## Returns\nThis function doesn't return a value but updates 'self.data' upon successful execution. If the data format is unsupported, it prints an error message and exits without making any changes to 'self.data'.\n\n## Example\nAssume we have a class `Dataset` that implements the `add` function:\n\n```python\nclass Dataset:\n#Class definition goes here...\n\nds = Dataset()\nds.add([1, 2, 3, 4])\nprint(ds.data) \n# output: array([1, 2, 3, 4])\n\nds.add(np.array([5, 6, 7, 8]))\nprint(ds.data)\n# output: array([1, 2, 3, 4, 5, 6, 7, 8])\n\nds.add(pd.Series([9, 10, 11, 12]))\nprint(ds.data)\n# output: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n```\n\nTrying to append unsupported data type:\n\n```python\nds.add({\"A\":1, \"B\":2, \"C\":3})\n# output: This data format (<class 'dict'>) is not supported\n```",
    "20&@M.SPOT.SPOT._rootsFinder": "# API Documentation\n\n## Signature\n```python\n_rootsFinder(fun, jac, bounds, npoints, method)\n```\n\n## Function Summary\nThis function finds possible roots of a scalar function within a defined interval. It applies a method ('regular' or 'random') to sample the search interval and uses optimization to minimize the objective function which is the sum of the squares of the function values and it's first order derivative.\n\n## Parameters\n\n- `fun` *(function)*: The scalar function of which roots are to be found.\n\n- `jac` *(function)*: The first order derivative of the scalar function `fun`. \n\n- `bounds` *(tuple)*: A tuple `(min, max)` representing the interval within which to search for the roots.\n\n- `npoints` *(int)*: The maximum number of roots to output.\n\n- `method` *(str)*: The method used to sample the search interval, either 'regular' or 'random'. \n\n    - 'regular': Regular sampling of the search interval\n    - 'random': Uniform distribution sample of the search interval\n\n## Returns\n\n- `numpy.array`: An array of possible roots of the function, rounded to five decimal places. The returned roots are unique (duplicates are removed).\n\n## Example\n\n```python\nimport numpy as np\n\n# Define scalar function and its derivative\nfun = np.sin\njac = np.cos\n\n# Call _rootsFinder function\nroots = _rootsFinder(fun, jac, (-1, 1), 10, 'regular')\n\nprint(roots)\n```\n\nThe output might look like the following, depending on the regular sampling values:\n\n```\n[-0.99104 -0.93154 -0.80485 -0.54074  0.07379  0.64857  0.81862  0.93121  0.99069]\n```\n\nThis output represents the computed roots of the sin(x) function in the interval (-1, 1) for a regular sampling method. Note that the resulting roots values can vary.",
    "20&@M.SPOT.SPOT._quantile": "# API Documentation\n\n---\n\n## Function signature\n_quantile(self, gamma: float, sigma: float) -> float\n\n---\n\n## Function Summary\nThis function calculates the quantile at the level 1-q for Generalized Pareto Distribution (GPD) given its parameters γ and σ. The GPD is used in extreme value theory.\n\n---\n\n## Parameters\n\n- `gamma` _(float)_: A GPD parameter, known as the shape parameter. It determines the tail heaviness of the distribution. Generally, this parameter should not fall into an excluded range. For a GPD, gamma should not be equal to zero to avoid a division by zero error.\n\n- `sigma` _(float)_: A GPD parameter, known as the scale parameter. It dictates the dispersion or width distribution of the GPD. It must be a strictly positive value.\n\n---\n\n## Returns\n\n- _float_: The function returns a float value, representing the quantile at level 1-q for the Generalized Pareto Distribution specified by the γ (gamma) and σ (sigma) parameters.\n\n---\n\n## Example\n\nAssuming we have an instantiated object `obj` of the class this function belongs to:\n\n```python\n>>> obj._quantile(0.5, 2.0)\n# This will return a float value, which represents the quantile level at 1-q for the GPD with γ=0.5 and σ=2.0.\n```\n\nPlease note that the above example assumes that we have the appropriate `self.n`, `self.proba`, `self.Nt`, and `self.init_threshold` variables initialized within the class object. The actual output would depend on these variables. \n\nThis function is private (due to the underscore prefix) and typically it's not meant to be directly accessed from outside the class.",
    "20&@M.SPOT.SPOT._log_likelihood": "# Log-likelihood for the Generalized Pareto Distribution Function\n\n---\n\n## Signature\n```python\ndef _log_likelihood(Y: numpy.array, gamma: float, sigma: float) -> float\n```\n\n## Function Summary\nThis function computes the log-likelihood for the Generalized Pareto Distribution (GPD) with mean equal to 0. It takes as input an array of observations, the GPD index parameter, and the GPD scale parameter.\n\n## Parameters\n\n- `Y` (`numpy.array`): The array of observations for which the log-likelihood is calculated.\n- `gamma` (`float`): The GPD index parameter. The value is a float and denotes shape parameter of the distribution.\n- `sigma` (`float`): The GPD scale parameter. It must be greater than zero and determines the scale of the distribution.\n\n## Returns\nThe function returns a `float` representing the log-likelihood of the sample Y being drawn from a Generalized Pareto Distribution with parameters γ, σ and μ=0. \n\n## Example\n\n```python\nimport numpy as np\nY = np.array([0.1, 0.2, 0.15, 0.17, 0.22])\ngamma = 0.5\nsigma = 0.1\n\nlog_likelihood = _log_likelihood(Y, gamma, sigma)\nprint(log_likelihood) \n# Output: -4.34513363400946\n```\n\nThis example demonstrates how to use the `_log_likelihood` function. It calculates the log-likelihood of array Y being drawn from a Generalized Pareto Distribution with specified γ and σ. The resulting log-likelihood value is printed to the console.",
    "20&@M.SPOT.SPOT._grimshaw": "---\n# Grimshaw Method\n\n## Signature\n```python\n_grimshaw(self, epsilon = 1e-8, n_points = 10)\n```\n## Function Summary\nComputes the Generalised Pareto Distribution (GPD) parameter estimation using Grimshaw's trick which involves likelihood estimation.\n\n## Parameters\n\n- `epsilon` (float, optional): This is a numerical value targeting the precision. It enhances the accuracy of computation. Default value is 1e-8.\n\n- `n_points` (int, optional): Maximum number of candidate points for which the maximum likelihood is calculated. Default value is 10.\n\n## Returns\n\n- `gamma_best` (float): Represents the gamma estimation after calculation\n\n- `sigma_best` (float): Represents the sigma estimation after calculation\n\n- `ll_best` (float): Holds the respective maximum likelihood value\n\nThese are the estimated Generalised Pareto Distribution parameters and their corresponding log-likelihood value. \n\nThe function returns these as a tuple (gamma_best, sigma_best, ll_best).\n\n## Example\n\nThis is a private method, used within the context of a larger class (not shown in this example). Here's an example of how it might be called within that class:\n\n```python\nclass TestClass:\n    def __init__(self):\n      self.peaks = np.array([1, 2, 3, 4, 5])\n\n    def _grimshaw(self, epsilon = 1e-8, n_points = 10):\n        # function body goes here\n\ntest_instance = TestClass()\ngamma_best, sigma_best, ll_best = test_instance._grimshaw(1e-10, 15)\n```\nIn the above example, epsilon is set to 1e-10 and n_points is set to 15. The function will return gamma_best, sigma_best, ll_best based on the attribute `peaks` of class object `test_instance`. The outputs would be numerical values.\n",
    "19&@M...test_netmedic": "### API Documentation for `test_netmedic` Function\n\n#### Signature:\n\n```python\ntest_netmedic(\n    data_source=\"real_micro_service\",\n    history_range=(0, -100),\n    current_range=(-100, None),\n    bin_size=100,\n    affected_node=14,\n    true_root_cause=[28],\n    verbose=False,\n    runtime_debug=False,\n    pc_aggregate=1,\n    pc_alpha=0.1,\n    *args,\n    **kws\n)\n```\n\n#### Function Summary:\n\nThe `test_netmedic` function performs diagnosis on a network by computing dependencies and impact graphs to identify root causes of issues within the network. It supports various parameters for custom analysis, including debugging and verbose output. It also allows saving results and intermediate data points to Excel files.\n\n#### Parameters:\n\n- `data_source` (str): The name of the data source to be analyzed.\n- `history_range` (tuple): A tuple representing the range of history to use in the analysis.\n- `current_range` (tuple): A tuple representing the range of current data to use in the analysis.\n- `bin_size` (int): The size of each bin when aggregating data.\n- `affected_node` (int): The node identifier that is affected and under analysis.\n- `true_root_cause` (list of int): A list of node identifiers representing the true root cause.\n- `verbose` (bool): Flag to output verbose logs; higher levels produce more output.\n- `runtime_debug` (bool): Flag to enable debug mode; if True, skips the usage of cached files.\n- `pc_aggregate` (int): The parameter for the PC algorithm's data aggregation process.\n- `pc_alpha` (float): The significance level for conditional independence tests in the PC algorithm.\n- `args` (*tuple): Variable length argument list for additional parameters.\n- `kws` (**dict): Arbitrary keyword arguments for providing additional options like custom data.\n\n##### Keyword Args:\n\n- `data` (np.ndarray): Optional. Custom dataset to analyze in place of loading from the source.\n- `data_head` (list): Optional. The headings for the data.\n- `window_start` (int): Optional. The starting index for the window of analysis.\n- `dep_graph` (np.ndarray): Optional. Precomputed dependency graph to use instead of generating a new one.\n- `disable_print` (bool): Optional. If True, does not print the performance metrics.\n\n#### Returns:\n\n- `tuple` of `(prkS, acc)`:\n  - `prkS` (float): The precision-recall at k statistic for the ranking of nodes.\n  - `acc` (float): The accuracy of the ranking based on the provided true root cause.\n\n#### Example:\n\n```python\n# Example call to test_netmedic function\nprecision_recall_at_k, accuracy = test_netmedic(\n    data_source=\"real_micro_service\",\n    history_range=(0, -100),\n    current_range=(-100, None),\n    bin_size=100,\n    affected_node=14,\n    true_root_cause=[28],\n    verbose=1\n)\n\n# Expected Output\n# ############# Net Medic ###############\n# precision_recall_at_k and accuracy would be printed if verbose is enabled and 'disable_print' is not True.\n# The function will return the tuple (precision_recall_at_k, accuracy).\n```\n\nThis function is typically used in the context of diagnosing and understanding the behavior of complex networks and their failures. It provides a detailed, algorithmic approach to root cause analysis with a focus on the PC algorithm for structure learning.",
    "18&@M...tbac_param_search": "# API Documentation\n\n## Function Signature\n```python\ndef tbac_param_search(pc_aggregate, pc_alpha, entry_point, true_root_cause, data_inwindow, data_head, dep_graph, window_start):\n```\n\n## Function Summary\nThe `tbac_param_search` function is used to perform parameter searching for the TBAC algorithm. It iterates over all the possible parameters except the PC parameters, tests them, and returns the performance results (PRKS and ACC) along with the time taken for the search.\n\n## Parameters\n- `pc_aggregate` (type: any) - The aggregate metrics of Procedure Call (PC).\n- `pc_alpha` (type: any) - Alpha value parameter for Procedure Call (PC).\n- `entry_point` (type: any) - The entry point where procedure call begins.\n- `true_root_cause` (type: any) - The real root cause which the algorithm is expected to determine.\n- `data_inwindow` (type: any) - The dataset which is within the specified timeframe (window).\n- `data_head` (type: any) - Header information for the dataset.\n- `dep_graph` (type: any) - Dependency graph for the procedure call.\n- `window_start` (type: any) - The start time of the window for which the function operates.\n\n## Returns\nA list of dictionaries where each dictionary contains:\n- `time` (type: float) - The time consumed for the parameter search operation. \n- `prks` (type: float) - The Pseudo-Ranked Precision, Recall, F-Score performance measure result after testing.\n- `acc` (type: float) - The accuracy of root cause determination after testing.\n\n## Example\n```python\nresult = tbac_param_search(3, 0.5, \"entry\", \"root_cause\", data_window, data_head, dep_graph, 0)\nprint(result)\n```\nIn this example, the function is being called with some placeholder values. The function will use these parameters to perform the search and testing, and then return the time consumed for the operation, the `prks` and the `acc`. The output will be something like:\n```python\n[{'time': 100.52803802490234, 'prks': 0.75, 'acc': 0.68}]\n```",
    "18&@M...netmedic_param_search": "**Signature**\n```\nnetmedic_param_search(entry_point, true_root_cause,\n                      pc_aggregate, pc_alpha, data_inwindow, data_head, dep_graph, window_start)\n```\n\n**Function Summary**\nThis function searches for potential parameters for NetMedic through iterations. It excludes Product code (PC) parameters and returns a list of parameter results.\n\n**Parameters**\n- `entry_point` `(str)`: Entry point for the network service.\n- `true_root_cause` `(str)`: The actual root cause of the issue.\n- `pc_aggregate` `(bool)`: Boolean to decide if to aggregate product code data.\n- `pc_alpha` `(float)`: The alpha level for the Product Code (PC) parameters.\n- `data_inwindow` `(dict)`: The window of data to examine the network service.\n- `data_head` `(dict)`: The initial data state for network service.\n- `dep_graph` `(dict)`: Dependency graph of the network service architecture.\n- `window_start` `(str)`: The starting point of the data window.\n\n**Returns**\n- `result_list` `(list)`: List of dictionaries. Each dictionary contains the history range, current range, bin size, time taken for computation, prks, and accuracy.\n\n**Example**\n```python\nresults = netmedic_param_search(\"entry1\", \"service_failure\",\n                                True, 0.05, \n                                {\"data\": 100}, {\"head_data\": 20}, \n                                {\"dep1\":\"dep2\"}, \"2022-03-03 12:00:00\")\n\n# Example result\n[\n    {\n        'history_range': (0, 50),\n        'current_range': (200, 250),\n        'bin_size': 5,\n        'time': 1.1221327781677246,\n        'prks': prks_object,\n        'acc': acc_score\n    }, \n    ...\n]\n```\nIn this example, we're searching for parameters of a network service with a given entry point, root cause, and other relevant parameters. This function returns a list of dictionaries that provides parameter combinations and their corresponding metrics.",
    "18&@M...monitorrank_param_search": "# monitorrank_param_search Function\n\n## Signature\n\n```python\nmonitorrank_param_search(pc_aggregate, pc_alpha, entry_point, true_root_cause, data_inwindow, data_head, dep_graph, window_start)\n```\n\n## Function Summary\n\nThis function executes the monitor rank algorithm over a range of parameter values and returns the results for each run. The results include execution time, accuracies, and the monitor rank scores alongside their parameters. This function is especially useful for testing and optimizing the performance of the monitor rank algorithm.\n\n## Parameters\n\n- `pc_aggregate`  (_any type_): Parameter for the monitor rank algorithm. Different types of aggregates could be applied depending on usage.\n- `pc_alpha` (_float_): The alpha parameter for the monitor rank algorithm.\n- `entry_point` (_string_): Entry point of the system where analysis begins.\n- `true_root_cause` (_any type_): The actual root cause.\n- `data_inwindow` (_Pandas DataFrame_): Data within a specific time window.\n- `data_head` (_Pandas DataFrame_): Some initial portion of the data.\n- `dep_graph` (_dictionary_): Dependency graph that represents the relationships between different entities in the system.\n- `window_start` (_int_ or _datetime_): The starting point of the window for the data being analyzed.\n\n## Returns\n\n- _List of dictionaries_: Each dictionary in the list corresponds to the result of running monitor rank algorithm with a specific parameter `rho`. Each dictionary contains:\n  - `rho` (_float_): The rho parameter used for this run.\n  - `time` (_float_): The time taken (in seconds) for this monitor rank algorithm run.\n  - `prks` (_any type_): The monitor ranks computed for this run.\n  - `acc` (_float_): The accuracy achieved for this run.\n\n## Example\n\n```python\nresults = monitorrank_param_search(\n    pc_aggregate=\"mean\",\n    pc_alpha=0.5,\n    entry_point=\"service_A\",\n    true_root_cause=\"error_B\",\n    data_inwindow=df_window,\n    data_head=df_head,\n    dep_graph=dep_graph,\n    window_start=0\n)\nfor result in results:\n    print(\"Parameters: rho={:.2f} Time: {:.4f} seconds Accuracy: {:.4f}\"\n          .format(result['rho'], result['time'], result['acc']))\n```\nThe above example will run the function with the provided parameters and print out each run's time, rho value, and accuracy.",
    "18&@M...load_data": "## API Documentation\n\n---\n\n### **Signature**\n`load_data(data_source)`\n\n### **Function Summary**\nThis function is responsible for loading data from the specifed 'data_source'. The data is loaded by using `load` function present in `utils.loaddata` package. The function returns a tuple containing the data and the data header.\n\n### **Parameters**\n\n| Parameter   | Type | Description   | \n| :---        | :----:  | :---        |\n| `data_source`  | String  | It specifies data location. The expected value can be either 'pymicro' or 'ibm_micro_service'.  |\n\n\n### **Returns**\n\n| Return   | Type | Description   | \n| :---        | :----:  | :---        |\n| `data`   | Numpy Array  | A numpy array of the data where each column is a variable, and the shape is `[T, N]`.   |\n| `data_head` | Variable type, typically a string | Generally represents the header of the data file, but this can vary based on the specific implementation of the load method in 'utils.loaddata'. |\n\n### **Example**\n\n```python\ndata, data_head = load_data('pymicro')\n```\n\n*Note: The actual data and data_head returned depends on the dataset present in the given 'data_source'.*",
    "18&@M...dycause_param_search": "## API Documentation\n\n### Signature\n```python\ndycause_param_search(entry_point, true_root_cause, data_inwindow, data_head, window_start)\n```\n\n### Function Summary\n\nThis function performs an extended Granger causality test, iterating across a fixed parameter space excluding the PC parameters. The Granger causality test is typically used to explore potential causality relations in the system represented by the input parameters, and store the results in a list for later analysis.\n\n### Parameters\n\n- `entry_point` (string) : Indicates the initial input point for the Granger causality test.\n- `true_root_cause` (string) : Provides the actual root cause which the Granger causality test attempts to identify.\n- `data_inwindow` (Pandas DataFrame): Represents the structured data used during the test within the defined window of examination.\n- `data_head` (Pandas DataFrame): Provides additional metadata or leading data for the Granger causality test.\n- `window_start` (integer or datetime): Defines the starting point of the data window that will be used for the test.\n\n### Returns\n\n- Returns `List[Dict]`: A list of dictionaries. Each dictionary carries the parameters used in an individual Granger causality test iteration and the achieved accuracy ('acc') for that set of parameters. The time taken ('time') for each iteration is also included in the dictionary.\n\n### Example\n\n```python\ndata_inwindow = pd.read_csv(\"data_inwindow.csv\")\ndata_head = pd.read_csv(\"data_head.csv\")\n\nresult = dycause_param_search(\n    entry_point=\"frontend_service\",\n    true_root_cause=\"issue_in_db\",\n    data_inwindow=data_inwindow,\n    data_head=data_head,\n    window_start=0)\n\nprint(result)\n# It should display a list of dictionaries with the documented information.\n```\n\n>Note: Make sure to have the necessary function `test_dycause` and its prerequisite data and libraries before running the function.\n>Please also note that the required import statement for Pandas is: `import pandas as pd`.",
    "18&@M...cloudranger_param_search": "## API Documentation\n\n### Signature\n```python\ncloudranger_param_search(ela, alpha, entry_point, true_root_cause, data_inwindow, data_head, dep_graph, window_start)\n```\n\n### Function Summary\nIt performs a grid search on `beta` and `rho` parameters from 0.1 to 1.0 with increment of 0.2, running the `test_cloud_ranger` function with different parameter combinations. It adds the results and the function run time of each iteration to a `result_list` and returns this list.\n\n### Parameters\n- `ela (int)`: Represents the parameter PC aggregate in the test_cloud_ranger function.\n- `alpha (float)`: Represents the parameter PC alpha in the test_cloud_ranger function.\n- `entry_point (str)`: Represents the front-end service or system to be tested.\n- `true_root_cause (str)`: Represents the actual root cause of an issue.\n- `data_inwindow (data type)`: A data structure containing data within a specific window.\n- `data_head (data type)`: A structure containing the first part of the data to be processed.\n- `dep_graph (graph object)`: A graph object showing dependencies between nodes.\n- `window_start (time object)`: The start time of the test window.\n\n(Note: the specific type of `data_inwindow`, `data_head` and `window_start` are not provided in function hence mentioned as 'data type' and 'time object'. Please update it as per actual types)\n\n### Returns\n- `result_list (list)`: A list of dictionaries, where each dictionary contains a set of parameters `ela`, `beta`, `rho` and their respective `time`, `prks` and `acc`values. \n    - `'ela': ela`: the PC aggregate value used\n    - `'beta': beta`: the beta value used\n    - `'rho': rho`: the rho value used\n    - `'time': toc`: time taken by the function to execute\n    - `'prks': prks`: prks value returned by the test_cloud_ranger function\n    - `'acc': acc`: acc value returned by the test_cloud_ranger function\n\n### Example\n\n```python\nresult = cloudranger_param_search(5, 0.5, 'entry1', 'root_cause1', data_in_window_val, data_head_val, graph_val, start_time_val)\nprint(result)\n# Expected Output (based on your specific inputs): \n# [ {'ela': 5, 'beta': 0.1, 'rho': 0.1, 'time': 0.35, 'prks': 0.89, 'acc': 0.8},\n#   {'ela': 5, 'beta': 0.1, 'rho': 0.3, 'time': 0.45, 'prks': 0.78, 'acc': 0.7},\n#    ........\n#   {'ela': 5, 'beta': 0.9, 'rho': 0.9, 'time': 0.37, 'prks': 0.95, 'acc': 0.9}]\n```\nThis is a mock output, actual result will depend on the provided arguments and the portion of the code that are abstracted for this function.",
    "18&@M...build_common_pc_graph": "# API Documentation\n\n## Signature\n\n```python\ndef build_common_pc_graph(data: numpy.ndarray, aggre_delta: int, alpha: float, window_start: int) -> Tuple[numpy.ndarray,float]:\n```\n\n## Function Summary\nThis function builds a PC (Peter-Clark) dependency graph using the provided parameters. If a cache file with the same parameters exists, it loads the cache file instead of building a new graph. The execution time of the PC algorithm is also recorded and returned. If a cached file is used, execution time will be -1.\n\n## Parameters\n- `data`(numpy.ndarray): The input data as a Numpy array with shape `[T,N]`. \n\n- `aggre_delta`(int): The aggregation size which represents the data resolution. \n\n- `alpha`(float): The significance value to be used in the PC algorithm.\n\n- `window_start`(int): The window start position in the raw data.\n\n## Returns\n- `dep_graph`(numpy.ndarray): The generated dependency graph in matrix format.\n\n- `toc`(float): The execution time of the PC algorithm in seconds. If a cached file is used, this will be -1.\n\n## Example\n\n```python\ndata = np.array([[1,2,3],[4,5,6],[7,8,9]])\naggre_delta = 1\nalpha = 0.1\nwindow_start = 0\n\ndep_graph, toc = build_common_pc_graph(data, aggre_delta, alpha, window_start)\n\nprint(dep_graph)\nprint(toc)\n```\nOutput could be:\n\n```python\n[[0 1 0]\n [1 0 0]\n [1 0 0]]\n0.001\n```\nThis represents the generated dependency graph and the execution time of the PC algorithm. If a cached file was used, toc would be `-1`.",
    "17&@M...test_cloud_ranger": "# API Documentation\n\n---\n\n## **Function Signature**\n\n```python\ntest_cloud_ranger(data_source=\"real_micro_service\", pc_aggregate=5, pc_alpha=0.1, testrun_round=1, frontend=18, true_root_cause=[6, 13, 28, 30, 31], beta=0.3, rho=0.2, save_data_fig=False, verbose=False, runtime_debug=False, *args, **kws)\n```\n\n## **Function Summary**\n\nRuns a series of steps including data loading, preprocessing, dependency graph generation, calculation of ranking, and measurement of performance accuracy based on precision and recall. It also handles caching & reloading previous results, enables flexibility in data source handling and trial repetition, and provides verbosity for debug information.\n\n## **Parameters**\n\n- `data_source` (str, default=\"real_micro_service\"): The source from which the raw data is loaded.\n- `pc_aggregate` (int, default=5): Integer value representing aggregation delta for time-series data pre-processing.\n- `pc_alpha` (float, default=0.1): Parameter of PC algorithm used for graph building.\n- `testrun_round` (int, default=1): Number of trial repetitions for performance metrics computation.\n- `frontend` (int, default=18): Index of the node where the random walk starts.\n- `true_root_cause` (list of int, default=[6, 13, 28, 30, 31]): Indices of the true root causes.\n- `beta` (float, default=0.3): Parameter used in the calculation of the ranking.\n- `rho` (float, default=0.2): Parameter used in the calculation of the ranking.\n- `save_data_fig` (bool, default=False): Flag whether to save transition matrix and graph or not.\n- `verbose` (bool, default=False): Flag to enable display of verbose/debugging information or not.\n- `runtime_debug` (bool, default=False): Flag to enable runtime debug mode where each process is always executed.\n\n## **Returns**\n\nReturns a tuple containing the list of precision values at multiple k and the accuracy. \n\n- `prkS` (list of float): The list of precision values at multiple k (PR@k).\n- `acc` (float): The accuracy value.\n\n## **Example**\n\nExample of test_cloud_ranger function call,\n\n```python\nprecision_at_k, accuracy = test_cloud_ranger(\n    data_source=\"sample_data_source\",\n    pc_aggregate=4,\n    pc_alpha=0.1,\n    testrun_round=2,\n    frontend=10,\n    true_root_cause=[2, 8, 15],\n    beta=0.3,\n    rho=0.2,\n    save_data_fig=True,\n    verbose=True,\n    runtime_debug=True\n)\n\nprint(f\"Precision at K: {precision_at_k}\")\nprint(f\"Accuracy: {accuracy}\") \n```\n\nThis would generate precision at multiple cut-off points and overall accuracy based on the parameters provided and data loaded from `'sample_data_source'`. Outputs will be saved. Debug mode will execute each process and verbose mode will print relevant statements and method parameters.",
    "17&@M...secondorder_randomwalk": "## Function Signature \n\n```python\nsecondorder_randomwalk(M, epochs, start_node, label=[], walk_step=1000, print_trace=False)\n```\n\n## Function Summary \n\nThe function performs a second order random walk on a given graph, represented by a transition matrix. Scores are computed based on the frequency of node visitation during the random walk. The scores are sorted in descending order and returned in a list of tuples in the form of (node, score).\n\n## Parameters\n\n- **M (numpy array)**: A 2D square numpy array representing a transition Matrix. The element M[i,j] represents the probability of transitioning from node i to node j in the graph.\n- **epochs (int)**: The number of walks to perform.\n- **start_node (int)**: The node in the graph from where the walk will start.\n- **label (list)**: A list of labels of nodes, default is empty list. The length of this list should be equal to the number of nodes in the graph.\n- **walk_step (int)**: The number of steps to be taken in each walk, default is 1000.\n- **print_trace (boolean)**: If set to True, the function will print the trace of the walk, default is False.\n\n## Returns\n\n- **score_list (list of tuples)**: This list contains tuples (label, score) sorted in descending order according to the score.\n\n## Example\n\n```python\nimport numpy as np\n\nM = np.array([[0, 0.5, 0.5, 0], \n              [1/3, 0, 1/3, 1/3], \n              [1/2, 1/2, 0, 0],\n              [0, 1, 0, 0]])\n\nlabel = ['A', 'B', 'C', 'D']\n\nstart_node = 1\nepochs = 100\n\nsecondorder_randomwalk(M, epochs, start_node, label)\n```\n\nThis may produce the output similar to:\n\n```python\n[('D', 250), ('B', 240), ('C', 240), ('A', 130)]\n```\nHere, node D is visited most frequently, followed by B and C, and finally A in 100 walks.",
    "17&@M...relaToRank": "# Function Documentation\n\n## **Signature**\n```python\nrelaToRank(rela, access, rankPaces, frontend, beta=0.1, rho=0.3, print_trace=False)\n```\n\n## **Function Summary**\nThis function endeavours to unravel the ranking structure based on the information about the relations between entities. For example, the function could be used for a social network analysis by determining the level of significance (relevance, rank, importance) of each node (social actor, individual, user).\n\n## **Parameters**\n- **rela (list of lists of int)**: Represents the importance or relevance of individual nodes. Each list corresponds to an individual node, and its elements indicate the relevance of the node to other nodes in the network.\n- **access (list of lists of int)**: Provides information about the existence of connections from one node to another within the network.\n- **rankPaces (list)**: Represents the pace (the frequency or entrance points) of random walks used for iterating through the network.\n- **frontend (int)**: Index of starting/front node in the path of the network.\n- **beta (float, optional)**: Probability for random jumps in the path of the network. Default is 0.1.\n- **rho (float, optional)**: Probability to add backward edges. Default is 0.3.\n- **print_trace (bool, optional)**: Indicates whether to print the trace of the algorithm or not. Default is False.\n\n## **Returns**\n- **l (list)**: Ranks of each node based on the information provided.\n- **P (list of lists of int)**: Matrix of the likelihood of the existence of connections from one node to another.\n- **M (3D-Numpy array of int)**: Tensor which provides the transition probabilities among nodes in the presence of a connection. \n\n## **Example**\n```python\nrela = [[1,1,0],[0,1,0],[0,1,1]]\naccess = [[1,0,0],[1,1,0],[1,1,1]]\nrankPaces = [1,2,3]\nfrontend = 1\n\nrank, P, M = relaToRank(rela, access, rankPaces, frontend)\n\nprint(rank)\nprint(P)\nprint(M)\n```\nPlease note that this example contains arbitrary values and real-world implementation may differ based on the specific use case of the function. The written function might need additional imported modules (like 'numpy', a module for performing secondorder_randomwalk, guiyi function) which are not defined in provided function.",
    "17&@M...guiyi": "---\n## Function: guiyi\n\n---\n\n**Signature:**\n```python\nguiyi(p)\n```\n\n**Function Summary:**\n\nThe function is used to normalize a given matrix column-wise.\n\n**Parameters:**\n\n1. `p` (`list` of `list` of `int` or `float`): The input is a matrix represented as a list of lists. Each sub-list represents a row in the matrix. The elements of the sub-lists should be numerical - either integers or floats.\n\n**Returns:**\n\nThis function returns a `list` of `list` (`float`). The output is a matrix (represented as a list of lists) that has been normalized column-wise.\n\n**Example:**\n\n```python\nimport numpy as np\n\np = [[1, 2, 3], [2, 3, 4], [3, 4, 5]]\nnormalized_p = guiyi(p)\n\nprint(normalized_p)\n```\nThe output will be:\n```python\n[[0.3333333, 0.4, 0.5],\n [0.6666666, 0.6, 0.75],\n [1.0, 0.8, 1.0]]\n```\nIn this example, we normalize the given 3x3 matrix using the `guiyi` function. The result is a 3x3 matrix that has been normalized column-wise.",
    "16&@M...worker_process": "# API Documentation\n\n## **Signature**\n```python\ndef worker_process(ind: int, params_dict: Dict[str, Any]) -> Tuple[Dict[str, Any], float, int, float]:\n```\n\n## **Function Summary**\nThis function runs a worker process for extended Granger causality tests on given dataset parameters, timing the execution and returning relevant test results along with the execution time.\n\n## **Parameters**\n\n- `ind` (int): An identifier for the worker process. Represents the process number or id.\n\n- `params_dict` (dict): A dictionary containing all the necessary parameters to run the 'test_granger_extend' function. These include the following:\n  - `'ela'` (float): Aggregation delta for data.\n  - `'bef'` (int): Length of time before start_time to consider.\n  - `'aft'` (int): Length of time after start_time to include.\n  - `'step'` (int): Step size for constructing the Granger graph.\n  - `'sig_value'` (float): Significant level threshold.\n  - `'lag'` (int): Time lag value.\n  - `'thres'` (float): A threshold ratio for auto_threshold feature.\n  - `'max_path_length'` (int): Maximum length of paths considered for root cause analysis.\n  - `'mean_method'` (str): The statistical method used to average data.\n  - `'topk_path'` (int): Number of top paths selected for evaluation.\n  - `'num_sel_node'` (int): Number of selected nodes in each path.\n\n## **Returns**\n\n- `prks` (Dict[str, Any]): The results of the granger extend test, structured as a dictionary.\n\n- `acc` (float): The accuracy of the results.\n\n- `ind` (int): The input identifier for the worker process.\n\n- `toc` (float): The time taken to complete the granger extend test in seconds.\n\n## **Example**\n\nHere is a simple example on how to call this function:\n\n```python\nparams = {\n    'ela': 2.0,\n    'bef': 10,\n    'aft': 10,\n    'step': 1,\n    'sig_value': 0.05,\n    'lag': 1,\n    'thres': 0.8,\n    'max_path_length': 3,\n    'mean_method': 'mean',\n    'topk_path': 5,\n    'num_sel_node': 2\n}\n\nresult = worker_process(1, params)\nprint(result)\n```\nThis will output a tuple which contains test results, accuracy, identifier, and execution time for the worker process.",
    "16&@M...main": "## **Function Name**: main\n\n### **Signature**:\n```python\ndef main():\n```\n\n### **Function Summary**:\nThe `main` function starts IBM Micro Service Test Suite by executing tests provided by a global list, `params_list`. Each test (which presumably involves performing some operations on the IBM Micro Service and recording metrics or results) is submitted to a process pool executor, which performs these tasks asynchronously in a separate process. The function records success and failure of these tasks, and any resulting metrics or exceptions. The output is stored in a `.pkl` file after all tasks are done.\n\n### **Parameters**:\nThis function does not take in any parameters.\n\n### **Returns**:\nThis function does not have a return, but it outputs a `.pkl` file containing a list of results gathered from executing the tasks defined in `params_list`. Each element in the `result_list` includes the parameters used for the task, the PRKS (unknown - not clearly defined in function), the ACC (unknown - not clearly defined in function), and the runtime of the task.\n  \n### **Exceptions**:\nErrors during the execution of tasks are caught and the number of failed tasks are accumulated and printed out with the exception details. However, the function completes its execution regardless of these exceptions.\n\n### **Example**:\n```python\n# considering the function can't be called without predefined variables, this is a hypothetical use case:\n\nparams_list = [{'param1': 1, 'param2': 2}, {'param1': 3, 'param2': 4}]\n\ndef worker_process(i, params_dict):\n    # this function would define what the tests are\n    # assume this just returns some arbitrary metrics/results for demo\n    return 'prks', 'acc', i, 'runtime'\n\nmain()\n# This would execute the tests defined by worker_process on params_list\n# and save the results in 'granger_extend_runtime_ibm_708.pkl'\n```\n\nNote: This documentation assumes that `params_list` and `worker_process` are defined in the global scope where `main` is called.",
    "15&@M.netmedic.enum_path": "# API Documentation\n\n## Signature\n```python\nenum_path(impact_graph, i, affected_node)\n```\n\n## Function Summary\n\nThis function explores all the possible paths from a given node `i` to the `affected_node` on a given `impact_graph`. It uses Breadth-first search (BFS) to navigate this graph and returns all adjustable paths discovered.\n\n## Parameters\n\n|    Name    | Type |  Description  |\n| :---       | :---: |   :---       |\n| `impact_graph` | `numpy.ndarray`  | A Numpy array representing the impact graph where each element `impact_graph[i,j]` indicates the weight of the edge from node `i` to node `j`. `0` indicates no edge.|\n| `i`  | `integer` | The starting node in the graph from which paths should be examined. |\n| `affected_node` | `integer` | The node to which paths should be identified. |\n\n## Returns\n\n|    Name    | Type |  Description  |\n| --- | --- | --- |\n| `path_list` | `set` | A set of all discovered paths from the `i` node to the `affected_node` on the `impact_graph`. Each path is represented as an ordered tuple of nodes.|\n\n## Example\n```python\nimport numpy as np\n\ngraph = np.array([\n    [0, 1, 1, 0],\n    [0, 0, 1, 0],\n    [0, 0, 0, 1],\n    [0, 0, 0, 0]\n])\nprint(enum_path(graph, 0, 3))\n```\nThis will output:\n```python\n{(0, 1, 2, 3), (0, 2, 3)}\n```\nIt means there two adjustable paths from 0 to 3 node in the graph: through the node 1 and 2, and directly through the node 2 respectively.",
    "15&@M.netmedic.divide_mean": "---\n\n## **Signature**\n`divide_mean(data: numpy.ndarray, K: int) -> numpy.ndarray:`\n\n---\n\n## **Function Summary**\nThis function takes a dataset and divides it into 'K' equal segments. It then calculates the mean of each segment. This is particularly useful in data analysis scenarios where the dataset needs to be divided and the mean of each segment calculated.\n\n---\n\n## **Parameters**\n\n- `data` (numpy.ndarray): The input dataset. This should be a 2D array where rows represent individual data entries and columns represent features.\n- `K` (int): The number of divisions or chunks to split the dataset into.\n\n---\n\n## **Returns**\n\n- Return Type: numpy.ndarray\n- Description: The function returns a 2D numpy array with the same number of rows as the input 'data'. Each column corresponds to the mean of a segment from the dataset. \n\n---\n\n## **Example**\n\n```python\nimport numpy as np\ndata = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                 [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]])\nK = 2\n\nprint(divide_mean(data, K))\n```\n\nThe expected output would be:\n\n```python\narray([[ 5.5,  1.5],\n       [15.5, 11.5]])\n```\n\nHere, the input data array is divided into 'K' (=2) chunks. For each chunk, the mean is calculated. The output array contains these individual mean values.\n",
    "15&@M.netmedic.compute_max_path_weight": "# API Documentation\n\n## Signature\n```python\ndef compute_max_path_weight(all_path, edge_weight)\n```\n\n## Function Summary\nThe `compute_max_path_weight` function calculates the geometric mean of weights for each path in `all_path` and returns the maximum geometric mean among them.\n\n## Parameters\n1. **all_path** (`list` of `list` of `int`): A list of paths where each path is represented by an ordered list integers indicating the nodes.\n2. **edge_weight** (`dict`): A dictionary that includes the weight of each edge. Each key is a tuple of two integers that represents two nodes from a path, and the associated value is the weight of the edge.\n\n## Returns\nThis function returns a `float` that represents the maximum geometric mean of path weights. If the `all_path` list is empty, the function will return `0.01`.\n\n## Example\n```python\npaths = [[1, 2, 3], [2, 3, 4]]\nweights = {(1,2):3, (2,3):4, (2,3):2, (3,4):6}\ncompute_max_path_weight(paths, weights)\n```\nThis will output `3.4641016151377544`, which is the maximum geometric mean computed from both provided paths' weights. Notice that `(1,2)` weight is `3`, `(2,3)` weight is `2` and `(3,4)` is `6`. For the first path, the geometric mean is `2.44948974` and for the second it's `3.4641016151377544`.\n",
    "15&@M.netmedic.compute_impact_matrix": "**Signature**\n```python\ndef compute_impact_matrix(impact_graph: numpy.ndarray, edge_weight: numpy.ndarray) -> numpy.ndarray\n```\n\n**Function Summary**\n\nThis function computes and returns the impact Matrix for a given impact graph and its edge weights. The function iterates over every combination of nodes in the graph. If the processed node is equal to the current node, the corresponding impact matrix value will be 1.0. Otherwise, the function will enumerate all paths between the two nodes and the impact matrix value will be the sum of all the edge weights along the path with the maximum total weight.\n\n**Parameters**\n\n- **impact_graph**(`numpy.ndarray`): A 2D numpy array representing the connectivity between nodes in a graph. The row and column indices represent nodes, and the element at a particular row and column indicates whether there is a connection between the nodes.\n\n- **edge_weight**(`numpy.ndarray`): A 2D numpy array representing the weight of every edge in the graph. The row and column indices represent nodes, and the element at a particular row and column signifies the weight of the edge between those nodes.\n\n**Return**\n\n- This function returns a `numpy.ndarray`, which represents the calculated impact matrix. Each element at index [i, j] in this 2D array represents the sum of impact weights from node i to node j.\n\n**Example**\n```python\nimport numpy as np\nimpact_graph = np.array([[0, 1, 1], [0, 0, 1], [1, 0, 0]])\nedge_weight = np.array([[0, 2, 3], [0, 0, 2], [1, 0, 0]])\nprint(compute_impact_matrix(impact_graph, edge_weight))\n```\nOutput:\n```python\narray([[1., 2., 5.],\n       [1., 1., 2.],\n       [1., 1., 1.]])\n```",
    "15&@M.netmedic.compute_global_impact": "## API Documentation\n\n---\n\n### **Signature**\n\n```python\ndef compute_global_impact(impact_matrix, abnormality)\n```\n\n### **Function Summary**\n\nThe `compute_global_impact` function computes the global impact by multiplying each element of an impact matrix with corresponding elements in abnormality vector, aggregating the results, and storing these in a new vector. It implements this operation in a traditional for-loop manner to clarify the process.\n\n### **Parameters**\n\n- **impact_matrix** (`numpy.ndarray`): The impact matrix used for the computation. It is expected to be a 2D square numpy array. Each element represents the local impact of an associated abnormality.\n\n- **abnormality** (`numpy.ndarray`): The abnormality vector. It is expected to be a 1D numpy array. Each element represents the abnormality degree of the associated item.\n\n### **Returns**\n\n- **global_impact** (`numpy.ndarray`): The function returns a 1D numpy array. Each element of the returned vector corresponds to a total global impact caused by associated item's abnormality and its local impacts.\n\n### **Example**\n\n```python\nimport numpy as np\n\nimpact_matrix = np.array([[1, 0.5, 0.2], [0.1, 2, 0.3], [0.2, 0.4, 3]])\nabnormality = np.array([0.5, 0.7, 0.9])\n\nglobal_impact = compute_global_impact(impact_matrix, abnormality)\n\nprint(global_impact)\n```\n\nOutput:\n\n```python\narray([0.740, 1.430, 3.000])\n```\nThe above sample code shows the usage of the `compute_global_impact` function. The function iterates over each column `c` of the `impact_matrix`, then over each entry `e` within that column, multiplies each `impact_matrix[c, e]` with the corresponding `abnormality[e]`, and adds up these products for that column to compute the `global_impact[c]`. The resulting `global_impact` numpy array is printed out.",
    "15&@M.netmedic.compute_edgeweight": "**Signature:**\n```python\ncompute_edgeweight(data, impact_graph, history_range, current_range, bin_size=100, delta=0.33)\n```\n\n**Function Summary:**\nThe function `compute_edgeweight` calculates the edges' weights of a graph based on provided data and ranges. It processes historical and current data, dividing it into bins, and computes weights based on the relative difference between historical and current state of each data point.\n\n**Parameters:**\n- `data` (_ndarray_): numpy array containing the data used to compute the edge weights.\n- `impact_graph` (_ndarray_): numpy array representing the graph to which edges' weights will be computed.\n- `history_range` (_tuple_): Tuple that specifies the slice of data for historical context.\n- `current_range` (_tuple_): Tuple that specifies the slice of data for current context.\n- `bin_size` (_int, optional_): Integer that specifies the size of the bin to divide historical data. Default is `100`.\n- `delta` (_float, optional_): Threshold for difference in calculation of weight. Default is `0.33`.\n\n**Returns:**\n- The function returns a numpy array (`ndarray`), which is a 2D matrix of the same shape as `impact_graph` filled with edge weights. Each cell [i,j] represents the weight of the edge from node `i` to node `j`.\n\n**Example:**\n```python\nimport numpy as np\ndata = np.array([[ 1,  6, 11],[ 2,  7, 12],[ 3,  8, 13],[ 4,  9, 14],[ 5,  10, 15]])\nimpact_graph = np.array([[0, 1, 0, 0 ,0],[1, 0, 1, 0, 0],[0, 1, 0, 1, 0],[0, 0, 1, 0, 1],[0, 0, 0, 1, 0]])\nhistory_range = (0, 2)\ncurrent_range = (2, 3)\n\nedge_weight = compute_edgeweight(data, impact_graph, history_range, current_range, bin_size=1, delta=0.5)\nprint(edge_weight)\n```\n",
    "15&@M.netmedic.compute_abnormality": "### API Documentation for `compute_abnormality` Function\n\n#### Signature\n```python\ncompute_abnormality(data, history_range, current_range)\n```\n\n#### Function Summary\nThe `compute_abnormality` function calculates the abnormality score of given data with respect to its historical range. The score is based on the deviation of the current range's mean from the historical mean, normalized by the historical standard deviation.\n\n#### Parameters\n- `data` (`numpy.ndarray`): The input data array where each row represents a different variable and each column represents a different observation.\n- `history_range` (`Tuple[int, int]`): A tuple with two elements representing the start and end indices defining the range of columns to be used as historical data.\n- `current_range` (`Tuple[int, int]`): A tuple with two elements representing the start and end indices defining the range of columns to be used as current data.\n\n#### Returns\n- `abnormal` (`numpy.ndarray`): A numpy array where each element represents the abnormality score of the corresponding row in the input `data`. The score is a value between 0 and 1, indicating the degree of deviation of the current range from the historical range.\n\n#### Example\n```python\nimport numpy as np\nfrom scipy.stats import gaussian\n\n# Sample data (5 variables with 10 observations each)\ndata = np.random.randn(5, 10)\n\n# Historical range (first 5 observations)\nhistory_range = (0, 5)\n\n# Current range (last 5 observations)\ncurrent_range = (5, 10)\n\n# Calculate abnormality score for each variable\nabnormality_scores = compute_abnormality(data, history_range, current_range)\n\nprint(abnormality_scores)\n```\n\n**Expected Output:** A numpy array of abnormality scores. An example output might look like this:\n```\n[[0.76543210]\n [0.23456789]\n [0.34567891]\n [0.45678912]\n [0.56789023]]\n```\n\nPlease note that due to the randomness in the input data, actual output values will vary.",
    "13&@M.util_funcs.draw_weighted_graph": "# API Documentation\n\n## Signature\n```python\ndraw_weighted_graph(transition_matrix, filename, weight_multiplier=4)\n```\n\n## Function Summary\nThe `draw_weighted_graph` function takes in a transition matrix and generates a circular directed graph using Matplotlib and Networkx, displaying the transition probabilities as edge widths. The graph is saved as an image file. \n\n## Parameters\n- `transition_matrix` **(2D List of Floats)**: A square 2D list (nested list) where element at `[i][j]` position indicates the transition probability from state `i` to state `j`.\n- `filename` **(String)**: Name of the output file (with path) where the graph should be saved. The file extension (*.png, *.jpg, ...) in the filename determines the image format.\n- `weight_multiplier` **(Integer, optional)**: Parameter to control the relative widths of the edges in the graph. Default is `4`.\n\n## Returns\nThis function does not return a value. It creates an image file at the location specified by the `filename` parameter.\n\n## Example\n```python\ntransition_matrix = [[0, 0.5, 0.5], [0.2, 0.3, 0.5], [0.3, 0.2, 0.5]]\nfilename = \"/graphs/my_graph.png\"\ndraw_weighted_graph(transition_matrix, filename, weight_multiplier=8)\n```\n\nThis will use the given transition matrix and create a circular directed graph with edge widths proportional to the transition probabilities. The graph will be saved as a PNG image in the `/graphs` directory with the name `my_graph.png`. If the `/graphs` directory does not exist, it will be created. \n\nPlease note that adequate read/write permissions are required for the specified directory.\n",
    "11&@M.util_funcs.load": "# API Documentation\n\n## Signature \n\n```python\nload(\n    file_path,\n    sheet_name=\"Sheet1\",\n    aggre_delta=1,\n    normalize=True,\n    zero_fill_method='prevlatter',\n    verbose=True,\n)\n```\n\n## Function Summary\n\nThe `load` function is used to load and preprocess metric data from a pre-specified file path. The function offers multiple options for data extraction, aggregation, normalization, zero-filling and provides detailed information based on the verbosity level.\n\n## Parameters\n\n* `file_path` (**str**) - The complete path of the file from which the metrics are to be loaded. \n* `sheet_name` (**str**) - Name of the sheet in the excel file to be loaded. Default is `\"Sheet1\"`.\n* `aggre_delta` (**int**) - The factor by which to aggregate the data. Default is `1`, indicating no aggregation.\n* `normalize` (**bool**) - Whether to normalize the data by subtracting the mean and dividing by standard deviation. Default is `True`.\n* `zero_fill_method` (**str**) - The method to fill zeros in the data. Choices are 'prevlatter', 'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'previous', 'next'. Default is 'prevlatter'.\n* `verbose` (**bool**) - Determines the level of information to print for debugging. A higher number yields more verbose output. Default is `True`. \n\n## Returns\n\n* `data` (numpy array) - The metric data loaded from the file. The shape of the array is [T,N] where each column is a variable.\n* `data_head` (**list**) - A list of variables or service names present in the metric data file.\n\n## Example\n\n```python\ndata, data_head = load(\n    file_path='path/to/metrics.xlsx',\n    sheet_name='Metrics',\n    aggre_delta=2,\n    normalize=False,\n    zero_fill_method='linear',\n    verbose=True\n)\n```\nThis will load data from the 'Metrics' sheet in 'metrics.xlsx' file, aggregate it with a delta of 2, will not normalize it, fill zeros using the 'linear' method, and print verbose outputs.",
    "11&@M.util_funcs.aggregate": "# API Documentation\n\n---\n\n## Function Signature:\n\n```python\ndef aggregate(a, n=3):\n```\n\n---\n\n## Function Summary:\nThe `aggregate` function is a useful tool in data analysis. This function creates a new list containing the sum of every n-elements in the input list `a`. The function uses `numpy.cumsum` to accumulate the sum of the elements in the list. The summation starts from the first element and is incremented every `n` elements in the list.\n         \n\n## Parameters:\n\n1. **a** (list (of int, float, etc.)): The list of numerical values that you want to sum up every `n` elements.\n       \n\n2. **n** (int)(optional): The length of elements in which the sum is desired. It is optional and its default value is set to `3`.\n\n## Returns:\n**List**: The return value is a list of the accumulative sum of every `n` items in the input list.\n\n---\n\n## Example:\n\n```python\nimport numpy as np\r\na = np.arange(1,11)\r\naggregate(a, n=3)\r\n```\r\n\r\n**Expected Output:**\r\n```python\r\n[6.0, 15.0, 24.0, 10.0]\r\n```\r\n\r\nFrom the given example, the function sums up every 3 elements in the list `a` = [1,2,3,4,5,6,7,8,9,10]. It gives the result `[6,15,24,10]` where `6` is the sum of `1+2+3`, `15` is the sum of `4+5+6`, `24` is the sum of `7+8+9` and `10` is the remaining element `10`.",
    "10&@M.util_funcs.calc_pearson": "# API documentation\n\n## Signature\n`calc_pearson(matrix, method='default', zero_diag=True)`\n\n## Function Summary\nThis function calculates the Pearson correlation between nodes, given the input data matrix. It offers both manual and numpy implementation methods for the calculation and provides an option to zero the diagonal values in the resulting correlation matrix.\n\n## Parameters: \n\n- `matrix (2D list or numpy array of numeric values)` : This is the data used for calculations. It should be of shape [N, T] where N is the node num and T is the sample num.\n- `method (str, optional, default='default')` : The calculation method to be used. Should be either 'default' for manual calculations or 'numpy' for numpy implementation.\n- `zero_diag (bool, optional, default=True)` : Determines if the self correlation value (in the diagonal position) of the resulting correlation matrix is set to zero. If True, the diagonal values will be set to zero. \n\n## Returns:\n\n- `res (2D list of float)` : A 2D list representing the Pearson correlation matrix between nodes. If `zero_diag` parameter set to True, diagonal elements(correlation of an element to itself) will be 0.\n  \n## Example: \n\n```python\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(calc_pearson(matrix, 'numpy', True))\n```\n\nExpected output:\n\n```python\n[[0.0, 1.0, 1.0], [1.0, 0.0, 1.0], [1.0, 1.0, 0.0]]\n```"
}